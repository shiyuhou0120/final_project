{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd36d42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install torch\n",
    "# !pip install sentence_transformers\n",
    "# !pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3011c116",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\student\\anaconda3\\envs\\web_scraping\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "from torch.optim import Adam\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0c81cd",
   "metadata": {},
   "source": [
    "### 原始評論(利用bert原始的分詞方法)(會把每一個字都拆開)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6b3c8a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = pd.read_csv('./booking_comments_分詞update.csv', header=0)\n",
    "\n",
    "# 篩選評論數大於等於 30 的飯店\n",
    "df_filtered = df_.groupby('飯店名稱').filter(lambda x: len(x) >= 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0a32b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 開始測量(爬取時間)\n",
    "startime = datetime.datetime.now()\n",
    "\n",
    "# 初始化 RoBERTa-wwm-ext 模型和分詞器\n",
    "tokenizer = BertTokenizer.from_pretrained('hfl/chinese-roberta-wwm-ext')\n",
    "model = BertModel.from_pretrained('hfl/chinese-roberta-wwm-ext')\n",
    "\n",
    "# 評論文本\n",
    "sentences = list(df_filtered.loc[:, '綜合評論'])\n",
    "\n",
    "# 初始化一個空的 list 用於儲存飯店資訊\n",
    "hotel_info_list = []\n",
    "\n",
    "# 獲取每個文本的嵌入向量，並儲存對應的飯店資訊\n",
    "embeddings = []\n",
    "for idx, sentence in enumerate(sentences):\n",
    "    tokens = tokenizer.tokenize(sentence)  # 分詞\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "    # 檢查序列長度是否超過 512\n",
    "    if len(inputs[\"input_ids\"][0]) > 512:\n",
    "        print(\"The sentence is too long. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings.append(outputs.last_hidden_state.mean(dim=1).numpy().flatten())\n",
    "    hotel_info_list.append(df_filtered.iloc[idx].to_dict())\n",
    "\n",
    "# 儲存飯店資訊與 embeddings\n",
    "df_hotel_info = pd.DataFrame(hotel_info_list)\n",
    "df_hotel_info.to_csv('hotel_info_roberta.csv', index=False)\n",
    "np.save('embeddings_roberta.npy', embeddings)\n",
    "\n",
    "# 結束測量\n",
    "endtime = datetime.datetime.now()\n",
    "\n",
    "# 輸出結果\n",
    "print(\"執行時間：\", endtime - startime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d30566c",
   "metadata": {},
   "source": [
    "#### 利用迴圈跑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b63782e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-roberta-wwm-ext were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1 of 110...\n",
      "Sentence at index 34 is not a string. Skipping...\n",
      "Sentence at index 36 is not a string. Skipping...\n",
      "Sentence at index 43 is not a string. Skipping...\n",
      "Sentence at index 45 is not a string. Skipping...\n",
      "Sentence at index 60 is not a string. Skipping...\n",
      "Sentence at index 69 is not a string. Skipping...\n",
      "Sentence at index 90 is not a string. Skipping...\n",
      "Sentence at index 95 is not a string. Skipping...\n",
      "Sentence at index 101 is not a string. Skipping...\n",
      "Sentence at index 106 is not a string. Skipping...\n",
      "Sentence at index 107 is not a string. Skipping...\n",
      "Sentence at index 120 is not a string. Skipping...\n",
      "Sentence at index 133 is not a string. Skipping...\n",
      "Sentence at index 138 is not a string. Skipping...\n",
      "Sentence at index 139 is not a string. Skipping...\n",
      "Sentence at index 142 is not a string. Skipping...\n",
      "Sentence at index 144 is not a string. Skipping...\n",
      "Sentence at index 145 is not a string. Skipping...\n",
      "Sentence at index 148 is not a string. Skipping...\n",
      "Sentence at index 165 is not a string. Skipping...\n",
      "Sentence at index 167 is not a string. Skipping...\n",
      "Sentence at index 174 is not a string. Skipping...\n",
      "Sentence at index 183 is not a string. Skipping...\n",
      "Sentence at index 187 is not a string. Skipping...\n",
      "Sentence at index 190 is not a string. Skipping...\n",
      "Sentence at index 194 is not a string. Skipping...\n",
      "Sentence at index 210 is not a string. Skipping...\n",
      "Sentence at index 224 is not a string. Skipping...\n",
      "Sentence at index 228 is not a string. Skipping...\n",
      "Sentence at index 238 is not a string. Skipping...\n",
      "Sentence at index 242 is not a string. Skipping...\n",
      "Sentence at index 243 is not a string. Skipping...\n",
      "Sentence at index 245 is not a string. Skipping...\n",
      "Sentence at index 247 is not a string. Skipping...\n",
      "Sentence at index 248 is not a string. Skipping...\n",
      "Sentence at index 249 is not a string. Skipping...\n",
      "Sentence at index 250 is not a string. Skipping...\n",
      "Sentence at index 251 is not a string. Skipping...\n",
      "Sentence at index 252 is not a string. Skipping...\n",
      "Sentence at index 259 is not a string. Skipping...\n",
      "Sentence at index 262 is not a string. Skipping...\n",
      "Sentence at index 264 is not a string. Skipping...\n",
      "Sentence at index 273 is not a string. Skipping...\n",
      "Sentence at index 274 is not a string. Skipping...\n",
      "Sentence at index 275 is not a string. Skipping...\n",
      "Sentence at index 281 is not a string. Skipping...\n",
      "Sentence at index 282 is not a string. Skipping...\n",
      "Sentence at index 284 is not a string. Skipping...\n",
      "Sentence at index 285 is not a string. Skipping...\n",
      "Sentence at index 288 is not a string. Skipping...\n",
      "Sentence at index 295 is not a string. Skipping...\n",
      "Sentence at index 302 is not a string. Skipping...\n",
      "Sentence at index 308 is not a string. Skipping...\n",
      "Sentence at index 315 is not a string. Skipping...\n",
      "Sentence at index 324 is not a string. Skipping...\n",
      "Sentence at index 336 is not a string. Skipping...\n",
      "Sentence at index 337 is not a string. Skipping...\n",
      "Sentence at index 339 is not a string. Skipping...\n",
      "Sentence at index 340 is not a string. Skipping...\n",
      "Sentence at index 342 is not a string. Skipping...\n",
      "Sentence at index 344 is not a string. Skipping...\n",
      "Sentence at index 346 is not a string. Skipping...\n",
      "Sentence at index 348 is not a string. Skipping...\n",
      "Sentence at index 352 is not a string. Skipping...\n",
      "Sentence at index 358 is not a string. Skipping...\n",
      "Sentence at index 359 is not a string. Skipping...\n",
      "Sentence at index 360 is not a string. Skipping...\n",
      "Sentence at index 363 is not a string. Skipping...\n",
      "Sentence at index 364 is not a string. Skipping...\n",
      "Sentence at index 365 is not a string. Skipping...\n",
      "Sentence at index 366 is not a string. Skipping...\n",
      "Sentence at index 368 is not a string. Skipping...\n",
      "Sentence at index 370 is not a string. Skipping...\n",
      "Sentence at index 403 is not a string. Skipping...\n",
      "Sentence at index 405 is not a string. Skipping...\n",
      "Sentence at index 420 is not a string. Skipping...\n",
      "Sentence at index 452 is not a string. Skipping...\n",
      "Sentence at index 456 is not a string. Skipping...\n",
      "Sentence at index 461 is not a string. Skipping...\n",
      "Sentence at index 466 is not a string. Skipping...\n",
      "Sentence at index 473 is not a string. Skipping...\n",
      "Sentence at index 480 is not a string. Skipping...\n",
      "Sentence at index 481 is not a string. Skipping...\n",
      "Sentence at index 482 is not a string. Skipping...\n",
      "Sentence at index 486 is not a string. Skipping...\n",
      "Sentence at index 490 is not a string. Skipping...\n",
      "Sentence at index 513 is not a string. Skipping...\n",
      "Sentence at index 523 is not a string. Skipping...\n",
      "Sentence at index 538 is not a string. Skipping...\n",
      "Sentence at index 540 is not a string. Skipping...\n",
      "Sentence at index 541 is not a string. Skipping...\n",
      "Sentence at index 553 is not a string. Skipping...\n",
      "Sentence at index 567 is not a string. Skipping...\n",
      "Sentence at index 583 is not a string. Skipping...\n",
      "Sentence at index 597 is not a string. Skipping...\n",
      "Sentence at index 651 is not a string. Skipping...\n",
      "Sentence at index 686 is not a string. Skipping...\n",
      "Sentence at index 687 is not a string. Skipping...\n",
      "Sentence at index 700 is not a string. Skipping...\n",
      "Sentence at index 716 is not a string. Skipping...\n",
      "Sentence at index 719 is not a string. Skipping...\n",
      "Sentence at index 723 is not a string. Skipping...\n",
      "Sentence at index 745 is not a string. Skipping...\n",
      "Sentence at index 747 is not a string. Skipping...\n",
      "Sentence at index 766 is not a string. Skipping...\n",
      "Sentence at index 770 is not a string. Skipping...\n",
      "Sentence at index 775 is not a string. Skipping...\n",
      "Sentence at index 784 is not a string. Skipping...\n",
      "Sentence at index 831 is not a string. Skipping...\n",
      "Sentence at index 863 is not a string. Skipping...\n",
      "Sentence at index 915 is not a string. Skipping...\n",
      "Sentence at index 938 is not a string. Skipping...\n",
      "Sentence at index 941 is not a string. Skipping...\n",
      "Sentence at index 958 is not a string. Skipping...\n",
      "Sentence at index 970 is not a string. Skipping...\n",
      "Sentence at index 973 is not a string. Skipping...\n",
      "Sentence at index 974 is not a string. Skipping...\n",
      "Sentence at index 1015 is not a string. Skipping...\n",
      "Sentence at index 1033 is not a string. Skipping...\n",
      "Sentence at index 1062 is not a string. Skipping...\n",
      "Sentence at index 1070 is not a string. Skipping...\n",
      "Sentence at index 1078 is not a string. Skipping...\n",
      "Sentence at index 1081 is not a string. Skipping...\n",
      "Sentence at index 1085 is not a string. Skipping...\n",
      "Sentence at index 1086 is not a string. Skipping...\n",
      "Sentence at index 1087 is not a string. Skipping...\n",
      "Sentence at index 1088 is not a string. Skipping...\n",
      "Sentence at index 1090 is not a string. Skipping...\n",
      "Sentence at index 1100 is not a string. Skipping...\n",
      "Sentence at index 1103 is not a string. Skipping...\n",
      "Sentence at index 1104 is not a string. Skipping...\n",
      "Sentence at index 1106 is not a string. Skipping...\n",
      "Sentence at index 1108 is not a string. Skipping...\n",
      "Sentence at index 1109 is not a string. Skipping...\n",
      "Sentence at index 1113 is not a string. Skipping...\n",
      "Sentence at index 1119 is not a string. Skipping...\n",
      "Sentence at index 1120 is not a string. Skipping...\n",
      "Sentence at index 1122 is not a string. Skipping...\n",
      "Sentence at index 1124 is not a string. Skipping...\n",
      "Sentence at index 1125 is not a string. Skipping...\n",
      "Sentence at index 1128 is not a string. Skipping...\n",
      "Sentence at index 1131 is not a string. Skipping...\n",
      "Sentence at index 1134 is not a string. Skipping...\n",
      "Sentence at index 1136 is not a string. Skipping...\n",
      "Sentence at index 1138 is not a string. Skipping...\n",
      "Sentence at index 1143 is not a string. Skipping...\n",
      "Sentence at index 1144 is not a string. Skipping...\n",
      "Sentence at index 1147 is not a string. Skipping...\n",
      "Sentence at index 1155 is not a string. Skipping...\n",
      "Sentence at index 1159 is not a string. Skipping...\n",
      "Sentence at index 1166 is not a string. Skipping...\n",
      "Sentence at index 1168 is not a string. Skipping...\n",
      "Sentence at index 1169 is not a string. Skipping...\n",
      "Sentence at index 1177 is not a string. Skipping...\n",
      "Sentence at index 1187 is not a string. Skipping...\n",
      "Sentence at index 1188 is not a string. Skipping...\n",
      "Sentence at index 1191 is not a string. Skipping...\n",
      "Sentence at index 1200 is not a string. Skipping...\n",
      "Sentence at index 1205 is not a string. Skipping...\n",
      "Sentence at index 1206 is not a string. Skipping...\n",
      "Sentence at index 1209 is not a string. Skipping...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence at index 1212 is not a string. Skipping...\n",
      "Sentence at index 1213 is not a string. Skipping...\n",
      "Sentence at index 1214 is not a string. Skipping...\n",
      "Sentence at index 1215 is not a string. Skipping...\n",
      "Sentence at index 1216 is not a string. Skipping...\n",
      "Sentence at index 1217 is not a string. Skipping...\n",
      "Sentence at index 1218 is not a string. Skipping...\n",
      "Sentence at index 1219 is not a string. Skipping...\n",
      "Sentence at index 1220 is not a string. Skipping...\n",
      "Sentence at index 1221 is not a string. Skipping...\n",
      "Sentence at index 1222 is not a string. Skipping...\n",
      "Sentence at index 1223 is not a string. Skipping...\n",
      "Sentence at index 1224 is not a string. Skipping...\n",
      "Sentence at index 1225 is not a string. Skipping...\n",
      "Sentence at index 1226 is not a string. Skipping...\n",
      "Sentence at index 1227 is not a string. Skipping...\n",
      "Sentence at index 1228 is not a string. Skipping...\n",
      "Sentence at index 1229 is not a string. Skipping...\n",
      "Sentence at index 1230 is not a string. Skipping...\n",
      "Sentence at index 1243 is not a string. Skipping...\n",
      "Sentence at index 1251 is not a string. Skipping...\n",
      "Sentence at index 1253 is not a string. Skipping...\n",
      "Sentence at index 1266 is not a string. Skipping...\n",
      "Sentence at index 1267 is not a string. Skipping...\n",
      "Sentence at index 1268 is not a string. Skipping...\n",
      "Sentence at index 1272 is not a string. Skipping...\n",
      "Sentence at index 1277 is not a string. Skipping...\n",
      "Sentence at index 1278 is not a string. Skipping...\n",
      "Sentence at index 1279 is not a string. Skipping...\n",
      "Sentence at index 1310 is not a string. Skipping...\n",
      "Sentence at index 1321 is not a string. Skipping...\n",
      "Sentence at index 1325 is not a string. Skipping...\n",
      "Sentence at index 1329 is not a string. Skipping...\n",
      "Sentence at index 1333 is not a string. Skipping...\n",
      "Sentence at index 1356 is not a string. Skipping...\n",
      "Sentence at index 1358 is not a string. Skipping...\n",
      "Sentence at index 1377 is not a string. Skipping...\n",
      "Sentence at index 1381 is not a string. Skipping...\n",
      "Sentence at index 1399 is not a string. Skipping...\n",
      "Sentence at index 1406 is not a string. Skipping...\n",
      "Sentence at index 1416 is not a string. Skipping...\n",
      "Sentence at index 1427 is not a string. Skipping...\n",
      "Sentence at index 1429 is not a string. Skipping...\n",
      "Sentence at index 1445 is not a string. Skipping...\n",
      "Sentence at index 1449 is not a string. Skipping...\n",
      "Sentence at index 1459 is not a string. Skipping...\n",
      "Sentence at index 1467 is not a string. Skipping...\n",
      "Sentence at index 1469 is not a string. Skipping...\n",
      "Sentence at index 1488 is not a string. Skipping...\n",
      "Sentence at index 1491 is not a string. Skipping...\n",
      "Sentence at index 1494 is not a string. Skipping...\n",
      "Sentence at index 1508 is not a string. Skipping...\n",
      "Sentence at index 1511 is not a string. Skipping...\n",
      "Sentence at index 1548 is not a string. Skipping...\n",
      "Sentence at index 1552 is not a string. Skipping...\n",
      "Sentence at index 1557 is not a string. Skipping...\n",
      "Sentence at index 1573 is not a string. Skipping...\n",
      "Sentence at index 1576 is not a string. Skipping...\n",
      "Sentence at index 1584 is not a string. Skipping...\n",
      "Sentence at index 1594 is not a string. Skipping...\n",
      "Sentence at index 1599 is not a string. Skipping...\n",
      "Sentence at index 1616 is not a string. Skipping...\n",
      "Sentence at index 1617 is not a string. Skipping...\n",
      "Sentence at index 1618 is not a string. Skipping...\n",
      "Sentence at index 1629 is not a string. Skipping...\n",
      "Sentence at index 1636 is not a string. Skipping...\n",
      "Sentence at index 1639 is not a string. Skipping...\n",
      "Sentence at index 1640 is not a string. Skipping...\n",
      "Sentence at index 1645 is not a string. Skipping...\n",
      "Sentence at index 1648 is not a string. Skipping...\n",
      "Sentence at index 1651 is not a string. Skipping...\n",
      "Sentence at index 1657 is not a string. Skipping...\n",
      "Sentence at index 1665 is not a string. Skipping...\n",
      "Sentence at index 1670 is not a string. Skipping...\n",
      "Sentence at index 1686 is not a string. Skipping...\n",
      "Sentence at index 1687 is not a string. Skipping...\n",
      "Sentence at index 1689 is not a string. Skipping...\n",
      "Sentence at index 1692 is not a string. Skipping...\n",
      "Sentence at index 1695 is not a string. Skipping...\n",
      "Sentence at index 1712 is not a string. Skipping...\n",
      "Sentence at index 1713 is not a string. Skipping...\n",
      "Sentence at index 1717 is not a string. Skipping...\n",
      "Sentence at index 1719 is not a string. Skipping...\n",
      "Sentence at index 1722 is not a string. Skipping...\n",
      "Sentence at index 1737 is not a string. Skipping...\n",
      "Sentence at index 1751 is not a string. Skipping...\n",
      "Sentence at index 1765 is not a string. Skipping...\n",
      "Sentence at index 1766 is not a string. Skipping...\n",
      "Sentence at index 1770 is not a string. Skipping...\n",
      "Sentence at index 1771 is not a string. Skipping...\n",
      "Sentence at index 1772 is not a string. Skipping...\n",
      "Sentence at index 1773 is not a string. Skipping...\n",
      "Sentence at index 1774 is not a string. Skipping...\n",
      "Sentence at index 1775 is not a string. Skipping...\n",
      "Sentence at index 1776 is not a string. Skipping...\n",
      "Sentence at index 1777 is not a string. Skipping...\n",
      "Sentence at index 1778 is not a string. Skipping...\n",
      "Sentence at index 1779 is not a string. Skipping...\n",
      "Sentence at index 1780 is not a string. Skipping...\n",
      "Sentence at index 1798 is not a string. Skipping...\n",
      "Sentence at index 1803 is not a string. Skipping...\n",
      "Sentence at index 1816 is not a string. Skipping...\n",
      "Sentence at index 1820 is not a string. Skipping...\n",
      "Sentence at index 1821 is not a string. Skipping...\n",
      "Sentence at index 1822 is not a string. Skipping...\n",
      "Sentence at index 1845 is not a string. Skipping...\n",
      "Sentence at index 1852 is not a string. Skipping...\n",
      "Sentence at index 1870 is not a string. Skipping...\n",
      "Sentence at index 1877 is not a string. Skipping...\n",
      "Sentence at index 1882 is not a string. Skipping...\n",
      "Sentence at index 1887 is not a string. Skipping...\n",
      "Sentence at index 1901 is not a string. Skipping...\n",
      "Sentence at index 1919 is not a string. Skipping...\n",
      "Sentence at index 1925 is not a string. Skipping...\n",
      "Sentence at index 1940 is not a string. Skipping...\n",
      "Sentence at index 1941 is not a string. Skipping...\n",
      "Sentence at index 1942 is not a string. Skipping...\n",
      "Sentence at index 1949 is not a string. Skipping...\n",
      "Sentence at index 1953 is not a string. Skipping...\n",
      "Sentence at index 1972 is not a string. Skipping...\n",
      "Sentence at index 1977 is not a string. Skipping...\n",
      "Sentence at index 1987 is not a string. Skipping...\n",
      "Sentence at index 1998 is not a string. Skipping...\n",
      "Sentence at index 2009 is not a string. Skipping...\n",
      "Sentence at index 2014 is not a string. Skipping...\n",
      "Sentence at index 2019 is not a string. Skipping...\n",
      "Sentence at index 2022 is not a string. Skipping...\n",
      "Sentence at index 2025 is not a string. Skipping...\n",
      "Sentence at index 2039 is not a string. Skipping...\n",
      "Sentence at index 2043 is not a string. Skipping...\n",
      "Sentence at index 2080 is not a string. Skipping...\n",
      "Sentence at index 2082 is not a string. Skipping...\n",
      "Sentence at index 2093 is not a string. Skipping...\n",
      "Sentence at index 2099 is not a string. Skipping...\n",
      "Sentence at index 2131 is not a string. Skipping...\n",
      "Sentence at index 2133 is not a string. Skipping...\n",
      "Sentence at index 2140 is not a string. Skipping...\n",
      "Sentence at index 2145 is not a string. Skipping...\n",
      "Sentence at index 2170 is not a string. Skipping...\n",
      "Sentence at index 2171 is not a string. Skipping...\n",
      "Sentence at index 2176 is not a string. Skipping...\n",
      "Sentence at index 2198 is not a string. Skipping...\n",
      "Sentence at index 2201 is not a string. Skipping...\n",
      "Sentence at index 2215 is not a string. Skipping...\n",
      "Sentence at index 2217 is not a string. Skipping...\n",
      "Sentence at index 2232 is not a string. Skipping...\n",
      "Sentence at index 2237 is not a string. Skipping...\n",
      "Sentence at index 2240 is not a string. Skipping...\n",
      "Sentence at index 2248 is not a string. Skipping...\n",
      "Sentence at index 2255 is not a string. Skipping...\n",
      "Sentence at index 2258 is not a string. Skipping...\n",
      "Sentence at index 2281 is not a string. Skipping...\n",
      "Sentence at index 2302 is not a string. Skipping...\n",
      "Sentence at index 2305 is not a string. Skipping...\n",
      "Sentence at index 2309 is not a string. Skipping...\n",
      "Sentence at index 2312 is not a string. Skipping...\n",
      "Sentence at index 2314 is not a string. Skipping...\n",
      "Sentence at index 2315 is not a string. Skipping...\n",
      "Sentence at index 2320 is not a string. Skipping...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence at index 2325 is not a string. Skipping...\n",
      "Sentence at index 2329 is not a string. Skipping...\n",
      "Sentence at index 2340 is not a string. Skipping...\n",
      "Sentence at index 2350 is not a string. Skipping...\n",
      "Sentence at index 2353 is not a string. Skipping...\n",
      "Sentence at index 2355 is not a string. Skipping...\n",
      "Sentence at index 2358 is not a string. Skipping...\n",
      "Sentence at index 2367 is not a string. Skipping...\n",
      "Sentence at index 2368 is not a string. Skipping...\n",
      "Sentence at index 2370 is not a string. Skipping...\n",
      "Sentence at index 2377 is not a string. Skipping...\n",
      "Sentence at index 2379 is not a string. Skipping...\n",
      "Sentence at index 2382 is not a string. Skipping...\n",
      "Sentence at index 2385 is not a string. Skipping...\n",
      "Sentence at index 2387 is not a string. Skipping...\n",
      "Sentence at index 2399 is not a string. Skipping...\n",
      "Sentence at index 2403 is not a string. Skipping...\n",
      "Sentence at index 2406 is not a string. Skipping...\n",
      "Sentence at index 2407 is not a string. Skipping...\n",
      "Sentence at index 2424 is not a string. Skipping...\n",
      "Sentence at index 2426 is not a string. Skipping...\n",
      "Sentence at index 2427 is not a string. Skipping...\n",
      "Sentence at index 2431 is not a string. Skipping...\n",
      "Sentence at index 2434 is not a string. Skipping...\n",
      "Sentence at index 2435 is not a string. Skipping...\n",
      "Sentence at index 2436 is not a string. Skipping...\n",
      "Sentence at index 2437 is not a string. Skipping...\n",
      "Sentence at index 2438 is not a string. Skipping...\n",
      "Sentence at index 2439 is not a string. Skipping...\n",
      "Sentence at index 2440 is not a string. Skipping...\n",
      "Sentence at index 2457 is not a string. Skipping...\n",
      "Sentence at index 2478 is not a string. Skipping...\n",
      "Sentence at index 2479 is not a string. Skipping...\n",
      "Sentence at index 2491 is not a string. Skipping...\n",
      "Sentence at index 2510 is not a string. Skipping...\n",
      "Sentence at index 2514 is not a string. Skipping...\n",
      "Sentence at index 2515 is not a string. Skipping...\n",
      "Sentence at index 2516 is not a string. Skipping...\n",
      "Sentence at index 2520 is not a string. Skipping...\n",
      "Sentence at index 2526 is not a string. Skipping...\n",
      "Sentence at index 2532 is not a string. Skipping...\n",
      "Sentence at index 2535 is not a string. Skipping...\n",
      "Sentence at index 2543 is not a string. Skipping...\n",
      "Sentence at index 2546 is not a string. Skipping...\n",
      "Sentence at index 2585 is not a string. Skipping...\n",
      "Sentence at index 2589 is not a string. Skipping...\n",
      "Sentence at index 2592 is not a string. Skipping...\n",
      "Sentence at index 2598 is not a string. Skipping...\n",
      "Sentence at index 2612 is not a string. Skipping...\n",
      "Sentence at index 2621 is not a string. Skipping...\n",
      "Sentence at index 2623 is not a string. Skipping...\n",
      "Sentence at index 2627 is not a string. Skipping...\n",
      "Sentence at index 2634 is not a string. Skipping...\n",
      "Sentence at index 2640 is not a string. Skipping...\n",
      "Sentence at index 2641 is not a string. Skipping...\n",
      "Sentence at index 2642 is not a string. Skipping...\n",
      "Sentence at index 2643 is not a string. Skipping...\n",
      "Sentence at index 2644 is not a string. Skipping...\n",
      "Sentence at index 2645 is not a string. Skipping...\n",
      "Sentence at index 2646 is not a string. Skipping...\n",
      "Sentence at index 2647 is not a string. Skipping...\n",
      "Sentence at index 2648 is not a string. Skipping...\n",
      "Sentence at index 2649 is not a string. Skipping...\n",
      "Sentence at index 2650 is not a string. Skipping...\n",
      "Sentence at index 2671 is not a string. Skipping...\n",
      "Sentence at index 2681 is not a string. Skipping...\n",
      "Sentence at index 2687 is not a string. Skipping...\n",
      "Sentence at index 2689 is not a string. Skipping...\n",
      "Sentence at index 2691 is not a string. Skipping...\n",
      "Sentence at index 2692 is not a string. Skipping...\n",
      "Sentence at index 2693 is not a string. Skipping...\n",
      "Sentence at index 2698 is not a string. Skipping...\n",
      "Sentence at index 2701 is not a string. Skipping...\n",
      "Sentence at index 2709 is not a string. Skipping...\n",
      "Sentence at index 2724 is not a string. Skipping...\n",
      "Sentence at index 2730 is not a string. Skipping...\n",
      "Sentence at index 2732 is not a string. Skipping...\n",
      "Sentence at index 2737 is not a string. Skipping...\n",
      "Sentence at index 2738 is not a string. Skipping...\n",
      "Sentence at index 2751 is not a string. Skipping...\n",
      "Sentence at index 2752 is not a string. Skipping...\n",
      "Sentence at index 2755 is not a string. Skipping...\n",
      "Sentence at index 2784 is not a string. Skipping...\n",
      "Sentence at index 2811 is not a string. Skipping...\n",
      "Sentence at index 2826 is not a string. Skipping...\n",
      "Sentence at index 2834 is not a string. Skipping...\n",
      "Sentence at index 2847 is not a string. Skipping...\n",
      "Sentence at index 2852 is not a string. Skipping...\n",
      "Sentence at index 2864 is not a string. Skipping...\n",
      "Sentence at index 2874 is not a string. Skipping...\n",
      "Sentence at index 2880 is not a string. Skipping...\n",
      "Sentence at index 2896 is not a string. Skipping...\n",
      "Sentence at index 2933 is not a string. Skipping...\n",
      "Sentence at index 2957 is not a string. Skipping...\n",
      "Sentence at index 2962 is not a string. Skipping...\n",
      "Sentence at index 2982 is not a string. Skipping...\n",
      "Sentence at index 2985 is not a string. Skipping...\n",
      "Sentence at index 2988 is not a string. Skipping...\n",
      "Sentence at index 2989 is not a string. Skipping...\n",
      "Sentence at index 2991 is not a string. Skipping...\n",
      "Sentence at index 2994 is not a string. Skipping...\n",
      "Sentence at index 2997 is not a string. Skipping...\n",
      "Sentence at index 2998 is not a string. Skipping...\n",
      "Sentence at index 3009 is not a string. Skipping...\n",
      "Sentence at index 3011 is not a string. Skipping...\n",
      "Sentence at index 3015 is not a string. Skipping...\n",
      "Sentence at index 3017 is not a string. Skipping...\n",
      "Sentence at index 3018 is not a string. Skipping...\n",
      "Sentence at index 3021 is not a string. Skipping...\n",
      "Sentence at index 3023 is not a string. Skipping...\n",
      "Sentence at index 3029 is not a string. Skipping...\n",
      "Sentence at index 3046 is not a string. Skipping...\n",
      "Sentence at index 3047 is not a string. Skipping...\n",
      "Sentence at index 3048 is not a string. Skipping...\n",
      "Sentence at index 3053 is not a string. Skipping...\n",
      "Sentence at index 3057 is not a string. Skipping...\n",
      "Sentence at index 3060 is not a string. Skipping...\n",
      "Sentence at index 3064 is not a string. Skipping...\n",
      "Sentence at index 3066 is not a string. Skipping...\n",
      "Sentence at index 3067 is not a string. Skipping...\n",
      "Sentence at index 3068 is not a string. Skipping...\n",
      "Sentence at index 3071 is not a string. Skipping...\n",
      "Sentence at index 3072 is not a string. Skipping...\n",
      "Sentence at index 3075 is not a string. Skipping...\n",
      "Sentence at index 3077 is not a string. Skipping...\n",
      "Sentence at index 3079 is not a string. Skipping...\n",
      "Sentence at index 3080 is not a string. Skipping...\n",
      "Sentence at index 3087 is not a string. Skipping...\n",
      "Sentence at index 3089 is not a string. Skipping...\n",
      "Sentence at index 3091 is not a string. Skipping...\n",
      "Sentence at index 3092 is not a string. Skipping...\n",
      "Sentence at index 3094 is not a string. Skipping...\n",
      "Sentence at index 3100 is not a string. Skipping...\n",
      "Sentence at index 3101 is not a string. Skipping...\n",
      "Sentence at index 3108 is not a string. Skipping...\n",
      "Sentence at index 3109 is not a string. Skipping...\n",
      "Sentence at index 3114 is not a string. Skipping...\n",
      "Sentence at index 3115 is not a string. Skipping...\n",
      "Sentence at index 3117 is not a string. Skipping...\n",
      "Sentence at index 3118 is not a string. Skipping...\n",
      "Sentence at index 3126 is not a string. Skipping...\n",
      "Sentence at index 3133 is not a string. Skipping...\n",
      "Sentence at index 3135 is not a string. Skipping...\n",
      "Sentence at index 3146 is not a string. Skipping...\n",
      "Sentence at index 3148 is not a string. Skipping...\n",
      "Sentence at index 3151 is not a string. Skipping...\n",
      "Sentence at index 3159 is not a string. Skipping...\n",
      "Sentence at index 3175 is not a string. Skipping...\n",
      "Sentence at index 3176 is not a string. Skipping...\n",
      "Sentence at index 3182 is not a string. Skipping...\n",
      "Sentence at index 3183 is not a string. Skipping...\n",
      "Sentence at index 3184 is not a string. Skipping...\n",
      "Sentence at index 3185 is not a string. Skipping...\n",
      "Sentence at index 3186 is not a string. Skipping...\n",
      "Sentence at index 3187 is not a string. Skipping...\n",
      "Sentence at index 3188 is not a string. Skipping...\n",
      "Sentence at index 3189 is not a string. Skipping...\n",
      "Sentence at index 3190 is not a string. Skipping...\n",
      "Sentence at index 3191 is not a string. Skipping...\n",
      "Sentence at index 3192 is not a string. Skipping...\n",
      "Sentence at index 3193 is not a string. Skipping...\n",
      "Sentence at index 3194 is not a string. Skipping...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence at index 3199 is not a string. Skipping...\n",
      "Sentence at index 3202 is not a string. Skipping...\n",
      "Sentence at index 3214 is not a string. Skipping...\n",
      "Sentence at index 3222 is not a string. Skipping...\n",
      "Sentence at index 3232 is not a string. Skipping...\n",
      "Sentence at index 3234 is not a string. Skipping...\n",
      "Sentence at index 3239 is not a string. Skipping...\n",
      "Sentence at index 3247 is not a string. Skipping...\n",
      "Sentence at index 3264 is not a string. Skipping...\n",
      "Sentence at index 3272 is not a string. Skipping...\n",
      "Sentence at index 3273 is not a string. Skipping...\n",
      "Sentence at index 3275 is not a string. Skipping...\n",
      "Sentence at index 3279 is not a string. Skipping...\n",
      "Sentence at index 3290 is not a string. Skipping...\n",
      "Sentence at index 3303 is not a string. Skipping...\n",
      "Sentence at index 3343 is not a string. Skipping...\n",
      "Sentence at index 3348 is not a string. Skipping...\n",
      "Sentence at index 3356 is not a string. Skipping...\n",
      "Sentence at index 3363 is not a string. Skipping...\n",
      "Sentence at index 3364 is not a string. Skipping...\n",
      "Sentence at index 3368 is not a string. Skipping...\n",
      "Sentence at index 3372 is not a string. Skipping...\n",
      "Sentence at index 3373 is not a string. Skipping...\n",
      "Sentence at index 3377 is not a string. Skipping...\n",
      "Sentence at index 3381 is not a string. Skipping...\n",
      "Sentence at index 3382 is not a string. Skipping...\n",
      "Sentence at index 3385 is not a string. Skipping...\n",
      "Sentence at index 3399 is not a string. Skipping...\n",
      "Sentence at index 3400 is not a string. Skipping...\n",
      "Sentence at index 3406 is not a string. Skipping...\n",
      "Sentence at index 3408 is not a string. Skipping...\n",
      "Sentence at index 3419 is not a string. Skipping...\n",
      "Sentence at index 3420 is not a string. Skipping...\n",
      "Sentence at index 3426 is not a string. Skipping...\n",
      "Sentence at index 3431 is not a string. Skipping...\n",
      "Sentence at index 3436 is not a string. Skipping...\n",
      "Sentence at index 3439 is not a string. Skipping...\n",
      "Sentence at index 3442 is not a string. Skipping...\n",
      "Sentence at index 3448 is not a string. Skipping...\n",
      "Sentence at index 3458 is not a string. Skipping...\n",
      "Sentence at index 3460 is not a string. Skipping...\n",
      "Sentence at index 3465 is not a string. Skipping...\n",
      "Sentence at index 3466 is not a string. Skipping...\n",
      "Sentence at index 3468 is not a string. Skipping...\n",
      "Sentence at index 3473 is not a string. Skipping...\n",
      "Sentence at index 3476 is not a string. Skipping...\n",
      "Sentence at index 3477 is not a string. Skipping...\n",
      "Sentence at index 3478 is not a string. Skipping...\n",
      "Sentence at index 3485 is not a string. Skipping...\n",
      "Sentence at index 3489 is not a string. Skipping...\n",
      "Sentence at index 3493 is not a string. Skipping...\n",
      "Sentence at index 3499 is not a string. Skipping...\n",
      "Sentence at index 3500 is not a string. Skipping...\n",
      "Sentence at index 3501 is not a string. Skipping...\n",
      "Sentence at index 3503 is not a string. Skipping...\n",
      "Sentence at index 3505 is not a string. Skipping...\n",
      "Sentence at index 3508 is not a string. Skipping...\n",
      "Sentence at index 3509 is not a string. Skipping...\n",
      "Sentence at index 3510 is not a string. Skipping...\n",
      "Sentence at index 3511 is not a string. Skipping...\n",
      "Sentence at index 3512 is not a string. Skipping...\n",
      "Sentence at index 3513 is not a string. Skipping...\n",
      "Sentence at index 3514 is not a string. Skipping...\n",
      "Sentence at index 3515 is not a string. Skipping...\n",
      "Sentence at index 3517 is not a string. Skipping...\n",
      "Sentence at index 3519 is not a string. Skipping...\n",
      "Sentence at index 3523 is not a string. Skipping...\n",
      "Sentence at index 3532 is not a string. Skipping...\n",
      "Sentence at index 3534 is not a string. Skipping...\n",
      "Sentence at index 3542 is not a string. Skipping...\n",
      "Sentence at index 3581 is not a string. Skipping...\n",
      "Sentence at index 3596 is not a string. Skipping...\n",
      "Sentence at index 3600 is not a string. Skipping...\n",
      "Sentence at index 3603 is not a string. Skipping...\n",
      "Sentence at index 3607 is not a string. Skipping...\n",
      "Sentence at index 3608 is not a string. Skipping...\n",
      "Sentence at index 3623 is not a string. Skipping...\n",
      "Sentence at index 3632 is not a string. Skipping...\n",
      "Sentence at index 3639 is not a string. Skipping...\n",
      "Sentence at index 3641 is not a string. Skipping...\n",
      "Sentence at index 3650 is not a string. Skipping...\n",
      "Sentence at index 3664 is not a string. Skipping...\n",
      "Sentence at index 3669 is not a string. Skipping...\n",
      "Sentence at index 3670 is not a string. Skipping...\n",
      "Sentence at index 3681 is not a string. Skipping...\n",
      "Sentence at index 3709 is not a string. Skipping...\n",
      "Sentence at index 3721 is not a string. Skipping...\n",
      "Sentence at index 3724 is not a string. Skipping...\n",
      "Sentence at index 3725 is not a string. Skipping...\n",
      "Sentence at index 3726 is not a string. Skipping...\n",
      "Sentence at index 3736 is not a string. Skipping...\n",
      "Sentence at index 3737 is not a string. Skipping...\n",
      "Sentence at index 3746 is not a string. Skipping...\n",
      "Sentence at index 3750 is not a string. Skipping...\n",
      "Sentence at index 3751 is not a string. Skipping...\n",
      "Sentence at index 3754 is not a string. Skipping...\n",
      "Sentence at index 3766 is not a string. Skipping...\n",
      "Sentence at index 3771 is not a string. Skipping...\n",
      "Sentence at index 3776 is not a string. Skipping...\n",
      "Sentence at index 3777 is not a string. Skipping...\n",
      "Sentence at index 3779 is not a string. Skipping...\n",
      "Sentence at index 3781 is not a string. Skipping...\n",
      "Sentence at index 3787 is not a string. Skipping...\n",
      "Sentence at index 3792 is not a string. Skipping...\n",
      "Sentence at index 3797 is not a string. Skipping...\n",
      "Sentence at index 3802 is not a string. Skipping...\n",
      "Sentence at index 3806 is not a string. Skipping...\n",
      "Sentence at index 3816 is not a string. Skipping...\n",
      "Sentence at index 3822 is not a string. Skipping...\n",
      "Sentence at index 3825 is not a string. Skipping...\n",
      "Sentence at index 3827 is not a string. Skipping...\n",
      "Sentence at index 3828 is not a string. Skipping...\n",
      "Sentence at index 3829 is not a string. Skipping...\n",
      "Sentence at index 3830 is not a string. Skipping...\n",
      "Sentence at index 3832 is not a string. Skipping...\n",
      "Sentence at index 3839 is not a string. Skipping...\n",
      "Sentence at index 3845 is not a string. Skipping...\n",
      "Sentence at index 3846 is not a string. Skipping...\n",
      "Sentence at index 3850 is not a string. Skipping...\n",
      "Sentence at index 3855 is not a string. Skipping...\n",
      "Sentence at index 3859 is not a string. Skipping...\n",
      "Sentence at index 3862 is not a string. Skipping...\n",
      "Sentence at index 3869 is not a string. Skipping...\n",
      "Sentence at index 3871 is not a string. Skipping...\n",
      "Sentence at index 3875 is not a string. Skipping...\n",
      "Sentence at index 3877 is not a string. Skipping...\n",
      "Sentence at index 3879 is not a string. Skipping...\n",
      "Sentence at index 3880 is not a string. Skipping...\n",
      "Sentence at index 3887 is not a string. Skipping...\n",
      "Sentence at index 3890 is not a string. Skipping...\n",
      "Sentence at index 3891 is not a string. Skipping...\n",
      "Sentence at index 3893 is not a string. Skipping...\n",
      "Sentence at index 3895 is not a string. Skipping...\n",
      "Sentence at index 3896 is not a string. Skipping...\n",
      "Sentence at index 3905 is not a string. Skipping...\n",
      "Sentence at index 3909 is not a string. Skipping...\n",
      "Sentence at index 3931 is not a string. Skipping...\n",
      "Sentence at index 3938 is not a string. Skipping...\n",
      "Sentence at index 3940 is not a string. Skipping...\n",
      "Sentence at index 3942 is not a string. Skipping...\n",
      "Sentence at index 3943 is not a string. Skipping...\n",
      "Sentence at index 3944 is not a string. Skipping...\n",
      "Sentence at index 3945 is not a string. Skipping...\n",
      "Sentence at index 3949 is not a string. Skipping...\n",
      "Sentence at index 3953 is not a string. Skipping...\n",
      "Sentence at index 3954 is not a string. Skipping...\n",
      "Sentence at index 3956 is not a string. Skipping...\n",
      "Sentence at index 3957 is not a string. Skipping...\n",
      "Sentence at index 3958 is not a string. Skipping...\n",
      "Sentence at index 3959 is not a string. Skipping...\n",
      "Sentence at index 3960 is not a string. Skipping...\n",
      "Sentence at index 3961 is not a string. Skipping...\n",
      "Sentence at index 3962 is not a string. Skipping...\n",
      "Sentence at index 3963 is not a string. Skipping...\n",
      "Sentence at index 3964 is not a string. Skipping...\n",
      "Sentence at index 3965 is not a string. Skipping...\n",
      "Sentence at index 3966 is not a string. Skipping...\n",
      "Sentence at index 3994 is not a string. Skipping...\n",
      "Sentence at index 3995 is not a string. Skipping...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence at index 4003 is not a string. Skipping...\n",
      "Sentence at index 4006 is not a string. Skipping...\n",
      "Sentence at index 4023 is not a string. Skipping...\n",
      "Sentence at index 4045 is not a string. Skipping...\n",
      "Sentence at index 4054 is not a string. Skipping...\n",
      "Sentence at index 4057 is not a string. Skipping...\n",
      "Sentence at index 4140 is not a string. Skipping...\n",
      "Sentence at index 4148 is not a string. Skipping...\n",
      "Sentence at index 4155 is not a string. Skipping...\n",
      "Sentence at index 4161 is not a string. Skipping...\n",
      "Sentence at index 4169 is not a string. Skipping...\n",
      "Sentence at index 4172 is not a string. Skipping...\n",
      "Sentence at index 4173 is not a string. Skipping...\n",
      "Sentence at index 4182 is not a string. Skipping...\n",
      "Sentence at index 4188 is not a string. Skipping...\n",
      "Sentence at index 4219 is not a string. Skipping...\n",
      "Sentence at index 4228 is not a string. Skipping...\n",
      "Sentence at index 4246 is not a string. Skipping...\n",
      "Sentence at index 4247 is not a string. Skipping...\n",
      "Sentence at index 4259 is not a string. Skipping...\n",
      "Sentence at index 4267 is not a string. Skipping...\n",
      "Sentence at index 4271 is not a string. Skipping...\n",
      "Sentence at index 4272 is not a string. Skipping...\n",
      "Sentence at index 4273 is not a string. Skipping...\n",
      "Sentence at index 4274 is not a string. Skipping...\n",
      "Sentence at index 4275 is not a string. Skipping...\n",
      "Sentence at index 4276 is not a string. Skipping...\n",
      "Sentence at index 4277 is not a string. Skipping...\n",
      "Sentence at index 4278 is not a string. Skipping...\n",
      "Sentence at index 4279 is not a string. Skipping...\n",
      "Sentence at index 4280 is not a string. Skipping...\n",
      "Sentence at index 4281 is not a string. Skipping...\n",
      "Sentence at index 4291 is not a string. Skipping...\n",
      "Sentence at index 4293 is not a string. Skipping...\n",
      "Sentence at index 4297 is not a string. Skipping...\n",
      "Sentence at index 4329 is not a string. Skipping...\n",
      "Sentence at index 4336 is not a string. Skipping...\n",
      "Sentence at index 4337 is not a string. Skipping...\n",
      "Sentence at index 4352 is not a string. Skipping...\n",
      "Sentence at index 4353 is not a string. Skipping...\n",
      "Sentence at index 4406 is not a string. Skipping...\n",
      "Sentence at index 4407 is not a string. Skipping...\n",
      "Sentence at index 4424 is not a string. Skipping...\n",
      "Sentence at index 4434 is not a string. Skipping...\n",
      "Sentence at index 4449 is not a string. Skipping...\n",
      "Sentence at index 4459 is not a string. Skipping...\n",
      "Sentence at index 4460 is not a string. Skipping...\n",
      "Sentence at index 4470 is not a string. Skipping...\n",
      "Sentence at index 4477 is not a string. Skipping...\n",
      "Sentence at index 4485 is not a string. Skipping...\n",
      "Sentence at index 4492 is not a string. Skipping...\n",
      "Sentence at index 4495 is not a string. Skipping...\n",
      "Sentence at index 4516 is not a string. Skipping...\n",
      "Sentence at index 4521 is not a string. Skipping...\n",
      "Sentence at index 4525 is not a string. Skipping...\n",
      "Sentence at index 4529 is not a string. Skipping...\n",
      "Sentence at index 4534 is not a string. Skipping...\n",
      "Sentence at index 4539 is not a string. Skipping...\n",
      "Sentence at index 4554 is not a string. Skipping...\n",
      "Sentence at index 4556 is not a string. Skipping...\n",
      "Sentence at index 4557 is not a string. Skipping...\n",
      "Sentence at index 4563 is not a string. Skipping...\n",
      "Sentence at index 4564 is not a string. Skipping...\n",
      "Sentence at index 4568 is not a string. Skipping...\n",
      "Sentence at index 4570 is not a string. Skipping...\n",
      "Sentence at index 4573 is not a string. Skipping...\n",
      "Sentence at index 4575 is not a string. Skipping...\n",
      "Sentence at index 4580 is not a string. Skipping...\n",
      "Sentence at index 4581 is not a string. Skipping...\n",
      "Sentence at index 4585 is not a string. Skipping...\n",
      "Sentence at index 4586 is not a string. Skipping...\n",
      "Sentence at index 4590 is not a string. Skipping...\n",
      "Sentence at index 4592 is not a string. Skipping...\n",
      "Sentence at index 4597 is not a string. Skipping...\n",
      "Sentence at index 4599 is not a string. Skipping...\n",
      "Sentence at index 4623 is not a string. Skipping...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 56\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 56\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m     57\u001b[0m embeddings\u001b[38;5;241m.\u001b[39mappend(outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mflatten())\n\u001b[0;32m     58\u001b[0m hotel_info_list\u001b[38;5;241m.\u001b[39mappend(df_filtered\u001b[38;5;241m.\u001b[39miloc[idx]\u001b[38;5;241m.\u001b[39mto_dict())\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\web_scraping\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\web_scraping\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1013\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1006\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m   1007\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m   1008\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m   1009\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m   1010\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m   1011\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m-> 1013\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1019\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1020\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[0;32m   1021\u001b[0m     embedding_output,\n\u001b[0;32m   1022\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1030\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1031\u001b[0m )\n\u001b[0;32m   1032\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\web_scraping\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\web_scraping\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:235\u001b[0m, in \u001b[0;36mBertEmbeddings.forward\u001b[1;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[0;32m    233\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m inputs_embeds \u001b[38;5;241m+\u001b[39m token_type_embeddings\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabsolute\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 235\u001b[0m     position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    236\u001b[0m     embeddings \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m position_embeddings\n\u001b[0;32m    237\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(embeddings)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\web_scraping\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\web_scraping\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\web_scraping\\lib\\site-packages\\torch\\nn\\functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2204\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2205\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2206\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2207\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2208\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "import datetime\n",
    "\n",
    "df_ = pd.read_csv('./booking_comments_分詞update.csv', header=0)\n",
    "\n",
    "# 篩選評論數大於等於 30 的飯店\n",
    "df_filtered = df_.groupby('飯店名稱').filter(lambda x: len(x) >= 30)\n",
    "\n",
    "# 初始化 RoBERTa-wwm-ext 模型和分詞器\n",
    "tokenizer = BertTokenizer.from_pretrained('hfl/chinese-roberta-wwm-ext')\n",
    "model = BertModel.from_pretrained('hfl/chinese-roberta-wwm-ext')\n",
    "\n",
    "# 定義每個批次的大小\n",
    "batch_size = 10000\n",
    "\n",
    "# 計算需要進行的批次數\n",
    "n_batches = (len(df_filtered) // batch_size) + 1;n_batches\n",
    "\n",
    "# 開始進行批次處理\n",
    "for batch_idx in range(n_batches):\n",
    "    print(f\"Processing batch {batch_idx + 1} of {n_batches}...\")\n",
    "    \n",
    "    # 開始測量(時間)\n",
    "    startime = datetime.datetime.now()\n",
    "    \n",
    "    start_idx = batch_idx * batch_size\n",
    "    end_idx = (batch_idx + 1) * batch_size\n",
    "\n",
    "    # 評論文本\n",
    "    sentences = list(df_filtered.loc[start_idx:end_idx, '正評'])\n",
    "\n",
    "    # 初始化一個空的 list 用於儲存飯店資訊\n",
    "    hotel_info_list = []\n",
    "\n",
    "    # 獲取每個文本的嵌入向量，並儲存對應的飯店資訊\n",
    "    embeddings = []\n",
    "    for idx, sentence in enumerate(sentences):\n",
    "        if isinstance(sentence, str):\n",
    "            tokens = tokenizer.tokenize(sentence)  # 分詞\n",
    "            inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "        else:\n",
    "            print(f\"Sentence at index {idx} is not a string. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # 檢查序列長度是否超過 512\n",
    "        if len(inputs[\"input_ids\"][0]) > 512:\n",
    "            print(\"The sentence is too long. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        embeddings.append(outputs.last_hidden_state.mean(dim=1).numpy().flatten())\n",
    "        hotel_info_list.append(df_filtered.iloc[idx].to_dict())\n",
    "\n",
    "    # 讀取原有的飯店資訊和 embeddings\n",
    "    if os.path.exists('hotel_info_roberta.csv') and os.path.exists('embeddings_roberta.npy'):\n",
    "        df_hotel_info_old = pd.read_csv('hotel_info_roberta.csv')\n",
    "        embeddings_old = np.load('embeddings_roberta.npy')\n",
    "    else:\n",
    "        df_hotel_info_old = pd.DataFrame()\n",
    "        embeddings_old = np.array([]).reshape(0,768)\n",
    "\n",
    "    # 儲存飯店資訊與 embeddings\n",
    "    df_hotel_info_new = pd.DataFrame(hotel_info_list)\n",
    "    df_hotel_info = pd.concat([df_hotel_info_old, df_hotel_info_new], ignore_index=True)\n",
    "    df_hotel_info.to_csv('hotel_info_roberta.csv', index=False)\n",
    "\n",
    "    embeddings_new = np.array(embeddings)\n",
    "    embeddings = np.vstack([embeddings_old, embeddings_new])\n",
    "    np.save('embeddings_roberta.npy', embeddings)\n",
    "    \n",
    "    # 結束測量\n",
    "    endtime = datetime.datetime.now()\n",
    "\n",
    "    # 輸出結果\n",
    "    print(\"執行時間：\", endtime - startime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edceaba2",
   "metadata": {},
   "source": [
    "#### 讀取並進行篩選"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0d1efa98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           相似度\n",
      "飯店名稱                          \n",
      "朝日民宿 - Peng's Family  0.672762\n",
      "亞哥之家                  0.667231\n",
      "九份輝明民宿                0.663794\n",
      "墾丁海園別館Hai Yuan Inn    0.662487\n",
      "百事達國際飯店               0.658002\n"
     ]
    }
   ],
   "source": [
    "def get_similar_hotels(df, embeddings, filter_dict, sentence, top_n):\n",
    "    \n",
    "    # 篩選\n",
    "    for key, value in filter_dict.items():\n",
    "        if key in df.columns:\n",
    "            operation, target_value = value\n",
    "            if operation == \"==\":\n",
    "                df = df[df[key] == target_value]\n",
    "            elif operation == \"!=\":\n",
    "                df = df[df[key] != target_value]\n",
    "            elif operation == \">\":\n",
    "                df = df[df[key] > target_value]\n",
    "            elif operation == \"<\":\n",
    "                df = df[df[key] < target_value]\n",
    "            elif operation == \">=\":\n",
    "                df = df[df[key] >= target_value]\n",
    "            elif operation == \"<=\":\n",
    "                df = df[df[key] <= target_value]\n",
    "    \n",
    "    embeddings_filtered = embeddings[df.index.values]\n",
    "\n",
    "    # 新的文本\n",
    "    tokens = tokenizer.tokenize(sentence)  # 分詞\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    new_embedding = outputs.last_hidden_state.mean(dim=1).numpy().flatten()\n",
    "\n",
    "    # 計算與新文本的相似性\n",
    "    similarities = cosine_similarity(np.vstack((embeddings_filtered, new_embedding)))\n",
    "\n",
    "    # 建立新的 DataFrame，儲存相似度與對應的飯店名稱\n",
    "    df_similarity = pd.DataFrame({'飯店名稱': df['飯店名稱'], '相似度': similarities[-1, :-1]})\n",
    "\n",
    "    # 以飯店名稱分組，計算每間飯店的平均相似度\n",
    "    df_avg_similarity = df_similarity.groupby('飯店名稱').mean()\n",
    "\n",
    "    # 取出平均相似度最高的前 top_n 間飯店\n",
    "    top_hotels = df_avg_similarity.sort_values(by='相似度', ascending=False).head(top_n)\n",
    "    \n",
    "    return top_hotels\n",
    "\n",
    "# 讀取 DataFrame\n",
    "df_hotel_info = pd.read_csv('hotel_info_roberta.csv')\n",
    "\n",
    "# 讀取 embeddings\n",
    "embeddings = np.load('embeddings_roberta.npy')\n",
    "\n",
    "# 建立篩選條件字典\n",
    "filter_dict = {'整體評分': (\">\", 7.0)}\n",
    "\n",
    "# 使用函數\n",
    "top_hotels = get_similar_hotels(df_hotel_info, embeddings, filter_dict, \"房間好\", 5)\n",
    "print(top_hotels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aad9583",
   "metadata": {},
   "source": [
    "### 測試自行斷詞後的結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45b12aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('0_10000.csv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c19b6062",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-roberta-wwm-ext were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n",
      "相似的文本 (索引 9302): ['房間', '小'], 相似度: 0.9714384078979492\n",
      "相似的文本 (索引 7713): ['房間', '算', '大'], 相似度: 0.9639410376548767\n",
      "相似的文本 (索引 6559): ['房間', '很', '大'], 相似度: 0.9479893445968628\n",
      "相似的文本 (索引 6710): ['房間', '很', '大'], 相似度: 0.9479893445968628\n",
      "相似的文本 (索引 6730): ['房間', '很', '大'], 相似度: 0.9479893445968628\n",
      "相似的文本 (索引 3851): ['房間', '很', '大'], 相似度: 0.9479893445968628\n",
      "相似的文本 (索引 7572): ['房間', '很', '大'], 相似度: 0.9479893445968628\n",
      "相似的文本 (索引 6796): ['房間', '很', '大'], 相似度: 0.9479893445968628\n",
      "相似的文本 (索引 9294): ['房間', '偏', '小'], 相似度: 0.927994966506958\n",
      "相似的文本 (索引 4188): ['房間', '稍', '小'], 相似度: 0.9203976392745972\n",
      "相似的文本 (索引 6951): ['乾淨', '房間', '大'], 相似度: 0.9122157096862793\n",
      "相似的文本 (索引 1642): ['房間', '太', '小'], 相似度: 0.900887131690979\n",
      "相似的文本 (索引 7985): ['房間', '大小'], 相似度: 0.8921487331390381\n",
      "相似的文本 (索引 1403): ['房間', '大小'], 相似度: 0.8921487331390381\n",
      "相似的文本 (索引 8195): ['房間', '大小'], 相似度: 0.8921487331390381\n",
      "相似的文本 (索引 6233): ['便宜', '房間', '大'], 相似度: 0.8913284540176392\n",
      "相似的文本 (索引 8610): ['房間', '小', '床', '很', '小'], 相似度: 0.8826092481613159\n",
      "相似的文本 (索引 774): ['房間', '很', '棒'], 相似度: 0.8820592164993286\n",
      "相似的文本 (索引 801): ['房間', '寬敞'], 相似度: 0.8808667659759521\n",
      "相似的文本 (索引 7569): ['房間', '乾淨', '和', '大'], 相似度: 0.8764951825141907\n",
      "相似的文本 (索引 992): ['房間', '浴室', '都', '很', '大'], 相似度: 0.8760460615158081\n",
      "相似的文本 (索引 6389): ['房間', '大', '舒適', '無'], 相似度: 0.8751977682113647\n",
      "相似的文本 (索引 436): ['房間', '很', '大', '太', '冷'], 相似度: 0.8732443451881409\n",
      "相似的文本 (索引 7235): ['房間', '大', '舒適'], 相似度: 0.8698259592056274\n",
      "相似的文本 (索引 7696): ['房間', '超大', '寬敞'], 相似度: 0.8666406273841858\n",
      "相似的文本 (索引 825): ['房間', '寬敞', '無'], 相似度: 0.8617373108863831\n",
      "相似的文本 (索引 7113): ['房間', '夠', '大', '舒服'], 相似度: 0.8598159551620483\n",
      "相似的文本 (索引 4631): ['沒', '窗戶', '房間', '小'], 相似度: 0.8595103621482849\n",
      "相似的文本 (索引 5244): ['房間', '格局', '很', '大', '舒適'], 相似度: 0.8584761619567871\n",
      "相似的文本 (索引 659): ['房', '內', '整體'], 相似度: 0.8537802696228027\n",
      "相似的文本 (索引 6986): ['房間', '格局'], 相似度: 0.8535634279251099\n",
      "相似的文本 (索引 5326): ['乾凈', '舒適', '房間', '大'], 相似度: 0.8530415296554565\n",
      "相似的文本 (索引 4075): ['房間', '不錯'], 相似度: 0.8530404567718506\n",
      "相似的文本 (索引 7602): ['床', '夠', '大'], 相似度: 0.8511996269226074\n",
      "相似的文本 (索引 8480): ['床', '夠', '大'], 相似度: 0.8511996269226074\n",
      "相似的文本 (索引 7678): ['床', '夠', '大'], 相似度: 0.8511996269226074\n",
      "相似的文本 (索引 2697): ['浴室', '大'], 相似度: 0.8497729301452637\n",
      "相似的文本 (索引 5879): ['都', '還', '可', '床', '與', '房間', '大小'], 相似度: 0.8483333587646484\n",
      "相似的文本 (索引 2580): ['房間', '大', '沒', '壓迫感'], 相似度: 0.8476015329360962\n",
      "相似的文本 (索引 4699): ['房間', '設施', '有', '蟲'], 相似度: 0.8450119495391846\n",
      "相似的文本 (索引 1810): ['車位', '不', '夠', '房間', '偏', '暗'], 相似度: 0.8447525501251221\n",
      "相似的文本 (索引 3250): ['空間', '小'], 相似度: 0.8442726731300354\n",
      "相似的文本 (索引 647): ['房', '內', '空間', '不', '太', '大'], 相似度: 0.8430776000022888\n",
      "相似的文本 (索引 7130): ['房間', '很', '大', '很', '寬敞'], 相似度: 0.842853307723999\n",
      "相似的文本 (索引 6667): ['浴室', '超大', '沒有'], 相似度: 0.840947687625885\n",
      "相似的文本 (索引 7133): ['床', '很', '大'], 相似度: 0.8402475714683533\n",
      "相似的文本 (索引 8936): ['房間', '隔音', '差'], 相似度: 0.8391607999801636\n",
      "相似的文本 (索引 860): ['房間', '很', '大', '床組', '很', '好', '睡'], 相似度: 0.837617039680481\n",
      "相似的文本 (索引 1551): ['床', '軟', '好', '睡', '房間', '有點', '小'], 相似度: 0.8361782431602478\n",
      "相似的文本 (索引 5521): ['人房', '空間', '大', '床', '好', '睡'], 相似度: 0.8358633518218994\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('hfl/chinese-roberta-wwm-ext')\n",
    "model = BertModel.from_pretrained('hfl/chinese-roberta-wwm-ext')\n",
    "\n",
    "sentences_tokenized = [eval(d) for d in df.loc[:, \"綜合評論_ws\"].values]\n",
    "\n",
    "# 獲取每個文本的嵌入向量\n",
    "embeddings = []\n",
    "sentences = []  # 新建一個列表來保存被處理的句子\n",
    "\n",
    "for sentence in sentences_tokenized:\n",
    "    \n",
    "    inputs = tokenizer.encode_plus(sentence, is_split_into_words=True, return_tensors=\"pt\")\n",
    "    \n",
    "    # 檢查序列長度是否超過 512\n",
    "    if len(inputs[\"input_ids\"][0]) > 512:\n",
    "        print(\"The sentence is too long. Skipping...\")\n",
    "        continue\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings.append(outputs.last_hidden_state.mean(dim = 1).numpy().flatten())\n",
    "    sentences.append(sentence)  # 將被處理的句子加到列表中\n",
    "\n",
    "# 新的文本\n",
    "new_sentence = \"房間大\"\n",
    "new_tokens = ['房間', '大'] # 分詞\n",
    "inputs = tokenizer(new_tokens, is_split_into_words=True, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "new_embedding = outputs.last_hidden_state.mean(dim=1).numpy().flatten()\n",
    "\n",
    "# 計算與新文本的相似性\n",
    "similarities = cosine_similarity(embeddings + [new_embedding])\n",
    "\n",
    "# 獲取最相似的 50 個文本的索引\n",
    "top50_indices = np.argsort(similarities[-1][:-1])[:-51:-1]\n",
    "\n",
    "# 印出最相似的 50 個文本\n",
    "for i, index in enumerate(top50_indices):\n",
    "    print(f\"相似的文本 (索引 {index}): {sentences[index]}, 相似度: {similarities[-1][index]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca29473a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "相似的文本 (索引 4343): ['早餐', '好吃'], 相似度: 1.000000238418579\n",
      "相似的文本 (索引 1529): ['早餐', '好吃'], 相似度: 1.000000238418579\n",
      "相似的文本 (索引 6567): ['早餐', '好吃'], 相似度: 1.000000238418579\n",
      "相似的文本 (索引 7509): ['早餐', '好吃'], 相似度: 1.000000238418579\n",
      "相似的文本 (索引 4348): ['早餐', '好吃'], 相似度: 1.000000238418579\n",
      "相似的文本 (索引 6958): ['早餐', '好吃'], 相似度: 1.000000238418579\n",
      "相似的文本 (索引 6491): ['早餐', '好吃'], 相似度: 1.000000238418579\n",
      "相似的文本 (索引 4098): ['早餐', '好吃'], 相似度: 1.000000238418579\n",
      "相似的文本 (索引 2429): ['早餐', '好吃'], 相似度: 1.000000238418579\n",
      "相似的文本 (索引 4827): ['早餐', '好吃'], 相似度: 1.000000238418579\n",
      "相似的文本 (索引 4190): ['早餐', '很', '好吃'], 相似度: 0.9652129411697388\n",
      "相似的文本 (索引 6052): ['早餐', '很', '好吃'], 相似度: 0.9652129411697388\n",
      "相似的文本 (索引 5181): ['早餐', '很', '好吃'], 相似度: 0.9652129411697388\n",
      "相似的文本 (索引 6429): ['早餐', '很', '好吃'], 相似度: 0.9652129411697388\n",
      "相似的文本 (索引 4300): ['早餐', '很', '好吃'], 相似度: 0.9652129411697388\n",
      "相似的文本 (索引 9561): ['早餐', '超', '好吃'], 相似度: 0.9607133865356445\n",
      "相似的文本 (索引 1245): ['早餐', '不', '好吃'], 相似度: 0.9587073922157288\n",
      "相似的文本 (索引 9445): ['早餐', '很', '好吃', '呦'], 相似度: 0.9332779049873352\n",
      "相似的文本 (索引 535): ['早餐', '很', '好'], 相似度: 0.9180421829223633\n",
      "相似的文本 (索引 1106): ['乾淨', '早餐', '好吃'], 相似度: 0.9138054847717285\n",
      "相似的文本 (索引 4362): ['早餐', '很', '棒'], 相似度: 0.9111084938049316\n",
      "相似的文本 (索引 4333): ['早餐', '很', '棒'], 相似度: 0.9111084938049316\n",
      "相似的文本 (索引 1109): ['早餐', '很', '棒'], 相似度: 0.9111084938049316\n",
      "相似的文本 (索引 599): ['早餐', '很', '棒'], 相似度: 0.9111084938049316\n",
      "相似的文本 (索引 9901): ['早餐', '很', '棒'], 相似度: 0.9111084938049316\n",
      "相似的文本 (索引 4880): ['早餐', '很', '棒'], 相似度: 0.9111084938049316\n",
      "相似的文本 (索引 741): ['早餐', '很', '棒'], 相似度: 0.9111084938049316\n",
      "相似的文本 (索引 4101): ['早餐', '很', '棒'], 相似度: 0.9111084938049316\n",
      "相似的文本 (索引 5153): ['早餐', '豐盛', '好吃'], 相似度: 0.909956693649292\n",
      "相似的文本 (索引 6007): ['早餐', '中西式', '都', '好吃'], 相似度: 0.9071521162986755\n",
      "相似的文本 (索引 4192): ['現', '煮', '早餐', '好好', '吃'], 相似度: 0.9032139778137207\n",
      "相似的文本 (索引 4140): ['早餐', '超棒'], 相似度: 0.9013780951499939\n",
      "相似的文本 (索引 1554): ['早餐', '好吃', '沒有'], 相似度: 0.9006266593933105\n",
      "相似的文本 (索引 7478): ['早餐', '好吃', '位置', '很', '好'], 相似度: 0.8989447951316833\n",
      "相似的文本 (索引 129): ['很', '棒', '的', '早餐'], 相似度: 0.8969095945358276\n",
      "相似的文本 (索引 6409): ['早餐', '豐盛', '好吃', '無'], 相似度: 0.8909722566604614\n",
      "相似的文本 (索引 9638): ['早餐', '很', '好吃', '很', '飽'], 相似度: 0.8906499743461609\n",
      "相似的文本 (索引 1098): ['早餐', '豐富', '美味'], 相似度: 0.8905787467956543\n",
      "相似的文本 (索引 4414): ['早餐', '好吃', '很', '豐盛'], 相似度: 0.8902044296264648\n",
      "相似的文本 (索引 4358): ['健康', '早餐', '很', '棒'], 相似度: 0.8896884322166443\n",
      "相似的文本 (索引 2910): ['早餐', '好吃值', '頗', '高'], 相似度: 0.8879324197769165\n",
      "相似的文本 (索引 4216): ['早餐', '很', '好吃', '超棒', '的'], 相似度: 0.8856137990951538\n",
      "相似的文本 (索引 7349): ['早餐', '很', '豐富'], 相似度: 0.8758634924888611\n",
      "相似的文本 (索引 1116): ['早餐', '超級', '好'], 相似度: 0.8735265731811523\n",
      "相似的文本 (索引 805): ['早餐', '非常', '棒'], 相似度: 0.8723956346511841\n",
      "相似的文本 (索引 4210): ['早餐', '非常', '棒'], 相似度: 0.8723956346511841\n",
      "相似的文本 (索引 1820): ['早餐', '非常', '不', '可口'], 相似度: 0.8720617890357971\n",
      "相似的文本 (索引 4296): ['豐盛', '的', '早餐'], 相似度: 0.8715789318084717\n",
      "相似的文本 (索引 757): ['早餐', '很', '豐盛'], 相似度: 0.8710860013961792\n",
      "相似的文本 (索引 8078): ['早餐', '很', '豐盛'], 相似度: 0.8710860013961792\n"
     ]
    }
   ],
   "source": [
    "# 新的文本\n",
    "new_sentence = \"早餐好吃\"\n",
    "new_tokens = ['早餐', '好吃'] # 分詞\n",
    "inputs = tokenizer(new_tokens, is_split_into_words=True, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "new_embedding = outputs.last_hidden_state.mean(dim=1).numpy().flatten()\n",
    "\n",
    "# 計算與新文本的相似性\n",
    "similarities = cosine_similarity(embeddings + [new_embedding])\n",
    "\n",
    "# 獲取最相似的 50 個文本的索引\n",
    "top50_indices = np.argsort(similarities[-1][:-1])[:-51:-1]\n",
    "\n",
    "# 印出最相似的 50 個文本\n",
    "for i, index in enumerate(top50_indices):\n",
    "    print(f\"相似的文本 (索引 {index}): {sentences[index]}, 相似度: {similarities[-1][index]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acea954e",
   "metadata": {},
   "source": [
    "##### 結果不是很好\n",
    "\n",
    "##### 無法有效判斷語意的相似度\n",
    "\n",
    "##### 這個現象的出現是由於您在計算評論與新評論之間的相似度時，採用了cosine similarity。這種相似度衡量的是兩個向量之間的角度，即它們的方向是否相似，而不是他們的長度或大小\n",
    "##### 如果您希望找到與新評論在語義上更相似的評論，一種可能的方法是使用一種能夠捕捉語義相似度的相似度度量，比如Word Mover's Distance。Word Mover's Distance是一種在詞嵌入空間中度量文本之間距離的方法，它可以捕捉詞與詞之間的相似性，並利用這種相似性來度量文本之間的距離。\n",
    "\n",
    "##### 另一種可能的方法是，不僅僅依賴於評論的平均詞向量來表示評論。例如，您可以使用一些更複雜的方法來獲得評論的向量表示，比如使用Doc2Vec模型，或者將BERT模型中不同層的輸出合併起來，這可能會獲得更豐富的表示。這樣，即使評論的長度不同，也能得到更好的結果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483fdb21",
   "metadata": {},
   "source": [
    "#### 使用Gensim的Doc2Vec模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d22f35c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1 相似的文本 (索引 [283]): ['住', '頂樓', '海景房', '真的', '是', '很', '賭', '運氣', '跟', '天氣', '的', '房型', '如果', '天氣', '好', '無敵', '海景', '第一', '排', '真的', '很', '美', '可惜', '當天', '入住', '外圍', '環流', '下雨天', '天氣', '非常', '糟糕', '也', '因為', '下雨天', '這', '間', '房型', '的', '缺點', '完全', '顯現', '雨水', '打', '在', '頂樓', '鐵皮屋', '聲音', '非常', '大', '其實', '外面', '沒有', '到', '很', '大', '的', '雨', '但', '在', '房', '內', '卻', '感覺', '是', '滂沱大雨', '整', '晚', '睡', '不', '好', '房', '內', '有', '小', '蟲', '床', '上', '有', '螞蟻', '不', '知道', '是', '不', '是', '因為', '下雨', '所以', '都', '跑出來', '了', '是', '我們', '自己', '運氣', '不', '好', '遇到', '雨天', '所以', '雨天', '在', '這', '間', '房', '內', '就', '變', '的', '比', '其它', '房型', '值', '還要', '低', '雖然', '隔天', '早上', '雨', '停', '了', '但', '照片', '都', '是', '黃黃', '的', '海水', '很', '可惜'], 相似度: 0.13586002588272095\n",
      "第 2 相似的文本 (索引 [83]): ['入住', '三', '次', '了', '每', '次', '都', '是', '兩', '晚', '以上', '覺得', '不錯', '房間', '細節', '的', '地方', '可以', '在', '加強', '喔'], 相似度: 0.12522174417972565\n",
      "第 3 相似的文本 (索引 [123]): ['整體', '還', '不錯', '床', '很', '舒服', '整體', '配置', '還', '不錯', '這', '次', '住', '的', '是', '四', '人', '房', '上下舖', '但是', '很', '舒適', '床位', '都', '有', '充電孔', '跟', '檯燈', '位置', '有點', '難找', '四', '人', '房', '空間', '略', '小', '了', '點'], 相似度: 0.12299995124340057\n",
      "第 4 相似的文本 (索引 [239]): ['水龍頭', '水量', '小', '了', '點', '若', '有', '早餐', '更', '讚'], 相似度: 0.12274613231420517\n",
      "第 5 相似的文本 (索引 [125]): ['服務', '很', '親切', '老闆娘', '服務', '態度', '很', '好', '讓', '人', '很', '放鬆', '的', '一', '家', '旅店'], 相似度: 0.120293527841568\n",
      "第 6 相似的文本 (索引 [224]): ['老闆', '待人', '親切', '早餐', '很', '用心', '在', '準備', '怕', '我們', '吃', '不', '飽', '很', '沒有'], 相似度: 0.1197710707783699\n",
      "第 7 相似的文本 (索引 [116]): ['房間', '寬敞', '又', '乾淨', '且', '有', '三溫暖', '適合', '紓壓', '的', '好', '飯店', '泳池', '若是', '室', '內', '就', '更', '棒', '了'], 相似度: 0.1191958412528038\n",
      "第 8 相似的文本 (索引 [189]): ['可以'], 相似度: 0.11368517577648163\n",
      "第 9 相似的文本 (索引 [43]): ['很', '雷', '的', '一', '次', '住宿', '體驗', '電梯', '進入', '會', '下沉', '電梯', '地板', '還', '會', '有', '異聲', '服務', '人員', '很', '客氣', '水壓', '不', '夠', '洗澡', '洗', '半', '會', '完全', '沒', '水', '只', '能', '乾', '等', '水', '蓮蓬頭', '壞', '了', '也', '沒有', '維護', '房', '內', '床舖', '很', '潮濕', '朋友', '的', '床位', '還', '有', '蟲'], 相似度: 0.1134708896279335\n",
      "第 10 相似的文本 (索引 [179]): ['整體', '感受', '還', '不錯', '服務', '態度', '電梯', '有', '卡片', '限制', '出入口', '有', '人', '管制', '安全性', '高', '飯店', '內', '盡然', '沒有', '洗衣間', '雖然', '有', '代洗', '服務', '但', '要', '隔天', '快', '中午', '才', '能', '回來'], 相似度: 0.11303973197937012\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# 分詞後評論\n",
    "sentences_tokenized = [eval(d) for d in df.loc[:, \"綜合評論_ws\"].values]\n",
    "\n",
    "# 準備訓練數據(轉成模型會吃的樣子)\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(sentences_tokenized)]\n",
    "\n",
    "# 訓練 Doc2Vec 模型\n",
    "model = Doc2Vec(documents, vector_size=50, min_count=2, epochs=40)\n",
    "\n",
    "# 现在，我们可以用模型获取句子的向量表示\n",
    "embeddings = [model.infer_vector(doc) for doc in sentences_tokenized]\n",
    "\n",
    "# 对新的句子执行相同的操作\n",
    "new_sentence = \"早餐不好\"\n",
    "new_embedding = model.infer_vector(new_sentence.split())\n",
    "\n",
    "# 将嵌入向量列表转换为 numpy 数组，以便我们可以使用 cosine_similarity 函数\n",
    "embeddings_np = np.array(embeddings)\n",
    "\n",
    "# 计算新的句子和已有句子的相似度\n",
    "similarities = cosine_similarity(embeddings_np, new_embedding.reshape(1, -1))\n",
    "\n",
    "# 以降序排列相似度\n",
    "top_indices = np.argsort(similarities, axis=0)[::-1]\n",
    "\n",
    "# 输出最相似的 10 个句子\n",
    "for i, index in enumerate(top_indices[:10]):\n",
    "    print(f\"第 {i+1} 相似的文本 (索引 {index}): {sentences_tokenized[index[0]]}, 相似度: {similarities[index[0]][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b1ae46",
   "metadata": {},
   "source": [
    "#### 使用BERT的不同層输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fee133fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_tokenized_ = [\" \".join(eval(d)) for d in df_.loc[0:50, \"綜合評論_ws\"].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8c622770",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-roberta-wwm-ext were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1 相似的文本 (索引 [45]): 早餐 超 豐盛 超, 相似度: 0.6999987363815308\n",
      "第 2 相似的文本 (索引 [3]): 沒有 附 早餐, 相似度: 0.6805572509765625\n",
      "第 3 相似的文本 (索引 [22]): 滿意, 相似度: 0.5698429346084595\n",
      "第 4 相似的文本 (索引 [23]): 沒有 髒, 相似度: 0.558742105960846\n",
      "第 5 相似的文本 (索引 [35]): 地 上 有 頭髮 不 夠 乾淨, 相似度: 0.5520568490028381\n",
      "第 6 相似的文本 (索引 [39]): 房間 升級 房間 及 衛浴 空間 舒適 早餐 每 天 都 是 培根 起司 口味, 相似度: 0.5247654914855957\n",
      "第 7 相似的文本 (索引 [40]): 草地 夠 大 風 好 大, 相似度: 0.5237064957618713\n",
      "第 8 相似的文本 (索引 [17]): 地點 方便 停車位 多 早餐 也 很 不錯 補菜 的 速度 也 很 快 早餐 沒有 醬瓜 房間 太 昏暗, 相似度: 0.5115199089050293\n",
      "第 9 相似的文本 (索引 [1]): 整體 上 都 不錯 無 飲水機 有點 不 方便, 相似度: 0.5094053745269775\n",
      "第 10 相似的文本 (索引 [8]): 有 桌遊 可以 玩, 相似度: 0.5080175399780273\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('hfl/chinese-roberta-wwm-ext')\n",
    "model = BertModel.from_pretrained('hfl/chinese-roberta-wwm-ext', output_hidden_states=True)\n",
    "\n",
    "embeddings = []\n",
    "for sentence in sentences_tokenized_:\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # 获取最后四层的输出\n",
    "    last_four_layers = outputs.hidden_states[-4:]\n",
    "    # 将它们合并起来\n",
    "    embedding = torch.mean(torch.stack(last_four_layers), dim=0).mean(dim=1).numpy()\n",
    "    embeddings.append(embedding.squeeze())  # 添加这个改动\n",
    "\n",
    "new_tokens = ['早餐', '不好'] # 分詞\n",
    "inputs = tokenizer(new_tokens, is_split_into_words=True, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "last_four_layers = outputs.hidden_states[-4:]\n",
    "new_embedding = torch.mean(torch.stack(last_four_layers), dim=0).mean(dim=1).numpy()\n",
    "\n",
    "embeddings_np = np.array(embeddings)\n",
    "\n",
    "similarities = cosine_similarity(embeddings_np, new_embedding.reshape(1, -1))\n",
    "top_indices = np.argsort(similarities, axis=0)[::-1]\n",
    "\n",
    "for i, index in enumerate(top_indices[:10]):\n",
    "    print(f\"第 {i+1} 相似的文本 (索引 {index}): {sentences_tokenized_[index[0]]}, 相似度: {similarities[index[0]][0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec0272d",
   "metadata": {},
   "source": [
    "### 觀察自行分詞與bert內建分詞的差異"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5d4c0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['房', '間', '小']\n",
      "{'input_ids': tensor([[ 101, 2791,  102],\n",
      "        [ 101, 7279,  102],\n",
      "        [ 101, 2207,  102]]), 'token_type_ids': tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1]])}\n",
      "['房間', '小']\n",
      "{'input_ids': tensor([[ 101, 2791, 7279, 2207,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "new_sentence = \"房間小\"\n",
    "\n",
    "# inputs = tokenizer(new_sentence, return_tensors=\"pt\")\n",
    "new_tokens = tokenizer.tokenize(new_sentence)\n",
    "inputs = tokenizer(new_tokens, return_tensors=\"pt\")\n",
    "\n",
    "print(new_tokens)\n",
    "print(inputs)\n",
    "\n",
    "new_tokens = ['房間', '小'] # 分詞\n",
    "inputs = tokenizer(new_tokens, is_split_into_words=True, return_tensors=\"pt\")\n",
    "\n",
    "print(new_tokens)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9b1224",
   "metadata": {},
   "source": [
    "#### 嘗試只保留有意義的詞性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36443dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('0_10000.csv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2676eaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_pos = set(['Nep', 'Nh', 'Nb'])  # 這 3 種詞性不保留\n",
    "\n",
    "# 分詞後評論\n",
    "sentences_tokenized = [eval(d) for d in df.loc[:, \"綜合評論_ws\"].values]\n",
    "\n",
    "# 分詞後詞性\n",
    "pos_tags = [eval(d) for d in df.loc[:, \"綜合評論_pos\"].values]\n",
    "\n",
    "short_sentence = []  # 放過濾後的句子\n",
    "\n",
    "for sentence, sentence_pos_tags in zip(sentences_tokenized, pos_tags):\n",
    "\n",
    "    short_sentence_sentence = []  # 存储这个句子过滤后的词（不带词性）\n",
    "    \n",
    "    for word_ws, word_pos in zip(sentence, sentence_pos_tags):\n",
    "        \n",
    "        # 只留名詞和動詞\n",
    "        is_N_or_V = word_pos.startswith(\"V\") or word_pos.startswith(\"N\")\n",
    "\n",
    "        # 去掉名詞裡的某些詞性\n",
    "        is_not_stop_pos = word_pos not in stop_pos\n",
    "\n",
    "        if is_N_or_V and is_not_stop_pos:\n",
    "            \n",
    "            short_sentence_sentence.append(f\"{word_ws}\")\n",
    "\n",
    "    short_sentence.append(short_sentence_sentence)\n",
    "\n",
    "# print(sentences_tokenized[0:3])\n",
    "# print(short_sentence[0:3])\n",
    "\n",
    "# 去除空的列表\n",
    "short_sentence = [sentence for sentence in short_sentence if sentence]  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a0feb1",
   "metadata": {},
   "source": [
    "### 利用篩選詞性後的結果做 roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9ebfee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-roberta-wwm-ext were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentence is too long. Skipping...\n",
      "相似的文本 (索引 3834): ['房間', '大'], 相似度: 0.9999998211860657\n",
      "相似的文本 (索引 6677): ['房間', '大'], 相似度: 0.9999998211860657\n",
      "相似的文本 (索引 7534): ['房間', '大'], 相似度: 0.9999998211860657\n",
      "相似的文本 (索引 6762): ['房間', '大'], 相似度: 0.9999998211860657\n",
      "相似的文本 (索引 6697): ['房間', '大'], 相似度: 0.9999998211860657\n",
      "相似的文本 (索引 6527): ['房間', '大'], 相似度: 0.9999998211860657\n",
      "相似的文本 (索引 4170): ['房間', '小'], 相似度: 0.9714384078979492\n",
      "相似的文本 (索引 9246): ['房間', '小'], 相似度: 0.9714384078979492\n",
      "相似的文本 (索引 1637): ['房間', '小'], 相似度: 0.9714384078979492\n",
      "相似的文本 (索引 7674): ['房間', '算', '大'], 相似度: 0.9639410376548767\n",
      "相似的文本 (索引 8884): ['房間', '差'], 相似度: 0.9296784996986389\n",
      "相似的文本 (索引 9238): ['房間', '偏', '小'], 相似度: 0.927994966506958\n",
      "相似的文本 (索引 989): ['房間', '浴室', '大'], 相似度: 0.9196709394454956\n",
      "相似的文本 (索引 8559): ['房間', '小', '床', '小'], 相似度: 0.9166770577430725\n",
      "相似的文本 (索引 6916): ['乾淨', '房間', '大'], 相似度: 0.9122157096862793\n",
      "相似的文本 (索引 7094): ['房間', '大', '寬敞'], 相似度: 0.9050126075744629\n",
      "相似的文本 (索引 5854): ['床', '房間', '大小'], 相似度: 0.8997808694839478\n",
      "相似的文本 (索引 7943): ['房間', '大小'], 相似度: 0.8921487331390381\n",
      "相似的文本 (索引 1398): ['房間', '大小'], 相似度: 0.8921487331390381\n",
      "相似的文本 (索引 8146): ['房間', '大小'], 相似度: 0.8921487331390381\n",
      "相似的文本 (索引 6206): ['便宜', '房間', '大'], 相似度: 0.8913284540176392\n",
      "相似的文本 (索引 771): ['房間', '棒'], 相似度: 0.8889296054840088\n",
      "相似的文本 (索引 6279): ['寬敞', '房間', '好'], 相似度: 0.886948823928833\n",
      "相似的文本 (索引 435): ['房間', '大', '冷'], 相似度: 0.8835815787315369\n",
      "相似的文本 (索引 4534): ['房間', '大', '舒服'], 相似度: 0.8827222585678101\n",
      "相似的文本 (索引 7077): ['房間', '大', '舒服'], 相似度: 0.8827222585678101\n",
      "相似的文本 (索引 7657): ['房間', '寬敞'], 相似度: 0.8808667659759521\n",
      "相似的文本 (索引 798): ['房間', '寬敞'], 相似度: 0.8808667659759521\n",
      "相似的文本 (索引 646): ['房', '內', '空間', '大'], 相似度: 0.880531907081604\n",
      "相似的文本 (索引 6360): ['房間', '大', '舒適', '無'], 相似度: 0.8751977682113647\n",
      "相似的文本 (索引 6103): ['浴室', '大', '房間', '小', '差'], 相似度: 0.8750342726707458\n",
      "相似的文本 (索引 6720): ['房間', '吵', '差'], 相似度: 0.8724051713943481\n",
      "相似的文本 (索引 4694): ['陽台', '棒', '房間', '小'], 相似度: 0.8704512119293213\n",
      "相似的文本 (索引 7814): ['房間', '大', '舒適'], 相似度: 0.8698259592056274\n",
      "相似的文本 (索引 8387): ['房間', '大', '舒適'], 相似度: 0.8698259592056274\n",
      "相似的文本 (索引 7198): ['房間', '大', '舒適'], 相似度: 0.8698259592056274\n",
      "相似的文本 (索引 7531): ['房間', '乾淨', '大'], 相似度: 0.8679437637329102\n",
      "相似的文本 (索引 6196): ['房間', '燈', '暗'], 相似度: 0.8660553693771362\n",
      "相似的文本 (索引 492): ['浴室', '大', '寬敞', '無'], 相似度: 0.8654581904411316\n",
      "相似的文本 (索引 8710): ['房間', '大', '舒適', '沒有'], 相似度: 0.8630586266517639\n",
      "相似的文本 (索引 8040): ['房間', '空間', '大', '住', '舒服'], 相似度: 0.8618459701538086\n",
      "相似的文本 (索引 822): ['房間', '寬敞', '無'], 相似度: 0.8617373108863831\n",
      "相似的文本 (索引 597): ['房間', '舒服', '好'], 相似度: 0.8612924814224243\n",
      "相似的文本 (索引 4612): ['沒', '窗戶', '房間', '小'], 相似度: 0.8595103621482849\n",
      "相似的文本 (索引 1805): ['車位', '夠', '房間', '偏', '暗'], 相似度: 0.858141303062439\n",
      "相似的文本 (索引 7589): ['房間', '不錯', '睡'], 相似度: 0.8581225275993347\n",
      "相似的文本 (索引 1594): ['房間', '舒適', '差'], 相似度: 0.8569594621658325\n",
      "相似的文本 (索引 8981): ['房間', '海景', '棒'], 相似度: 0.854246973991394\n",
      "相似的文本 (索引 658): ['房', '內', '整體'], 相似度: 0.8537802696228027\n",
      "相似的文本 (索引 6950): ['房間', '格局'], 相似度: 0.8535634279251099\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('hfl/chinese-roberta-wwm-ext')\n",
    "model = BertModel.from_pretrained('hfl/chinese-roberta-wwm-ext')\n",
    "\n",
    "# 獲取每個文本的嵌入向量\n",
    "embeddings = []\n",
    "sentences = []  # 新建一個列表來保存被處理的句子\n",
    "\n",
    "for sentence in short_sentence:\n",
    "    \n",
    "    inputs = tokenizer.encode_plus(sentence, is_split_into_words=True, return_tensors=\"pt\")\n",
    "    \n",
    "    # 檢查序列長度是否超過 512\n",
    "    if len(inputs[\"input_ids\"][0]) > 512:\n",
    "        print(\"The sentence is too long. Skipping...\")\n",
    "        continue\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings.append(outputs.last_hidden_state.mean(dim = 1).numpy().flatten())\n",
    "    sentences.append(sentence)  # 將被處理的句子加到列表中\n",
    "\n",
    "# 新的文本\n",
    "new_sentence = \"房間大\"\n",
    "new_tokens = ['房間', '大'] # 分詞\n",
    "inputs = tokenizer(new_tokens, is_split_into_words=True, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "new_embedding = outputs.last_hidden_state.mean(dim=1).numpy().flatten()\n",
    "\n",
    "# 計算與新文本的相似性\n",
    "similarities = cosine_similarity(embeddings + [new_embedding])\n",
    "\n",
    "# 獲取最相似的 50 個文本的索引\n",
    "top50_indices = np.argsort(similarities[-1][:-1])[:-51:-1]\n",
    "\n",
    "# 印出最相似的 50 個文本\n",
    "for i, index in enumerate(top50_indices):\n",
    "    print(f\"相似的文本 (索引 {index}): {sentences[index]}, 相似度: {similarities[-1][index]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33adf89",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "697e1cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "飯店名稱：陽光滿屋民宿\n",
      "平均相似度:0.11570316506156822\n",
      "前10條相似的評論:\n",
      "評論：舒適 房間，相似度：0.5655955612619636\n",
      "評論：乾淨 舒適 房間，相似度：0.4529094419157692\n",
      "評論：推薦 房間 乾淨 不錯 房間 內 無 冰箱 有 共用，相似度：0.3734171246078148\n",
      "評論：房間 乾淨 看 出 屋主 用心，相似度：0.22074116484931985\n",
      "評論：整體 棒 地點 不錯 大 片 窗戶 外面 有 連結感 地板 乾淨 熱水 燙 水壓 足 沒 暖氣 訂 個 房間 牆壁 上 有 幅 畫 房間 陽光感 風格 不符 感覺 看 久 頭 暈 正面 床 難 看到，相似度：0.1696819678971862\n",
      "評論：陽光 地方 房間 不錯 熱水 熱 晚上 安靜 有 小 螞蟻，相似度：0.15946363273550007\n",
      "評論：民宿 地點 活水湖 近 車程 五 分鐘 無 市區 鬧區 噪音 老闆 熱心 解答 任何 問題 房間 整潔 乾淨 希望 有 含 早餐 退房 時間 早點 房間 冷氣 涼 需要 設定到 低 溫度，相似度：0.15127160639049955\n",
      "評論：海岸 公園 近 民宿 房間 內 看見 海 民宿 地點 不錯 安靜 停車 方便 進門處 鞋子 沒 地方 擺 顯 亂 房間 垃圾桶 裏 遺留 上 個 房客 泡麵碗 洗手台 螞蟻 一 大 堆 腳 撞 狂 床框，相似度：0.14818309750777228\n",
      "評論：房間 乾淨 水壓 大 熱水 快 來 洗澡 舒服 市區 開車 分鐘 內 到達 路 邊 停車 方便 消毒水 味道 重，相似度：0.11527584378112371\n",
      "評論：早上 五點 多 有 公雞 叫 起床 六點 多 有 戰機 飛過 房間 廁所 乾淨，相似度：0.09638061657609528\n",
      "\n",
      "\n",
      "飯店名稱：家有囍宿\n",
      "平均相似度:0.09766333643934164\n",
      "前10條相似的評論:\n",
      "評論：房間 大 間 衛浴 設備 讚 有 按摩 浴缸 值 高 下 次 來 時 有 房間 住，相似度：0.3924007059710153\n",
      "評論：早餐 直接 送 餐券 方便 安靜 舒適 房間 大 枕頭 扁 床 軟 中間 陷下去 睡 右邊 一點 建議 翻面，相似度：0.09591597622569287\n",
      "評論：老闆 親切 民宿 整潔 生活 機能 佳 便利商店 各 種 食 小吃 餐廳 有 停車 方便 民宿 客廳 廚房 應有盡有 床 軟，相似度：0.0\n",
      "評論：按摩 浴缸 有 老闆 服務 態度 沒有，相似度：0.0\n",
      "評論：早餐 換 早餐店 餐券 環境 鬧中取靜 女性 友好 沒 化妝桌 備品，相似度：0.0\n",
      "\n",
      "\n",
      "飯店名稱：魔法精靈民宿\n",
      "平均相似度:0.09533750208640472\n",
      "前10條相似的評論:\n",
      "評論：老闆 親切 房間 乾淨 整潔 房間 乾淨 整潔 電視 訊號 不佳，相似度：0.2786818920562877\n",
      "評論：喜歡 貴婦 風格 入住 每 間 房間 個 有 特色 房 內 獨立 冰箱 一樓 有 共用 冰箱，相似度：0.10266811628933122\n",
      "評論：老闆娘 人 親切 推離 火車站 附近，相似度：0.0\n",
      "評論：火車站 近 客廳 空間 提供 很多 茶水 餅乾，相似度：0.0\n",
      "\n",
      "\n",
      "飯店名稱：朝日民宿 - Peng's Family\n",
      "平均相似度:0.08154258317783944\n",
      "前10條相似的評論:\n",
      "評論：房間 差，相似度：1.0\n",
      "評論：房間 新 乾淨，相似度：0.603128363134945\n",
      "評論：房間 有 螞蟻，相似度：0.36113605940999854\n",
      "評論：房間 乾淨 退房，相似度：0.3388064899506201\n",
      "評論：房間 乾淨 漂亮，相似度：0.330553406895637\n",
      "評論：價格 實惠 房間 乾淨，相似度：0.25329024901002034\n",
      "評論：位置 佳 早餐 不錯 房間 有 蟑螂 木 地板 有 鐵釘 露出 電視 有 雜音 房間 設備 老舊，相似度：0.22584727722100517\n",
      "評論：老闆娘 親切 服務 房間 裝潢 用心 價格 沒有，相似度：0.19145850105749293\n",
      "評論：舒適 寬敞 房間 棒 機車 停進去，相似度：0.17583815361392374\n",
      "評論：算 一 間 旅遊 選擇 入住 民宿 房間 寬敞 光線 明亮 海岸 公路 近 老闆娘 客氣 房間 沒有 小 茶几 四 人 房 有 一 把 椅子 浴室 洗手台 沒有 置物台，相似度：0.1663963089878792\n",
      "\n",
      "\n",
      "飯店名稱：海洋風情民宿\n",
      "平均相似度:0.0786606961632868\n",
      "前10條相似的評論:\n",
      "評論：房間 乾淨 舒適 老闆娘 人 親切，相似度：0.2889327186669041\n",
      "評論：居住 環境 安靜 房間 乾淨，相似度：0.22282414969116604\n",
      "評論：老闆娘 人 親切 房間 漂亮 舒適 下 次 有 機會 入住，相似度：0.1992473366109996\n",
      "評論：老闆娘 親切 小孩 友善 房間 天花板 晚上 有 星空 孩子 喜歡，相似度：0.13088128555018277\n",
      "評論：房間 屋頂 漂亮 入住 時 告知 人 多 水 熱 等 分鐘 等 一 個 多 小時 水 冷 冷 大 冬天 冷水 有 棉被 薄 涼被 厚，相似度：0.1073690257158103\n",
      "評論：第一 次 住有 星空 景觀 房型 感覺 開心 民宿 業者 熱心 介紹 附近 景點 提供 相關 資料 住宅區 寧靜 週圍 好 停車 房間 乾淨 舒適 好 民宿，相似度：0.0733345338876655\n",
      "評論：住宿 地點 佳 環境 清幽 風景區 市區 近 停車 算 方便 客房 大 夜燈 美麗 投影 天花板 星空 讓 人 忘卻 一切 煩惱 浴室 空間 大 多功能 蓮蓬頭 喜好 調整 有 浴缸 泡澡 洗去 一 天 疲憊 浴室 漱口杯 蓋 煙蒂 想要 使用 拿起來 有 滿滿 菸味 上 一 位 房客 藏 浴室 部分 請 打掃 時 注意 一下，相似度：0.0\n",
      "評論：入住 時 老闆娘 接待 親切 提供 多 實用 旅遊 建議 代訂 早餐 好吃 之後 去 花蓮 想 入住，相似度：0.0\n",
      "評論：整體 環境 清潔度 滿意 老闆娘 熱情 無，相似度：0.0\n",
      "評論：純真 童趣 溫馨 民宿，相似度：0.0\n",
      "\n",
      "\n",
      "飯店名稱：班卡拉渡假旅店\n",
      "平均相似度:0.07695721092689337\n",
      "前10條相似的評論:\n",
      "評論：床 大 好 睡 房間 大，相似度：1.0\n",
      "評論：房間 大 舒適 無，相似度：0.5655955612619636\n",
      "評論：寬敞 房間 好，相似度：0.4340281943246662\n",
      "評論：沒有 房間 內 浴室 沒有 天花板 房間 隔開 上廁所 味道 跑到 房間 內，相似度：0.3449057772268462\n",
      "評論：老闆 主動 升等 房間 房間 乾淨 舒適 乾淨 舒適 便宜 無，相似度：0.3074710721695949\n",
      "評論：裝修 更新 一下 好 房間 大 停車 方便 廁所 房間 沒有 隔間 夫妻 入住 感覺 方便 味道 留在 房間 內，相似度：0.29420243523027806\n",
      "評論：房間 寬敞 舒適 床 軟 好 睡 早餐 好吃，相似度：0.292365179018659\n",
      "評論：停車 方便 房間 內 有 蚊子，相似度：0.27175433504817104\n",
      "評論：房間 格局 不佳，相似度：0.25024482283159055\n",
      "評論：房間 整潔 環境 清幽 安靜，相似度：0.22636900209342142\n",
      "\n",
      "\n",
      "飯店名稱：小巷民宿\n",
      "平均相似度:0.07618021017611262\n",
      "前10條相似的評論:\n",
      "評論：房間 舒適 乾淨 停車 方便 床 軟 晚上 房間 冷，相似度：0.4869619749612681\n",
      "評論：民宿 主人 友善 房間 舒適 乾淨 早餐 好吃，相似度：0.20305808167314005\n",
      "評論：家 一樣 舒適 進到 民宿 回到 家 裡面 一樣 民宿 夫婦 位 孩子 親切 問候 房間 大 床 枕頭 舒服 重點 房 內 有 個 大 桌子 房 內 用餐 方便 可惜 疫情 期間 吃 到手 作 早餐 安全 第一，相似度：0.07178204512671808\n",
      "評論：推薦給 喜歡 整潔 簡單 平價 清靜 住宿 條件 者 整潔，相似度：0.0\n",
      "評論：民宿 主人 用心 準備 早餐 熱情 分享 介紹 景點 寄宿 朋友 家 一樣 棒 住宿 經驗，相似度：0.0\n",
      "評論：溫馨 早餐 老闆娘 老闆 動手 做 好吃，相似度：0.0\n",
      "評論：乾淨 陽台 熱情 老闆，相似度：0.0\n",
      "評論：安靜 清潔 老闆 用心 經營 安排 太平山 武陵 農場 中繼 過夜 好 地點，相似度：0.0\n",
      "評論：住宿 環境 舒適 早餐 配合 天氣 提供 熱飲 熱食 好，相似度：0.0\n",
      "評論：民宿 男女 主人 服務 親切 備 小 點心 供 房客 解饞，相似度：0.0\n",
      "\n",
      "\n",
      "飯店名稱：羅東公園61號(峇里風)特色包棟民宿\n",
      "平均相似度:0.07312529675005397\n",
      "前10條相似的評論:\n",
      "評論：兩 間 房間 包 棟 不錯 早餐 用心 好吃 老闆娘 人 親切 二樓 雙人房 毛玻璃 設計 住在 同 一 層 樓 無 隱私 房間 內 沒有 衛浴 不便 民宿 乾淨 二樓 馬桶 內側 邊邊 霉垢 刷 乾淨，相似度：0.14625059350010794\n",
      "評論：超棒 好吃，相似度：0.0\n",
      "\n",
      "\n",
      "飯店名稱：陽光聖亞旅店\n",
      "平均相似度:0.07209086411636584\n",
      "前10條相似的評論:\n",
      "評論：房間 海景 棒，相似度：0.38642716625265017\n",
      "評論：房間 乾淨 寬敞，相似度：0.37642315786029673\n",
      "評論：房間 漂亮 風景 美，相似度：0.27259975472114806\n",
      "評論：喜歡 待在 房間 看到 海 人 這兒 適合 房間 乾淨 床 被子 舒適，相似度：0.25865871848912453\n",
      "評論：老闆 服務 親切 人 房間 明亮 乾淨，相似度：0.24982073940400212\n",
      "評論：地點 佳 服務 好 風景 漂亮 房間 乾淨 舒適 地點 佳 服務 好 風景 漂亮 房間 乾淨 舒適，相似度：0.21682307644909057\n",
      "評論：位於 對面 房間 欣賞 美景 附近 有 便利商店 便利 員工 服務 棒 熱情 有禮 價格 不錯 值 高 房間 乾淨，相似度：0.21589650312418757\n",
      "評論：海景 覺得 房間 放 礦泉水，相似度：0.21485729573158635\n",
      "評論：房間 風景 佳 乾淨 整潔 老闆 用心 熱情，相似度：0.19828412589351113\n",
      "評論：房間 照片 無異，相似度：0.195551135265147\n",
      "\n",
      "\n",
      "飯店名稱：風華渡假旅館\n",
      "平均相似度:0.06731216603744469\n",
      "前10條相似的評論:\n",
      "評論：房間 大，相似度：1.0\n",
      "評論：房間 大，相似度：1.0\n",
      "評論：房間 吵 差，相似度：1.0\n",
      "評論：不錯 房間 大 地點，相似度：0.4289619225684878\n",
      "評論：房間 佈置 整體 上 不錯 房間 內 蚊子 多 晚上 咬，相似度：0.3653512350265948\n",
      "評論：房間 菸味 重，相似度：0.3040495689640538\n",
      "評論：房間 大 氣氛 佳 舒適，相似度：0.2768113029495801\n",
      "評論：房間 乾淨 整齊 浴室，相似度：0.27473817713622095\n",
      "評論：浴室 大 泡澡 舒服 房間 美，相似度：0.27257806811496355\n",
      "評論：舒適度 房間 有 煙味，相似度：0.2551117363687556\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "\n",
    "df_tf_idf = pd.read_csv('0_10000.csv', header=0)\n",
    "\n",
    "sentences_tokenized = [eval(d) for d in df_tf_idf[\"綜合評論_ws\"].values]\n",
    "pos_tags = [eval(d) for d in df_tf_idf[\"綜合評論_pos\"].values]\n",
    "\n",
    "# 嘗試只保留有意義的詞性\n",
    "stop_pos = set(['Nep', 'Nh', 'Nb'])  # 這 3 種詞性不保留\n",
    "\n",
    "short_with_pos = []  # 放過濾後的詞性與句子\n",
    "short_sentence = []  # 放過濾後的句子\n",
    "\n",
    "for sentence, sentence_pos_tags in zip(sentences_tokenized, pos_tags):\n",
    "    short_with_pos_sentence = []  # 儲存這個句子過濾後的詞（帶詞性）\n",
    "    short_sentence_sentence = []  # 儲存這個句子過濾後的詞（不帶詞性）\n",
    "    \n",
    "    for word_ws, word_pos in zip(sentence, sentence_pos_tags):\n",
    "        \n",
    "        # 只留名詞和動詞\n",
    "        is_N_or_V = word_pos.startswith(\"V\") or word_pos.startswith(\"N\")\n",
    "\n",
    "        # 去掉名詞裡的某些詞性\n",
    "        is_not_stop_pos = word_pos not in stop_pos\n",
    "\n",
    "        if is_N_or_V and is_not_stop_pos:\n",
    "            short_with_pos_sentence.append(f\"{word_ws}({word_pos})\")\n",
    "            short_sentence_sentence.append(f\"{word_ws}\")\n",
    "\n",
    "    short_with_pos.append(short_with_pos_sentence)\n",
    "    short_sentence.append(short_sentence_sentence)\n",
    "\n",
    "# 將 '綜合評論_ws' 欄位更新為 'short_sentence' 列表\n",
    "df_tf_idf['綜合評論_ws'] = [' '.join(s) for s in short_sentence]\n",
    "\n",
    "# 將綜合評論過濾後為空的資料去除\n",
    "non_empty_mask = df_tf_idf['綜合評論_ws'].str.strip() != ''\n",
    "\n",
    "# 使用此遮罩來過濾 DataFrame\n",
    "df_tf_idf = df_tf_idf[non_empty_mask]\n",
    "\n",
    "# 使用 TF-IDF 將所有評論文本轉換為數值向量\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(df_tf_idf['綜合評論_ws'])\n",
    "\n",
    "# 假設新用戶輸入的需求\n",
    "new_user_input = \"房間 大\"\n",
    "\n",
    "# 將新用戶輸入轉換為相同的數值向量\n",
    "new_vector = vectorizer.transform([new_user_input])\n",
    "\n",
    "# 獲取所有的飯店名稱\n",
    "hotel_names = df['飯店名稱'].unique()\n",
    "\n",
    "# 初始化一個空的字典來儲存每個飯店的所有評論相似度和評論文本\n",
    "hotel_reviews_similarities = {}\n",
    "\n",
    "# 對每個飯店進行處理\n",
    "for hotel_name in hotel_names:\n",
    "    # 只選取該飯店的評論\n",
    "    hotel_reviews = df_tf_idf[df_tf_idf['飯店名稱'] == hotel_name]['綜合評論_ws']\n",
    "    \n",
    "    # 使用擬合好的TF-IDF將飯店的評論轉換為數值向量\n",
    "    X = vectorizer.transform(hotel_reviews)\n",
    "\n",
    "    # 計算新用戶輸入與所有評論的 cosine similarity\n",
    "    cos_similarities = cosine_similarity(new_vector, X).flatten()\n",
    "    \n",
    "    # 將這個飯店的所有評論相似度和評論文本儲存到字典中\n",
    "    hotel_reviews_similarities[hotel_name] = list(zip(hotel_reviews, cos_similarities))\n",
    "\n",
    "# 初始化一個空的字典來儲存每個飯店與新用戶需求的平均相似度\n",
    "hotel_similarities = {hotel: np.mean([sim for _, sim in reviews]) for hotel, reviews in hotel_reviews_similarities.items()}\n",
    "\n",
    "# 對飯店的平均相似度進行排序，並只取前10個\n",
    "top10_hotels = sorted(hotel_similarities.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "# 輸出與新用戶需求最相似的前 10 個飯店\n",
    "for hotel_name, avg_sim in top10_hotels:\n",
    "    print(f\"飯店名稱：{hotel_name}\")\n",
    "    print(f\"平均相似度:{avg_sim}\")\n",
    "    \n",
    "    # 對這家飯店的所有評論相似度進行排序，並只取前 10 個\n",
    "    top10_reviews = sorted(hotel_reviews_similarities[hotel_name], key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "    # 輸出與新用戶需求最相似的前10條評論\n",
    "    print(\"前10條相似的評論:\")\n",
    "    for review_text, sim in top10_reviews:\n",
    "        print(f\"評論：{review_text}，相似度：{sim}\")\n",
    "    \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9130ce6c",
   "metadata": {},
   "source": [
    "#### 將同一間飯店評論合併後，使用 roberta 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d145de73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-roberta-wwm-ext were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (5910) must match the size of tensor b (512) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 67\u001b[0m\n\u001b[0;32m     65\u001b[0m         inputs \u001b[38;5;241m=\u001b[39m tokenizer(sentence, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     66\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 67\u001b[0m             outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m     68\u001b[0m         hotel_embeddings[i]\u001b[38;5;241m.\u001b[39mappend(outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mflatten())\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# Compute the average embedding for each hotel\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\web_scraping\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\web_scraping\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1013\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1006\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m   1007\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m   1008\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m   1009\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m   1010\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m   1011\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m-> 1013\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1019\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1020\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[0;32m   1021\u001b[0m     embedding_output,\n\u001b[0;32m   1022\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1030\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1031\u001b[0m )\n\u001b[0;32m   1032\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\web_scraping\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\web_scraping\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:236\u001b[0m, in \u001b[0;36mBertEmbeddings.forward\u001b[1;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabsolute\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    235\u001b[0m     position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embeddings(position_ids)\n\u001b[1;32m--> 236\u001b[0m     embeddings \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m position_embeddings\n\u001b[0;32m    237\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(embeddings)\n\u001b[0;32m    238\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(embeddings)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (5910) must match the size of tensor b (512) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from collections import defaultdict\n",
    "\n",
    "def sentence_based_sliding_window(text, window_size):\n",
    "    sentences = re.split('(?<=[。])', text)\n",
    "    current_group = []\n",
    "    current_length = 0\n",
    "    grouped_sentences = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_length = len(tokenizer.tokenize(sentence))\n",
    "        if sentence_length > window_size:\n",
    "            # If the length of a single sentence exceeds the window size, skip that data\n",
    "            continue\n",
    "        elif current_length + sentence_length > window_size:\n",
    "            grouped_sentences.append(\"\".join(current_group))\n",
    "            current_group = [sentence]\n",
    "            current_length = sentence_length\n",
    "        else:\n",
    "            current_group.append(sentence)\n",
    "            current_length += sentence_length\n",
    "\n",
    "    # Add the remaining sentences to grouped_sentences\n",
    "    if current_group:\n",
    "        grouped_sentences.append(\"\".join(current_group))\n",
    "\n",
    "    return grouped_sentences\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('hfl/chinese-roberta-wwm-ext')\n",
    "model = BertModel.from_pretrained('hfl/chinese-roberta-wwm-ext')\n",
    "\n",
    "\n",
    "df = pd.read_csv('./booking_comments_分詞update.csv', header=0)\n",
    "\n",
    "# Merge the comprehensive comments of the same hotel using a period\n",
    "df_grouped = df[0:10000].groupby('飯店名稱')['綜合評論'].apply(lambda x: '。'.join(x)).reset_index()\n",
    "\n",
    "# Text\n",
    "sentences = list(df_grouped.loc[:, '綜合評論'])\n",
    "\n",
    "# Get the embeddings for each text\n",
    "embeddings = []\n",
    "hotel_indices = []\n",
    "hotel_embeddings = defaultdict(list)\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    # If the sentence is too long, split it into multiple fragments\n",
    "    if len(tokenizer.tokenize(sentence)) > tokenizer.model_max_length:\n",
    "        sentence_windows = sentence_based_sliding_window(sentence, 4000)\n",
    "        for sentence_window in sentence_windows:\n",
    "            if len(tokenizer.tokenize(sentence_window)) > tokenizer.model_max_length:\n",
    "                continue\n",
    "            inputs = tokenizer(sentence_window, return_tensors=\"pt\", truncation=False, padding=True)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "            hotel_embeddings[i].append(outputs.last_hidden_state.mean(dim=1).numpy().flatten())\n",
    "    else:\n",
    "        inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=False, padding=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        hotel_embeddings[i].append(outputs.last_hidden_state.mean(dim=1).numpy().flatten())\n",
    "\n",
    "# Compute the average embedding for each hotel\n",
    "for i, hotel_embedding in hotel_embeddings.items():\n",
    "    embeddings.append(np.mean(hotel_embedding, axis=0))\n",
    "    hotel_indices.append(i)\n",
    "\n",
    "# 新的文本\n",
    "new_sentence = \"房間大\"\n",
    "inputs = tokenizer(new_sentence, return_tensors=\"pt\", truncation=True, max_length=512, padding=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "new_embedding = outputs.last_hidden_state.mean(dim=1).numpy().flatten()\n",
    "\n",
    "\n",
    "# Calculate similarity with the new text\n",
    "similarities = cosine_similarity(embeddings + [new_embedding])\n",
    "\n",
    "# Get the indices of the top 10 most similar texts\n",
    "top10_indices = np.argsort(similarities[-1][:-1])[:-10:-1]\n",
    "\n",
    "# Print the top 10 most similar texts\n",
    "for index in top10_indices:\n",
    "    print(f\"Similar hotel: {df_grouped.loc[hotel_indices[index], '飯店名稱']}, similarity: {similarities[-1][index]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac70b1f3",
   "metadata": {},
   "source": [
    "#### 將同一間飯店評論合併後，使用可處理長句子的 bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b866d944",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_based_sliding_window(text, window_size):\n",
    "    sentences = re.split('(?<=[。])', text)\n",
    "    current_group = []\n",
    "    current_length = 0\n",
    "    grouped_sentences = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_length = len(tokenizer.tokenize(sentence))\n",
    "        if sentence_length > window_size:\n",
    "            # 如果單一句子的長度超過窗口大小，跳過該筆數據\n",
    "            continue\n",
    "        elif current_length + sentence_length > window_size:\n",
    "            grouped_sentences.append(\"\".join(current_group))\n",
    "            current_group = [sentence]\n",
    "            current_length = sentence_length\n",
    "        else:\n",
    "            current_group.append(sentence)\n",
    "            current_length += sentence_length\n",
    "\n",
    "    # 把剩下的句子加入 grouped_sentences\n",
    "    if current_group:\n",
    "        grouped_sentences.append(\"\".join(current_group))\n",
    "    \n",
    "    return grouped_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c19da6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "相似的飯店: 大寶的民宿 Tabohouse, 相似度: 0.9586089849472046\n",
      "相似的飯店: 翠園歐風庭園民宿, 相似度: 0.9230974912643433\n",
      "相似的飯店: 花爵墾丁, 相似度: 0.9208060503005981\n",
      "相似的飯店: 白鷺灣 民宿(安平古堡), 相似度: 0.9181367754936218\n",
      "相似的飯店: 希望恆春休閒會館, 相似度: 0.9181272983551025\n",
      "相似的飯店: 卡爾登飯店 the Carlton, 相似度: 0.9166074991226196\n",
      "相似的飯店: 陽光滿屋民宿, 相似度: 0.9149898886680603\n",
      "相似的飯店: 家有囍宿, 相似度: 0.9144690036773682\n",
      "相似的飯店: 海洋風情民宿, 相似度: 0.9132213592529297\n"
     ]
    }
   ],
   "source": [
    "from transformers import LongformerModel, LongformerTokenizer\n",
    "from collections import defaultdict\n",
    "\n",
    "tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
    "model = LongformerModel.from_pretrained('allenai/longformer-base-4096')\n",
    "\n",
    "df = pd.read_csv('./booking_comments_分詞update.csv', header=0)\n",
    "\n",
    "# 將相同飯店的綜合評論用句號合併\n",
    "df_grouped = df[0:10000].groupby('飯店名稱')['綜合評論'].apply(lambda x: '。'.join(x)).reset_index()\n",
    "\n",
    "# 文本\n",
    "sentences = list(df_grouped.loc[:, '綜合評論'])\n",
    "\n",
    "# 獲取每個文本的嵌入向量\n",
    "embeddings = []\n",
    "hotel_indices = []\n",
    "hotel_embeddings = defaultdict(list)\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    # 如果句子太長，就將其分割成多個片段\n",
    "    if len(tokenizer.tokenize(sentence)) > tokenizer.model_max_length:\n",
    "        sentence_windows = sentence_based_sliding_window(sentence, 4000)\n",
    "        for sentence_window in sentence_windows:\n",
    "            if len(tokenizer.tokenize(sentence_window)) > tokenizer.model_max_length:\n",
    "                continue\n",
    "            inputs = tokenizer(sentence_window, return_tensors=\"pt\", truncation=False, padding=True)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "            hotel_embeddings[i].append(outputs.last_hidden_state.mean(dim=1).numpy().flatten())\n",
    "    else:\n",
    "        inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=False, padding=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        hotel_embeddings[i].append(outputs.last_hidden_state.mean(dim=1).numpy().flatten())\n",
    "\n",
    "# 計算每一間飯店的平均向量\n",
    "for i, hotel_embedding in hotel_embeddings.items():\n",
    "    embeddings.append(np.mean(hotel_embedding, axis=0))\n",
    "    hotel_indices.append(i)\n",
    "\n",
    "# 新的文本\n",
    "new_sentence = \"房間大\"\n",
    "inputs = tokenizer(new_sentence, return_tensors=\"pt\", truncation=False, padding=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "new_embedding = outputs.last_hidden_state.mean(dim=1).numpy().flatten()\n",
    "\n",
    "# 計算與新文本的相似性\n",
    "similarities = cosine_similarity(embeddings + [new_embedding])\n",
    "\n",
    "# 獲取最相似的 10 個文本的索引\n",
    "top10_indices = np.argsort(similarities[-1][:-1])[:-10:-1]\n",
    "\n",
    "# 印出最相似的 10 個文本\n",
    "for index in top10_indices:\n",
    "    print(f\"相似的飯店: {df_grouped.loc[hotel_indices[index], '飯店名稱']}, 相似度: {similarities[-1][index]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cf20c8",
   "metadata": {},
   "source": [
    "#### 將同一間飯店評論合併後，使用可處理長句子的 bert，並排除評論數不足30的飯店"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707cd6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LongformerModel, LongformerTokenizer\n",
    "\n",
    "tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
    "model = LongformerModel.from_pretrained('allenai/longformer-base-4096')\n",
    "\n",
    "df = pd.read_csv('./booking_comments_分詞update.csv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaab081",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# 篩選評論數大於等於 30 的飯店\n",
    "df_filtered = df[0:10000].groupby('飯店名稱').filter(lambda x: len(x) >= 30);df_filtered\n",
    "\n",
    "# 將相同飯店的綜合評論用句號合併，並保留其他重要欄位\n",
    "df_grouped = df_filtered.groupby('飯店名稱').agg({\n",
    "    '綜合評論': lambda x: '。'.join(x),\n",
    "    '縣市': lambda x: x.iloc[0],\n",
    "    '鄉鎮': lambda x: x.iloc[0],\n",
    "    '整體評分': lambda x: x.iloc[0],\n",
    "    '單項評分_整潔度': lambda x: x.iloc[0],\n",
    "    '單項評分_舒適程度': lambda x: x.iloc[0],\n",
    "    '單項評分_住宿地點': lambda x: x.iloc[0],\n",
    "    '單項評分_設施': lambda x: x.iloc[0],\n",
    "    '單項評分_員工素質': lambda x: x.iloc[0],\n",
    "    '單項評分_性價比': lambda x: x.iloc[0]\n",
    "}).reset_index();df_grouped\n",
    "\n",
    "# 文本\n",
    "sentences = list(df_grouped.loc[:, '綜合評論'])\n",
    "\n",
    "# 獲取每個文本的嵌入向量\n",
    "embeddings = []\n",
    "hotel_indices = []\n",
    "hotel_embeddings = defaultdict(list)\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    \n",
    "    # 如果句子太長，就分割成多個片段\n",
    "    if len(tokenizer.tokenize(sentence)) > tokenizer.model_max_length:\n",
    "        sentence_windows = sentence_based_sliding_window(sentence, 4000)\n",
    "        for sentence_window in sentence_windows:\n",
    "            if len(tokenizer.tokenize(sentence_window)) > tokenizer.model_max_length:\n",
    "                continue\n",
    "            inputs = tokenizer(sentence_window, return_tensors=\"pt\", truncation=False, padding=True)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "            hotel_embeddings[i].append(outputs.last_hidden_state.mean(dim=1).numpy().flatten())\n",
    "    else:\n",
    "        inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=False, padding=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        hotel_embeddings[i].append(outputs.last_hidden_state.mean(dim=1).numpy().flatten())\n",
    "\n",
    "# 計算每間飯店的平均向量\n",
    "for i, hotel_embedding in hotel_embeddings.items():\n",
    "    embeddings.append(np.mean(hotel_embedding, axis=0))\n",
    "    hotel_indices.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "203b096b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 儲存嵌入向量\n",
    "np.save('embeddings_longformer_base_4096.npy', embeddings)\n",
    "\n",
    "# 儲存飯店資訊\n",
    "df_grouped.loc[hotel_indices, :].to_csv('hotel_info_longformerbase4096.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a795046b",
   "metadata": {},
   "source": [
    "#### 使用迴圈跑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0156871",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_based_sliding_window(text, window_size):\n",
    "    sentences = re.split('(?<=[。])', text)\n",
    "    current_group = []\n",
    "    current_length = 0\n",
    "    grouped_sentences = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_length = len(tokenizer.tokenize(sentence))\n",
    "        if sentence_length > window_size:\n",
    "            # If the length of a single sentence exceeds the window size, skip that data\n",
    "            continue\n",
    "        elif current_length + sentence_length > window_size:\n",
    "            grouped_sentences.append(\"\".join(current_group))\n",
    "            current_group = [sentence]\n",
    "            current_length = sentence_length\n",
    "        else:\n",
    "            current_group.append(sentence)\n",
    "            current_length += sentence_length\n",
    "\n",
    "    # Add the remaining sentences to grouped_sentences\n",
    "    if current_group:\n",
    "        grouped_sentences.append(\"\".join(current_group))\n",
    "\n",
    "    return grouped_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "664d61e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing hotels 0 to 50\n",
      "Processing hotels 50 to 100\n",
      "Processing hotels 100 to 150\n",
      "Processing hotels 150 to 200\n",
      "Processing hotels 200 to 250\n",
      "Processing hotels 250 to 300\n",
      "Processing hotels 300 to 350\n",
      "Processing hotels 350 to 400\n",
      "Processing hotels 400 to 450\n",
      "Processing hotels 450 to 500\n",
      "Processing hotels 500 to 550\n",
      "Processing hotels 550 to 600\n",
      "Processing hotels 600 to 650\n",
      "Processing hotels 650 to 700\n",
      "Processing hotels 700 to 750\n",
      "Processing hotels 750 to 800\n",
      "Processing hotels 800 to 850\n",
      "Processing hotels 850 to 900\n",
      "Processing hotels 900 to 950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4097 > 4096). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 63\u001b[0m\n\u001b[0;32m     61\u001b[0m         inputs \u001b[38;5;241m=\u001b[39m tokenizer(sentence, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     62\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 63\u001b[0m             outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m     64\u001b[0m         hotel_embeddings[i]\u001b[38;5;241m.\u001b[39mappend(outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mflatten())\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, hotel_embedding \u001b[38;5;129;01min\u001b[39;00m hotel_embeddings\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\web_scraping\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\web_scraping\\lib\\site-packages\\transformers\\models\\longformer\\modeling_longformer.py:1743\u001b[0m, in \u001b[0;36mLongformerModel.forward\u001b[1;34m(self, input_ids, attention_mask, global_attention_mask, head_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1737\u001b[0m \u001b[38;5;66;03m# We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;66;03m# ourselves in which case we just need to make it broadcastable to all heads.\u001b[39;00m\n\u001b[0;32m   1739\u001b[0m extended_attention_mask: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_extended_attention_mask(attention_mask, input_shape)[\n\u001b[0;32m   1740\u001b[0m     :, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, :\n\u001b[0;32m   1741\u001b[0m ]\n\u001b[1;32m-> 1743\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\n\u001b[0;32m   1745\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1747\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[0;32m   1748\u001b[0m     embedding_output,\n\u001b[0;32m   1749\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1754\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1755\u001b[0m )\n\u001b[0;32m   1756\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\web_scraping\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\web_scraping\\lib\\site-packages\\transformers\\models\\longformer\\modeling_longformer.py:471\u001b[0m, in \u001b[0;36mLongformerEmbeddings.forward\u001b[1;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds)\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    470\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_embeddings(input_ids)\n\u001b[1;32m--> 471\u001b[0m position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    472\u001b[0m token_type_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_type_embeddings(token_type_ids)\n\u001b[0;32m    474\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m inputs_embeds \u001b[38;5;241m+\u001b[39m position_embeddings \u001b[38;5;241m+\u001b[39m token_type_embeddings\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\web_scraping\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\web_scraping\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\web_scraping\\lib\\site-packages\\torch\\nn\\functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2204\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2205\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2206\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2207\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2208\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from transformers import LongformerModel, LongformerTokenizer\n",
    "import re\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 如果有 GPU 就使用，否則用 CPU \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
    "model = LongformerModel.from_pretrained('allenai/longformer-base-4096')\n",
    "model = model.to(device)\n",
    "\n",
    "df = pd.read_csv('./booking_comments_分詞update.csv', header=0)\n",
    "\n",
    "# 讀取之前的檔案\n",
    "try:\n",
    "    embeddings = np.load('embeddings_longformer_base_4096.npy').tolist()\n",
    "    df_grouped = pd.read_csv('hotel_info_longformerbase4096.csv')\n",
    "except FileNotFoundError:\n",
    "    embeddings = []\n",
    "    df_grouped = pd.DataFrame()\n",
    "\n",
    "hotel_names = df['飯店名稱'].unique()\n",
    "n_hotels = len(hotel_names)\n",
    "\n",
    "# 分批執行(每50間飯店儲存一次)\n",
    "for i in range(0, n_hotels, 50):\n",
    "    print(f'Processing hotels {i} to {min(i + 50, n_hotels)}')\n",
    "    batch_hotel_names = hotel_names[i:i + 50]\n",
    "    df_filtered = df[df['飯店名稱'].isin(batch_hotel_names)]\n",
    "    df_grouped_batch = df_filtered.groupby('飯店名稱').agg({\n",
    "        '正評': lambda x: '。'.join(str(v) for v in x if not pd.isna(v)),\n",
    "        '縣市': lambda x: x.iloc[0],\n",
    "        '鄉鎮': lambda x: x.iloc[0],\n",
    "        '整體評分': lambda x: x.iloc[0],\n",
    "        '單項評分_整潔度': lambda x: x.iloc[0],\n",
    "        '單項評分_舒適程度': lambda x: x.iloc[0],\n",
    "        '單項評分_住宿地點': lambda x: x.iloc[0],\n",
    "        '單項評分_設施': lambda x: x.iloc[0],\n",
    "        '單項評分_員工素質': lambda x: x.iloc[0],\n",
    "        '單項評分_性價比': lambda x: x.iloc[0]\n",
    "    }).reset_index()\n",
    "    \n",
    "    sentences = list(df_grouped_batch.loc[:, '正評'])\n",
    "    hotel_embeddings = defaultdict(list)\n",
    "\n",
    "    for sentence in sentences:\n",
    "        \n",
    "        # 如果文本過長就拆分\n",
    "        if len(tokenizer.tokenize(sentence)) > tokenizer.model_max_length:\n",
    "            sentence_windows = sentence_based_sliding_window(sentence, 4000)\n",
    "            for sentence_window in sentence_windows:\n",
    "                if len(tokenizer.tokenize(sentence_window)) > tokenizer.model_max_length:\n",
    "                    continue\n",
    "                inputs = tokenizer(sentence_window, return_tensors=\"pt\", truncation=False, padding=True).to(device)\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(**inputs)\n",
    "                hotel_embeddings[i].append(outputs.last_hidden_state.mean(dim=1).cpu().numpy().flatten())\n",
    "        else:\n",
    "            inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=False, padding=True).to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "            hotel_embeddings[i].append(outputs.last_hidden_state.mean(dim=1).cpu().numpy().flatten())\n",
    "    \n",
    "    # 計算平均向量\n",
    "    for _, hotel_embedding in hotel_embeddings.items():\n",
    "        embeddings.append(np.mean(hotel_embedding, axis=0))\n",
    "    \n",
    "    # 合併檔案內容\n",
    "    df_grouped = pd.concat([df_grouped, df_grouped_batch])\n",
    "    \n",
    "    # 儲存\n",
    "    np.save('embeddings_longformer_base_4096.npy', np.array(embeddings))\n",
    "    df_grouped.to_csv('hotel_info_longformerbase4096.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691c3acf",
   "metadata": {},
   "source": [
    "#### 進行二次篩選(根據某些特定條件)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d2bba03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_hotels(condition, embeddings, new_embedding, hotel_data, n):\n",
    "    \n",
    "    # 根據條件篩選出飯店\n",
    "    filtered_hotels = hotel_data[condition]\n",
    "\n",
    "    # 取得符合條件的飯店嵌入向量\n",
    "    filtered_embeddings = embeddings[filtered_hotels.index]\n",
    "\n",
    "    # 計算與新文本的相似性\n",
    "    similarities = cosine_similarity(np.concatenate([filtered_embeddings, new_embedding[None, :]]))\n",
    "\n",
    "    # 獲取最相似的 n 個文本的索引\n",
    "    topn_indices = np.argsort(similarities[-1][:-1])[:-n-1:-1]\n",
    "\n",
    "    # 印出最相似的 n 個飯店\n",
    "    for index in topn_indices:\n",
    "        print(f\"相似的飯店:\\n{filtered_hotels.iloc[index][['飯店名稱']]}\")\n",
    "        print(f\"縣市:{filtered_hotels.iloc[index][['縣市']]}\")\n",
    "        print(f\"鄉鎮:{filtered_hotels.iloc[index][['鄉鎮']]}\")\n",
    "        print(f\"整體評分:{filtered_hotels.iloc[index][['整體評分']]}\")\n",
    "        print(f\"相似度: {similarities[-1][index]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6396d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900\n",
      "18\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 128 is out of bounds for axis 0 with size 18",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 28\u001b[0m\n\u001b[0;32m     14\u001b[0m new_embedding \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m#     # 新的文本\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m#     tokens = tokenizer.tokenize(sentence)  # 分詞\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m#     inputs = tokenizer(sentence, return_tensors=\"pt\")\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m \n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# 搜尋'南投縣'的前 5 個相似的飯店\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m \u001b[43mget_similar_hotels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_hotel_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m縣市\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m南投縣\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_embedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_hotel_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[14], line 7\u001b[0m, in \u001b[0;36mget_similar_hotels\u001b[1;34m(condition, embeddings, new_embedding, hotel_data, n)\u001b[0m\n\u001b[0;32m      4\u001b[0m filtered_hotels \u001b[38;5;241m=\u001b[39m hotel_data[condition]\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# 取得符合條件的飯店嵌入向量\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m filtered_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43membeddings\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfiltered_hotels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# 計算與新文本的相似性\u001b[39;00m\n\u001b[0;32m     10\u001b[0m similarities \u001b[38;5;241m=\u001b[39m cosine_similarity(np\u001b[38;5;241m.\u001b[39mconcatenate([filtered_embeddings, new_embedding[\u001b[38;5;28;01mNone\u001b[39;00m, :]]))\n",
      "\u001b[1;31mIndexError\u001b[0m: index 128 is out of bounds for axis 0 with size 18"
     ]
    }
   ],
   "source": [
    "# 讀取 DataFrame\n",
    "df_hotel_info = pd.read_csv('hotel_info_longformerbase4096.csv')\n",
    "print(len(df_hotel_info))\n",
    "# 讀取 embeddings\n",
    "embeddings = np.load('embeddings_longformer_base_4096.npy')\n",
    "print(len(embeddings))\n",
    "\n",
    "# 新的文本\n",
    "new_sentence = \"房間大\"\n",
    "inputs = tokenizer(new_sentence, return_tensors=\"pt\", truncation=True, max_length=512, padding=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "new_embedding = outputs.last_hidden_state.mean(dim=1).numpy().flatten()\n",
    "\n",
    "\n",
    "\n",
    "#     # 新的文本\n",
    "#     tokens = tokenizer.tokenize(sentence)  # 分詞\n",
    "#     inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**inputs)\n",
    "#     new_embedding = outputs.last_hidden_state.mean(dim=1).numpy().flatten()\n",
    "\n",
    "\n",
    "# 搜尋'南投縣'的前 5 個相似的飯店\n",
    "get_similar_hotels(df_hotel_info['縣市'] == '南投縣', embeddings, new_embedding, df_hotel_info, 5)\n",
    "\n",
    "# # 搜尋整體評分大於 8 的前 10 個相似的飯店\n",
    "# get_similar_hotels(hotel_data['整體評分'] > 8, embeddings, new_embedding, hotel_data, 10)\n",
    "\n",
    "# # 搜尋'南投縣'且整體評分大於 8 的前 15 個相似的飯店\n",
    "# get_similar_hotels((hotel_data['縣市'] == '南投縣') & (hotel_data['整體評分'] > 8), embeddings, new_embedding, hotel_data, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c291be26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "飯店名稱: 太魯閣阿騫的家民宿, 整體評分: 9.6\n",
      "飯店名稱: 鹿台民宿, 整體評分: 9.1\n",
      "飯店名稱: 花蓮綠舍 The Green Villa l 花蓮親子溜滑梯民宿, 整體評分: 8.8\n",
      "飯店名稱: 親水棧民宿, 整體評分: 8.6\n",
      "飯店名稱: 磐石旅店, 整體評分: 8.4\n",
      "飯店名稱: 雁窩民宿, 整體評分: 8.3\n",
      "飯店名稱: 朝日民宿 - Peng's Family, 整體評分: 8.0\n",
      "飯店名稱: 卡爾登飯店 the Carlton, 整體評分: 7.9\n",
      "飯店名稱: 樂活休閒海景飯店, 整體評分: 7.9\n"
     ]
    }
   ],
   "source": [
    "# top10_hotel_scores = []\n",
    "# for name in top10_hotel_name:\n",
    "#     score = df[df['飯店名稱'] == name]['整體評分'].values[0]\n",
    "#     top10_hotel_scores.append((name, score))\n",
    "\n",
    "# # 根據評分進行排序，評分高的在前\n",
    "# top10_hotel_scores_sorted = sorted(top10_hotel_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# # 印出排序後的飯店和評分\n",
    "# for name, score in top10_hotel_scores_sorted:\n",
    "#     print(f\"飯店名稱: {name}, 整體評分: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fbb50b",
   "metadata": {},
   "source": [
    "#### GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1fdeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import time\n",
    "\n",
    "openai.api_key = 'your-api-key'\n",
    "\n",
    "questions = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"Who wrote the book '1984'?\",\n",
    "    \"What is the distance from Earth to the Moon?\"\n",
    "]\n",
    "\n",
    "responses = []\n",
    "\n",
    "for question in questions:\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-004\",\n",
    "        prompt=question,\n",
    "        max_tokens=100\n",
    "    )\n",
    "    responses.append(response.choices[0].text.strip())\n",
    "    time.sleep(1)  # 避免在短时间内发送过多请求\n",
    "\n",
    "for question, answer in zip(questions, responses):\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f8a448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下載預訓練模型和分詞工具\n",
    "model = BertModel.from_pretrained('bert-base-chinese', ignore_mismatched_sizes=True)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "\n",
    "# 假設數據\n",
    "texts = [('今天天氣真好', '今天天氣很不錯'), ('你好早安', '我想回家')]\n",
    "scores = [1.0, 0.0]\n",
    "\n",
    "# 对数据进行编码\n",
    "input_ids = tokenizer(texts, return_tensors='pt', padding=True, truncation=True);input_ids\n",
    "\n",
    "# 分詞\n",
    "# input_ids = tokenizer.encode(\"你好，世界！\", add_special_tokens=True);input_ids\n",
    "\n",
    "# 創建 Tensors\n",
    "scores = torch.tensor(scores)\n",
    "\n",
    "# 使用模型得到句子的表示\n",
    "outputs = model(input_ids)\n",
    "\n",
    "# 計算句子表示的餘弦相似度\n",
    "similarity_scores = torch.nn.functional.cosine_similarity(outputs[0], outputs[1])\n",
    "\n",
    "# # 微调模型\n",
    "# optimizer = Adam(model.parameters())\n",
    "# loss = torch.nn.functional.mse_loss(similarity_scores, scores)\n",
    "# loss.backward()\n",
    "# optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df30a475",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\student\\anaconda3\\envs\\web_scraping\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "# 載入預訓練模型及其分詞器\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "model = BertModel.from_pretrained('bert-base-chinese')\n",
    "\n",
    "# 輸入你的文本\n",
    "text = \"請將我換成你想要的任何文本。\"\n",
    "\n",
    "# 使用分詞器將文本轉換為模型可以理解的形式\n",
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "# 使用模型獲得文本的表示\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# 'outputs' 是一個 tuple，我們需要的是文本表示，它被儲存在第一個元素中\n",
    "text_embedding = outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba0d84d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.7956,  0.8762, -0.1610,  ...,  1.1726, -0.1286, -0.5807],\n",
      "         [-0.7632,  0.2071,  0.1727,  ...,  0.1169, -0.9048,  0.0898],\n",
      "         [-0.9492,  0.2783,  0.9922,  ...,  1.1149, -0.1266,  0.0922],\n",
      "         ...,\n",
      "         [ 0.0631,  0.8451,  0.0971,  ...,  1.0712,  0.7042, -0.1742],\n",
      "         [-0.3739,  1.4240,  0.2010,  ...,  0.6602, -0.2418, -0.4565],\n",
      "         [-1.2477, -0.0717,  0.4747,  ...,  0.0043, -0.0576,  0.0467]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(text_embedding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
