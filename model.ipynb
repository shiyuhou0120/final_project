{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd36d42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install torch\n",
    "# !pip install sentence_transformers\n",
    "# !pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3011c116",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\student\\anaconda3\\envs\\web_scraping\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "from torch.optim import Adam\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0c81cd",
   "metadata": {},
   "source": [
    "### 原始評論(利用roberta原始的分詞方法)(會把每一個字都拆開)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b3c8a83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5514"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ = pd.read_csv('./booking_comments_分詞update.csv', header=0)\n",
    "\n",
    "# 篩選評論數大於等於 30 的飯店\n",
    "df_filtered = df_.groupby('飯店名稱').filter(lambda x: len(x) >= 30)\n",
    "\n",
    "len(df_filtered['飯店名稱'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0a32b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 開始測量(爬取時間)\n",
    "startime = datetime.datetime.now()\n",
    "\n",
    "# 初始化 RoBERTa-wwm-ext 模型和分詞器\n",
    "tokenizer = BertTokenizer.from_pretrained('hfl/chinese-roberta-wwm-ext')\n",
    "model = BertModel.from_pretrained('hfl/chinese-roberta-wwm-ext')\n",
    "\n",
    "# 評論文本\n",
    "sentences = list(df_filtered.loc[:, '綜合評論'])\n",
    "\n",
    "# 初始化一個空的 list 用於儲存飯店資訊\n",
    "hotel_info_list = []\n",
    "\n",
    "# 獲取每個文本的嵌入向量，並儲存對應的飯店資訊\n",
    "embeddings = []\n",
    "for idx, sentence in enumerate(sentences):\n",
    "    tokens = tokenizer.tokenize(sentence)  # 分詞\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "    # 檢查序列長度是否超過 512\n",
    "    if len(inputs[\"input_ids\"][0]) > 512:\n",
    "        print(\"The sentence is too long. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings.append(outputs.last_hidden_state.mean(dim=1).numpy().flatten())\n",
    "    hotel_info_list.append(df_filtered.iloc[idx].to_dict())\n",
    "\n",
    "# 儲存飯店資訊與 embeddings\n",
    "df_hotel_info = pd.DataFrame(hotel_info_list)\n",
    "df_hotel_info.to_csv('hotel_info_roberta.csv', index=False)\n",
    "np.save('embeddings_roberta.npy', embeddings)\n",
    "\n",
    "# 結束測量\n",
    "endtime = datetime.datetime.now()\n",
    "\n",
    "# 輸出結果\n",
    "print(\"執行時間：\", endtime - startime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d30566c",
   "metadata": {},
   "source": [
    "#### 利用迴圈跑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63782e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-roberta-wwm-ext were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1 of 11...\n",
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n",
      "執行時間： 0:04:15.174158\n",
      "Processing batch 2 of 11...\n",
      "The sentence is too long. Skipping...\n",
      "執行時間： 0:04:04.726883\n",
      "Processing batch 3 of 11...\n",
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n",
      "執行時間： 0:04:16.201854\n",
      "Processing batch 4 of 11...\n",
      "The sentence is too long. Skipping...\n",
      "執行時間： 0:04:07.544061\n",
      "Processing batch 5 of 11...\n",
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n",
      "執行時間： 0:04:17.144718\n",
      "Processing batch 6 of 11...\n",
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "import datetime\n",
    "\n",
    "df_ = pd.read_csv('./booking_comments_分詞update.csv', header=0)\n",
    "\n",
    "# 篩選評論數大於等於 30 的飯店\n",
    "df_filtered = df_.groupby('飯店名稱').filter(lambda x: len(x) >= 30)\n",
    "\n",
    "df_filtered = df_filtered[0:50000]\n",
    "\n",
    "df_filtered = df_filtered.reset_index(drop=True)\n",
    "\n",
    "# 初始化 RoBERTa-wwm-ext 模型和分詞器\n",
    "tokenizer = BertTokenizer.from_pretrained('hfl/chinese-roberta-wwm-ext')\n",
    "model = BertModel.from_pretrained('hfl/chinese-roberta-wwm-ext')\n",
    "\n",
    "# 定義每個批次的大小\n",
    "batch_size = 5000\n",
    "\n",
    "# 計算需要進行的批次數\n",
    "n_batches = (len(df_filtered) // batch_size) + 1\n",
    "\n",
    "# 開始進行批次處理\n",
    "for batch_idx in range(n_batches):\n",
    "    print(f\"Processing batch {batch_idx + 1} of {n_batches}...\")\n",
    "    \n",
    "    # 開始測量(時間)\n",
    "    startime = datetime.datetime.now()\n",
    "    \n",
    "    start_idx = batch_idx * batch_size\n",
    "\n",
    "    end_idx = (batch_idx + 1) * batch_size\n",
    "\n",
    "    # 評論文本\n",
    "    sentences = list(df_filtered.loc[start_idx:end_idx-1, '綜合評論'])\n",
    "\n",
    "    # 如果 sentences 是空的，則跳過該批次\n",
    "    if not sentences:\n",
    "        continue\n",
    "\n",
    "    # 初始化一個空的 list 用於儲存飯店資訊\n",
    "    hotel_info_list = []\n",
    "\n",
    "    # 獲取每個文本的嵌入向量，並儲存對應的飯店資訊\n",
    "    embeddings = []\n",
    "    for idx, sentence in enumerate(sentences):\n",
    "        if isinstance(sentence, str):\n",
    "            tokens = tokenizer.tokenize(sentence)  # 分詞\n",
    "            inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "        else:\n",
    "            print(f\"Sentence at index {idx} is not a string. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # 檢查序列長度是否超過 512\n",
    "        if len(inputs[\"input_ids\"][0]) > 512:\n",
    "            print(\"The sentence is too long. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        embeddings.append(outputs.last_hidden_state.mean(dim=1).numpy().flatten())\n",
    "        hotel_info_list.append(df_filtered.iloc[start_idx + idx].to_dict())\n",
    "\n",
    "    # 讀取原有的飯店資訊和 embeddings\n",
    "    if os.path.exists('hotel_info_roberta_綜合評論.csv') and os.path.exists('embeddings_roberta_綜合評論.npy'):\n",
    "        df_hotel_info_old = pd.read_csv('hotel_info_roberta_綜合評論.csv')\n",
    "        embeddings_old = np.load('embeddings_roberta_綜合評論.npy')\n",
    "\n",
    "    else:\n",
    "        df_hotel_info_old = pd.DataFrame()\n",
    "        embeddings_old = np.array([]).reshape(0,768)\n",
    "\n",
    "    # 儲存飯店資訊與 embeddings\n",
    "    df_hotel_info_new = pd.DataFrame(hotel_info_list)\n",
    "    df_hotel_info = pd.concat([df_hotel_info_old, df_hotel_info_new], ignore_index=True)\n",
    "    df_hotel_info.to_csv('hotel_info_roberta_綜合評論.csv', index=False)\n",
    "\n",
    "    embeddings_new = np.array(embeddings)\n",
    "    embeddings = np.vstack([embeddings_old, embeddings_new])\n",
    "    np.save('embeddings_roberta_綜合評論.npy', embeddings)\n",
    "    \n",
    "    # 結束測量\n",
    "    endtime = datetime.datetime.now()\n",
    "\n",
    "    # 輸出結果\n",
    "    print(\"執行時間：\", endtime - startime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edceaba2",
   "metadata": {},
   "source": [
    "### 讀取並進行篩選(先計算相似度再取平均)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "624ee49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "\n",
    "def get_similar_hotels(df, embeddings, filter_dict, sentence, top_n, batch_size=5000):\n",
    "    # 篩選\n",
    "    for key, value in filter_dict.items():\n",
    "        if key in df.columns:\n",
    "            operation, target_value = value\n",
    "            if operation == \"==\":\n",
    "                df = df[df[key] == target_value]\n",
    "            elif operation == \"!=\":\n",
    "                df = df[df[key] != target_value]\n",
    "            elif operation == \">\":\n",
    "                df = df[df[key] > target_value]\n",
    "            elif operation == \"<\":\n",
    "                df = df[df[key] < target_value]\n",
    "            elif operation == \">=\":\n",
    "                df = df[df[key] >= target_value]\n",
    "            elif operation == \"<=\":\n",
    "                df = df[df[key] <= target_value]\n",
    "\n",
    "    embeddings_filtered = embeddings[df.index.values]\n",
    "\n",
    "    # 新的文本\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    new_embedding = outputs.last_hidden_state.mean(dim=1).numpy().flatten()\n",
    "\n",
    "    # 計算與新文本的相似性\n",
    "    similarities = []\n",
    "    num_batches = len(embeddings_filtered) // batch_size + 1\n",
    "    for i in range(num_batches):\n",
    "        start_index = i * batch_size\n",
    "        end_index = start_index + batch_size\n",
    "        batch = embeddings_filtered[start_index:end_index]\n",
    "        batch_similarity = cosine_similarity(np.vstack((batch, new_embedding)))\n",
    "        similarities.append(batch_similarity[-1, :-1])\n",
    "    similarities = np.concatenate(similarities)\n",
    "\n",
    "    # 建立新的 DataFrame，儲存相似度與對應的飯店名稱\n",
    "    df_similarity = pd.DataFrame({'飯店名稱': df['飯店名稱'], '相似度': similarities})\n",
    "\n",
    "    # 以飯店名稱分組，計算每間飯店的平均相似度\n",
    "    df_avg_similarity = df_similarity.groupby('飯店名稱').mean()\n",
    "\n",
    "    # 取出平均相似度最高的前 top_n 間飯店\n",
    "    top_hotels = df_avg_similarity.sort_values(by='相似度', ascending=False).head(top_n)\n",
    "    \n",
    "    return top_hotels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d086b405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取 DataFrame\n",
    "df_hotel_info = pd.read_csv('hotel_info_roberta_綜合評論.csv')\n",
    "\n",
    "# 讀取 embeddings\n",
    "embeddings = np.load('embeddings_roberta_綜合評論.npy')\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('hfl/chinese-roberta-wwm-ext')\n",
    "model = BertModel.from_pretrained('hfl/chinese-roberta-wwm-ext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ee15a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      相似度\n",
      "飯店名稱                     \n",
      "磐石旅店             0.652121\n",
      "太魯閣阿騫的家民宿        0.651672\n",
      "鹿台民宿             0.651581\n",
      "貓咪民宿Mini館-貓行為諮詢  0.650581\n",
      "墾丁君臨農場           0.650515\n",
      "嘉義優遊商旅           0.650436\n",
      "星享道酒店            0.650195\n",
      "宜蘭明水露渡假民宿        0.650051\n",
      "台糖長榮酒店- 台南       0.649993\n",
      "漫遊舍民宿            0.649875\n"
     ]
    }
   ],
   "source": [
    "# 建立篩選條件字典\n",
    "filter_dict = {'整體評分': (\">\", 7.0)}\n",
    "\n",
    "# 使用函數\n",
    "top_hotels = get_similar_hotels(df_hotel_info, embeddings, filter_dict, \"房間好\", 10)\n",
    "\n",
    "print(top_hotels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238858e6",
   "metadata": {},
   "source": [
    "### 讀取並進行篩選(先對向量取平均再計算相似度)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d8c70de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算每間飯店的評論向量的平均值\n",
    "def compute_average_vectors(df, embeddings):\n",
    "    \n",
    "    df['embedding'] = list(embeddings)\n",
    "    df_avg_embedding = df.groupby('飯店名稱')['embedding'].apply(np.mean)\n",
    "\n",
    "    return df_avg_embedding\n",
    "\n",
    "# 根據篩選條件與新文本計算平均相似度\n",
    "def compute_similarity(df_avg_embedding, df, filter_dict, sentence):\n",
    "    \n",
    "    # 篩選\n",
    "    for key, value in filter_dict.items():\n",
    "        if key in df.columns:\n",
    "            operation, target_value = value\n",
    "            if operation == \"==\":\n",
    "                df = df[df[key] == target_value]\n",
    "            elif operation == \"!=\":\n",
    "                df = df[df[key] != target_value]\n",
    "            elif operation == \">\":\n",
    "                df = df[df[key] > target_value]\n",
    "            elif operation == \"<\":\n",
    "                df = df[df[key] < target_value]\n",
    "            elif operation == \">=\":\n",
    "                df = df[df[key] >= target_value]\n",
    "            elif operation == \"<=\":\n",
    "                df = df[df[key] <= target_value]\n",
    "\n",
    "    df_avg_embedding = df_avg_embedding[df_avg_embedding.index.isin(df['飯店名稱'])]\n",
    "\n",
    "    # 新的文本\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    new_embedding = outputs.last_hidden_state.mean(dim=1).numpy().flatten()\n",
    "\n",
    "    # 計算與新文本的相似性\n",
    "    similarities = cosine_similarity(np.vstack((df_avg_embedding.values.tolist(), new_embedding)))[-1, :-1]\n",
    "\n",
    "    # 建立新的 DataFrame，儲存相似度與對應的飯店名稱\n",
    "    df_similarity = pd.DataFrame({'飯店名稱': df_avg_embedding.index, '相似度': similarities})\n",
    "\n",
    "    return df_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84059542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取 DataFrame\n",
    "df_hotel_info = pd.read_csv('hotel_info_roberta_綜合評論.csv')\n",
    "\n",
    "# 讀取 embeddings\n",
    "embeddings = np.load('embeddings_roberta_綜合評論.npy')\n",
    "\n",
    "# 計算平均向量\n",
    "df_avg_embedding = compute_average_vectors(df_hotel_info, embeddings)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('hfl/chinese-roberta-wwm-ext')\n",
    "model = BertModel.from_pretrained('hfl/chinese-roberta-wwm-ext')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf24241e",
   "metadata": {},
   "source": [
    "#### 篩選並計算相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bee9a0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          飯店名稱       相似度\n",
      "31       町記憶旅店  0.706321\n",
      "0       168中壢館  0.705444\n",
      "33        磐石旅店  0.703912\n",
      "9       嘉義優遊商旅  0.703567\n",
      "30     班卡拉渡假旅店  0.703365\n",
      "23      星享道酒店   0.703294\n",
      "7   台糖長榮酒店- 台南  0.703290\n",
      "37        花田民宿  0.703198\n",
      "21     寒軒國際大飯店  0.702821\n",
      "1       九份輝明民宿  0.702528\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['貓咪民宿Mini館-貓行為諮詢', '星享道酒店 ', '花漾水舞休閒會館', '168中壢館', '台糖長榮酒店- 台南',\n",
       "       '全國大飯店', '鹿台民宿', '花蓮浪淘沙看海民宿', '大新精緻商旅', '良友精緻商旅', '合歡精緻商旅',\n",
       "       '漫遊舍民宿', '非凡假期大飯店', '台北漫步 - 西門館', '卡爾登飯店 the Carlton', '親水棧民宿',\n",
       "       '花蓮觀月民宿-寵物友善-含車位-現煮早餐', '墾丁旅店', '宜蘭明水露渡假民宿',\n",
       "       '花蓮綠舍 The Green Villa l 花蓮親子溜滑梯民宿', '樂活休閒海景飯店',\n",
       "       '依比鴨鴨水岸會館 Ducking House', '羅東雲朵朵Cloud B&B', '歸園田居', '寒軒國際大飯店',\n",
       "       '太魯閣阿騫的家民宿', '驛家旅店', '班卡拉渡假旅店', '墾丁很久以前海景民宿 ', '風華渡假旅館', '埔里西站旅舘',\n",
       "       '墾丁海園別館Hai Yuan Inn', '雁窩民宿', '百事達國際飯店', '晶悅精品旅館', '墾丁航海民宿',\n",
       "       '大地酒店', '九份輝明民宿', '海園二館', \"朝日民宿 - Peng's Family\", '亞哥之家', '陽光聖亞旅店',\n",
       "       '思泊客台北101', '町記憶旅店', '墾丁好時光', '花田民宿', '墾丁君臨農場', '磐石旅店', '嘉義優遊商旅'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 建立篩選條件字典\n",
    "filter_dict = {'整體評分': (\">\", 3.0)}\n",
    "\n",
    "df_similarity = compute_similarity(df_avg_embedding, df_hotel_info, filter_dict, \"早餐好吃\")\n",
    "top_hotels = df_similarity.sort_values(by='相似度', ascending=False).head(10)\n",
    "\n",
    "print(top_hotels)\n",
    "\n",
    "# print(len(df_avg_embedding))\n",
    "# print(len(df_hotel_info))\n",
    "# print(len(embeddings))\n",
    "df_hotel_info['飯店名稱'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aad9583",
   "metadata": {},
   "source": [
    "### 測試自行斷詞後的結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45b12aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('0_10000.csv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c19b6062",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-roberta-wwm-ext were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n",
      "相似的文本 (索引 9302): ['房間', '小'], 相似度: 0.9714384078979492\n",
      "相似的文本 (索引 7713): ['房間', '算', '大'], 相似度: 0.9639410376548767\n",
      "相似的文本 (索引 6559): ['房間', '很', '大'], 相似度: 0.9479893445968628\n",
      "相似的文本 (索引 6710): ['房間', '很', '大'], 相似度: 0.9479893445968628\n",
      "相似的文本 (索引 6730): ['房間', '很', '大'], 相似度: 0.9479893445968628\n",
      "相似的文本 (索引 3851): ['房間', '很', '大'], 相似度: 0.9479893445968628\n",
      "相似的文本 (索引 7572): ['房間', '很', '大'], 相似度: 0.9479893445968628\n",
      "相似的文本 (索引 6796): ['房間', '很', '大'], 相似度: 0.9479893445968628\n",
      "相似的文本 (索引 9294): ['房間', '偏', '小'], 相似度: 0.927994966506958\n",
      "相似的文本 (索引 4188): ['房間', '稍', '小'], 相似度: 0.9203976392745972\n",
      "相似的文本 (索引 6951): ['乾淨', '房間', '大'], 相似度: 0.9122157096862793\n",
      "相似的文本 (索引 1642): ['房間', '太', '小'], 相似度: 0.900887131690979\n",
      "相似的文本 (索引 7985): ['房間', '大小'], 相似度: 0.8921487331390381\n",
      "相似的文本 (索引 1403): ['房間', '大小'], 相似度: 0.8921487331390381\n",
      "相似的文本 (索引 8195): ['房間', '大小'], 相似度: 0.8921487331390381\n",
      "相似的文本 (索引 6233): ['便宜', '房間', '大'], 相似度: 0.8913284540176392\n",
      "相似的文本 (索引 8610): ['房間', '小', '床', '很', '小'], 相似度: 0.8826092481613159\n",
      "相似的文本 (索引 774): ['房間', '很', '棒'], 相似度: 0.8820592164993286\n",
      "相似的文本 (索引 801): ['房間', '寬敞'], 相似度: 0.8808667659759521\n",
      "相似的文本 (索引 7569): ['房間', '乾淨', '和', '大'], 相似度: 0.8764951825141907\n",
      "相似的文本 (索引 992): ['房間', '浴室', '都', '很', '大'], 相似度: 0.8760460615158081\n",
      "相似的文本 (索引 6389): ['房間', '大', '舒適', '無'], 相似度: 0.8751977682113647\n",
      "相似的文本 (索引 436): ['房間', '很', '大', '太', '冷'], 相似度: 0.8732443451881409\n",
      "相似的文本 (索引 7235): ['房間', '大', '舒適'], 相似度: 0.8698259592056274\n",
      "相似的文本 (索引 7696): ['房間', '超大', '寬敞'], 相似度: 0.8666406273841858\n",
      "相似的文本 (索引 825): ['房間', '寬敞', '無'], 相似度: 0.8617373108863831\n",
      "相似的文本 (索引 7113): ['房間', '夠', '大', '舒服'], 相似度: 0.8598159551620483\n",
      "相似的文本 (索引 4631): ['沒', '窗戶', '房間', '小'], 相似度: 0.8595103621482849\n",
      "相似的文本 (索引 5244): ['房間', '格局', '很', '大', '舒適'], 相似度: 0.8584761619567871\n",
      "相似的文本 (索引 659): ['房', '內', '整體'], 相似度: 0.8537802696228027\n",
      "相似的文本 (索引 6986): ['房間', '格局'], 相似度: 0.8535634279251099\n",
      "相似的文本 (索引 5326): ['乾凈', '舒適', '房間', '大'], 相似度: 0.8530415296554565\n",
      "相似的文本 (索引 4075): ['房間', '不錯'], 相似度: 0.8530404567718506\n",
      "相似的文本 (索引 7602): ['床', '夠', '大'], 相似度: 0.8511996269226074\n",
      "相似的文本 (索引 8480): ['床', '夠', '大'], 相似度: 0.8511996269226074\n",
      "相似的文本 (索引 7678): ['床', '夠', '大'], 相似度: 0.8511996269226074\n",
      "相似的文本 (索引 2697): ['浴室', '大'], 相似度: 0.8497729301452637\n",
      "相似的文本 (索引 5879): ['都', '還', '可', '床', '與', '房間', '大小'], 相似度: 0.8483333587646484\n",
      "相似的文本 (索引 2580): ['房間', '大', '沒', '壓迫感'], 相似度: 0.8476015329360962\n",
      "相似的文本 (索引 4699): ['房間', '設施', '有', '蟲'], 相似度: 0.8450119495391846\n",
      "相似的文本 (索引 1810): ['車位', '不', '夠', '房間', '偏', '暗'], 相似度: 0.8447525501251221\n",
      "相似的文本 (索引 3250): ['空間', '小'], 相似度: 0.8442726731300354\n",
      "相似的文本 (索引 647): ['房', '內', '空間', '不', '太', '大'], 相似度: 0.8430776000022888\n",
      "相似的文本 (索引 7130): ['房間', '很', '大', '很', '寬敞'], 相似度: 0.842853307723999\n",
      "相似的文本 (索引 6667): ['浴室', '超大', '沒有'], 相似度: 0.840947687625885\n",
      "相似的文本 (索引 7133): ['床', '很', '大'], 相似度: 0.8402475714683533\n",
      "相似的文本 (索引 8936): ['房間', '隔音', '差'], 相似度: 0.8391607999801636\n",
      "相似的文本 (索引 860): ['房間', '很', '大', '床組', '很', '好', '睡'], 相似度: 0.837617039680481\n",
      "相似的文本 (索引 1551): ['床', '軟', '好', '睡', '房間', '有點', '小'], 相似度: 0.8361782431602478\n",
      "相似的文本 (索引 5521): ['人房', '空間', '大', '床', '好', '睡'], 相似度: 0.8358633518218994\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('hfl/chinese-roberta-wwm-ext')\n",
    "model = BertModel.from_pretrained('hfl/chinese-roberta-wwm-ext')\n",
    "\n",
    "sentences_tokenized = [eval(d) for d in df.loc[:, \"綜合評論_ws\"].values]\n",
    "\n",
    "# 獲取每個文本的嵌入向量\n",
    "embeddings = []\n",
    "sentences = []  # 新建一個列表來保存被處理的句子\n",
    "\n",
    "for sentence in sentences_tokenized:\n",
    "    \n",
    "    inputs = tokenizer.encode_plus(sentence, is_split_into_words=True, return_tensors=\"pt\")\n",
    "    \n",
    "    # 檢查序列長度是否超過 512\n",
    "    if len(inputs[\"input_ids\"][0]) > 512:\n",
    "        print(\"The sentence is too long. Skipping...\")\n",
    "        continue\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings.append(outputs.last_hidden_state.mean(dim = 1).numpy().flatten())\n",
    "    sentences.append(sentence)  # 將被處理的句子加到列表中\n",
    "\n",
    "# 新的文本\n",
    "new_sentence = \"房間大\"\n",
    "new_tokens = ['房間', '大'] # 分詞\n",
    "inputs = tokenizer(new_tokens, is_split_into_words=True, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "new_embedding = outputs.last_hidden_state.mean(dim=1).numpy().flatten()\n",
    "\n",
    "# 計算與新文本的相似性\n",
    "similarities = cosine_similarity(embeddings + [new_embedding])\n",
    "\n",
    "# 獲取最相似的 50 個文本的索引\n",
    "top50_indices = np.argsort(similarities[-1][:-1])[:-51:-1]\n",
    "\n",
    "# 印出最相似的 50 個文本\n",
    "for i, index in enumerate(top50_indices):\n",
    "    print(f\"相似的文本 (索引 {index}): {sentences[index]}, 相似度: {similarities[-1][index]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca29473a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "相似的文本 (索引 4343): ['早餐', '好吃'], 相似度: 1.000000238418579\n",
      "相似的文本 (索引 1529): ['早餐', '好吃'], 相似度: 1.000000238418579\n",
      "相似的文本 (索引 6567): ['早餐', '好吃'], 相似度: 1.000000238418579\n",
      "相似的文本 (索引 7509): ['早餐', '好吃'], 相似度: 1.000000238418579\n",
      "相似的文本 (索引 4348): ['早餐', '好吃'], 相似度: 1.000000238418579\n",
      "相似的文本 (索引 6958): ['早餐', '好吃'], 相似度: 1.000000238418579\n",
      "相似的文本 (索引 6491): ['早餐', '好吃'], 相似度: 1.000000238418579\n",
      "相似的文本 (索引 4098): ['早餐', '好吃'], 相似度: 1.000000238418579\n",
      "相似的文本 (索引 2429): ['早餐', '好吃'], 相似度: 1.000000238418579\n",
      "相似的文本 (索引 4827): ['早餐', '好吃'], 相似度: 1.000000238418579\n",
      "相似的文本 (索引 4190): ['早餐', '很', '好吃'], 相似度: 0.9652129411697388\n",
      "相似的文本 (索引 6052): ['早餐', '很', '好吃'], 相似度: 0.9652129411697388\n",
      "相似的文本 (索引 5181): ['早餐', '很', '好吃'], 相似度: 0.9652129411697388\n",
      "相似的文本 (索引 6429): ['早餐', '很', '好吃'], 相似度: 0.9652129411697388\n",
      "相似的文本 (索引 4300): ['早餐', '很', '好吃'], 相似度: 0.9652129411697388\n",
      "相似的文本 (索引 9561): ['早餐', '超', '好吃'], 相似度: 0.9607133865356445\n",
      "相似的文本 (索引 1245): ['早餐', '不', '好吃'], 相似度: 0.9587073922157288\n",
      "相似的文本 (索引 9445): ['早餐', '很', '好吃', '呦'], 相似度: 0.9332779049873352\n",
      "相似的文本 (索引 535): ['早餐', '很', '好'], 相似度: 0.9180421829223633\n",
      "相似的文本 (索引 1106): ['乾淨', '早餐', '好吃'], 相似度: 0.9138054847717285\n",
      "相似的文本 (索引 4362): ['早餐', '很', '棒'], 相似度: 0.9111084938049316\n",
      "相似的文本 (索引 4333): ['早餐', '很', '棒'], 相似度: 0.9111084938049316\n",
      "相似的文本 (索引 1109): ['早餐', '很', '棒'], 相似度: 0.9111084938049316\n",
      "相似的文本 (索引 599): ['早餐', '很', '棒'], 相似度: 0.9111084938049316\n",
      "相似的文本 (索引 9901): ['早餐', '很', '棒'], 相似度: 0.9111084938049316\n",
      "相似的文本 (索引 4880): ['早餐', '很', '棒'], 相似度: 0.9111084938049316\n",
      "相似的文本 (索引 741): ['早餐', '很', '棒'], 相似度: 0.9111084938049316\n",
      "相似的文本 (索引 4101): ['早餐', '很', '棒'], 相似度: 0.9111084938049316\n",
      "相似的文本 (索引 5153): ['早餐', '豐盛', '好吃'], 相似度: 0.909956693649292\n",
      "相似的文本 (索引 6007): ['早餐', '中西式', '都', '好吃'], 相似度: 0.9071521162986755\n",
      "相似的文本 (索引 4192): ['現', '煮', '早餐', '好好', '吃'], 相似度: 0.9032139778137207\n",
      "相似的文本 (索引 4140): ['早餐', '超棒'], 相似度: 0.9013780951499939\n",
      "相似的文本 (索引 1554): ['早餐', '好吃', '沒有'], 相似度: 0.9006266593933105\n",
      "相似的文本 (索引 7478): ['早餐', '好吃', '位置', '很', '好'], 相似度: 0.8989447951316833\n",
      "相似的文本 (索引 129): ['很', '棒', '的', '早餐'], 相似度: 0.8969095945358276\n",
      "相似的文本 (索引 6409): ['早餐', '豐盛', '好吃', '無'], 相似度: 0.8909722566604614\n",
      "相似的文本 (索引 9638): ['早餐', '很', '好吃', '很', '飽'], 相似度: 0.8906499743461609\n",
      "相似的文本 (索引 1098): ['早餐', '豐富', '美味'], 相似度: 0.8905787467956543\n",
      "相似的文本 (索引 4414): ['早餐', '好吃', '很', '豐盛'], 相似度: 0.8902044296264648\n",
      "相似的文本 (索引 4358): ['健康', '早餐', '很', '棒'], 相似度: 0.8896884322166443\n",
      "相似的文本 (索引 2910): ['早餐', '好吃值', '頗', '高'], 相似度: 0.8879324197769165\n",
      "相似的文本 (索引 4216): ['早餐', '很', '好吃', '超棒', '的'], 相似度: 0.8856137990951538\n",
      "相似的文本 (索引 7349): ['早餐', '很', '豐富'], 相似度: 0.8758634924888611\n",
      "相似的文本 (索引 1116): ['早餐', '超級', '好'], 相似度: 0.8735265731811523\n",
      "相似的文本 (索引 805): ['早餐', '非常', '棒'], 相似度: 0.8723956346511841\n",
      "相似的文本 (索引 4210): ['早餐', '非常', '棒'], 相似度: 0.8723956346511841\n",
      "相似的文本 (索引 1820): ['早餐', '非常', '不', '可口'], 相似度: 0.8720617890357971\n",
      "相似的文本 (索引 4296): ['豐盛', '的', '早餐'], 相似度: 0.8715789318084717\n",
      "相似的文本 (索引 757): ['早餐', '很', '豐盛'], 相似度: 0.8710860013961792\n",
      "相似的文本 (索引 8078): ['早餐', '很', '豐盛'], 相似度: 0.8710860013961792\n"
     ]
    }
   ],
   "source": [
    "# 新的文本\n",
    "new_sentence = \"早餐好吃\"\n",
    "new_tokens = ['早餐', '好吃'] # 分詞\n",
    "inputs = tokenizer(new_tokens, is_split_into_words=True, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "new_embedding = outputs.last_hidden_state.mean(dim=1).numpy().flatten()\n",
    "\n",
    "# 計算與新文本的相似性\n",
    "similarities = cosine_similarity(embeddings + [new_embedding])\n",
    "\n",
    "# 獲取最相似的 50 個文本的索引\n",
    "top50_indices = np.argsort(similarities[-1][:-1])[:-51:-1]\n",
    "\n",
    "# 印出最相似的 50 個文本\n",
    "for i, index in enumerate(top50_indices):\n",
    "    print(f\"相似的文本 (索引 {index}): {sentences[index]}, 相似度: {similarities[-1][index]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acea954e",
   "metadata": {},
   "source": [
    "##### 結果不是很好\n",
    "\n",
    "##### 無法有效判斷語意的相似度\n",
    "\n",
    "##### 這個現象的出現是由於您在計算評論與新評論之間的相似度時，採用了cosine similarity。這種相似度衡量的是兩個向量之間的角度，即它們的方向是否相似，而不是他們的長度或大小\n",
    "##### 如果您希望找到與新評論在語義上更相似的評論，一種可能的方法是使用一種能夠捕捉語義相似度的相似度度量，比如Word Mover's Distance。Word Mover's Distance是一種在詞嵌入空間中度量文本之間距離的方法，它可以捕捉詞與詞之間的相似性，並利用這種相似性來度量文本之間的距離。\n",
    "\n",
    "##### 另一種可能的方法是，不僅僅依賴於評論的平均詞向量來表示評論。例如，您可以使用一些更複雜的方法來獲得評論的向量表示，比如使用Doc2Vec模型，或者將BERT模型中不同層的輸出合併起來，這可能會獲得更豐富的表示。這樣，即使評論的長度不同，也能得到更好的結果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483fdb21",
   "metadata": {},
   "source": [
    "#### 使用Gensim的Doc2Vec模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d22f35c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1 相似的文本 (索引 [283]): ['住', '頂樓', '海景房', '真的', '是', '很', '賭', '運氣', '跟', '天氣', '的', '房型', '如果', '天氣', '好', '無敵', '海景', '第一', '排', '真的', '很', '美', '可惜', '當天', '入住', '外圍', '環流', '下雨天', '天氣', '非常', '糟糕', '也', '因為', '下雨天', '這', '間', '房型', '的', '缺點', '完全', '顯現', '雨水', '打', '在', '頂樓', '鐵皮屋', '聲音', '非常', '大', '其實', '外面', '沒有', '到', '很', '大', '的', '雨', '但', '在', '房', '內', '卻', '感覺', '是', '滂沱大雨', '整', '晚', '睡', '不', '好', '房', '內', '有', '小', '蟲', '床', '上', '有', '螞蟻', '不', '知道', '是', '不', '是', '因為', '下雨', '所以', '都', '跑出來', '了', '是', '我們', '自己', '運氣', '不', '好', '遇到', '雨天', '所以', '雨天', '在', '這', '間', '房', '內', '就', '變', '的', '比', '其它', '房型', '值', '還要', '低', '雖然', '隔天', '早上', '雨', '停', '了', '但', '照片', '都', '是', '黃黃', '的', '海水', '很', '可惜'], 相似度: 0.13586002588272095\n",
      "第 2 相似的文本 (索引 [83]): ['入住', '三', '次', '了', '每', '次', '都', '是', '兩', '晚', '以上', '覺得', '不錯', '房間', '細節', '的', '地方', '可以', '在', '加強', '喔'], 相似度: 0.12522174417972565\n",
      "第 3 相似的文本 (索引 [123]): ['整體', '還', '不錯', '床', '很', '舒服', '整體', '配置', '還', '不錯', '這', '次', '住', '的', '是', '四', '人', '房', '上下舖', '但是', '很', '舒適', '床位', '都', '有', '充電孔', '跟', '檯燈', '位置', '有點', '難找', '四', '人', '房', '空間', '略', '小', '了', '點'], 相似度: 0.12299995124340057\n",
      "第 4 相似的文本 (索引 [239]): ['水龍頭', '水量', '小', '了', '點', '若', '有', '早餐', '更', '讚'], 相似度: 0.12274613231420517\n",
      "第 5 相似的文本 (索引 [125]): ['服務', '很', '親切', '老闆娘', '服務', '態度', '很', '好', '讓', '人', '很', '放鬆', '的', '一', '家', '旅店'], 相似度: 0.120293527841568\n",
      "第 6 相似的文本 (索引 [224]): ['老闆', '待人', '親切', '早餐', '很', '用心', '在', '準備', '怕', '我們', '吃', '不', '飽', '很', '沒有'], 相似度: 0.1197710707783699\n",
      "第 7 相似的文本 (索引 [116]): ['房間', '寬敞', '又', '乾淨', '且', '有', '三溫暖', '適合', '紓壓', '的', '好', '飯店', '泳池', '若是', '室', '內', '就', '更', '棒', '了'], 相似度: 0.1191958412528038\n",
      "第 8 相似的文本 (索引 [189]): ['可以'], 相似度: 0.11368517577648163\n",
      "第 9 相似的文本 (索引 [43]): ['很', '雷', '的', '一', '次', '住宿', '體驗', '電梯', '進入', '會', '下沉', '電梯', '地板', '還', '會', '有', '異聲', '服務', '人員', '很', '客氣', '水壓', '不', '夠', '洗澡', '洗', '半', '會', '完全', '沒', '水', '只', '能', '乾', '等', '水', '蓮蓬頭', '壞', '了', '也', '沒有', '維護', '房', '內', '床舖', '很', '潮濕', '朋友', '的', '床位', '還', '有', '蟲'], 相似度: 0.1134708896279335\n",
      "第 10 相似的文本 (索引 [179]): ['整體', '感受', '還', '不錯', '服務', '態度', '電梯', '有', '卡片', '限制', '出入口', '有', '人', '管制', '安全性', '高', '飯店', '內', '盡然', '沒有', '洗衣間', '雖然', '有', '代洗', '服務', '但', '要', '隔天', '快', '中午', '才', '能', '回來'], 相似度: 0.11303973197937012\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# 分詞後評論\n",
    "sentences_tokenized = [eval(d) for d in df.loc[:, \"綜合評論_ws\"].values]\n",
    "\n",
    "# 準備訓練數據(轉成模型會吃的樣子)\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(sentences_tokenized)]\n",
    "\n",
    "# 訓練 Doc2Vec 模型\n",
    "model = Doc2Vec(documents, vector_size=50, min_count=2, epochs=40)\n",
    "\n",
    "# 现在，我们可以用模型获取句子的向量表示\n",
    "embeddings = [model.infer_vector(doc) for doc in sentences_tokenized]\n",
    "\n",
    "# 对新的句子执行相同的操作\n",
    "new_sentence = \"早餐不好\"\n",
    "new_embedding = model.infer_vector(new_sentence.split())\n",
    "\n",
    "# 将嵌入向量列表转换为 numpy 数组，以便我们可以使用 cosine_similarity 函数\n",
    "embeddings_np = np.array(embeddings)\n",
    "\n",
    "# 计算新的句子和已有句子的相似度\n",
    "similarities = cosine_similarity(embeddings_np, new_embedding.reshape(1, -1))\n",
    "\n",
    "# 以降序排列相似度\n",
    "top_indices = np.argsort(similarities, axis=0)[::-1]\n",
    "\n",
    "# 输出最相似的 10 个句子\n",
    "for i, index in enumerate(top_indices[:10]):\n",
    "    print(f\"第 {i+1} 相似的文本 (索引 {index}): {sentences_tokenized[index[0]]}, 相似度: {similarities[index[0]][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b1ae46",
   "metadata": {},
   "source": [
    "#### 使用BERT的不同層输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fee133fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_tokenized_ = [\" \".join(eval(d)) for d in df_.loc[0:50, \"綜合評論_ws\"].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8c622770",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-roberta-wwm-ext were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1 相似的文本 (索引 [45]): 早餐 超 豐盛 超, 相似度: 0.6999987363815308\n",
      "第 2 相似的文本 (索引 [3]): 沒有 附 早餐, 相似度: 0.6805572509765625\n",
      "第 3 相似的文本 (索引 [22]): 滿意, 相似度: 0.5698429346084595\n",
      "第 4 相似的文本 (索引 [23]): 沒有 髒, 相似度: 0.558742105960846\n",
      "第 5 相似的文本 (索引 [35]): 地 上 有 頭髮 不 夠 乾淨, 相似度: 0.5520568490028381\n",
      "第 6 相似的文本 (索引 [39]): 房間 升級 房間 及 衛浴 空間 舒適 早餐 每 天 都 是 培根 起司 口味, 相似度: 0.5247654914855957\n",
      "第 7 相似的文本 (索引 [40]): 草地 夠 大 風 好 大, 相似度: 0.5237064957618713\n",
      "第 8 相似的文本 (索引 [17]): 地點 方便 停車位 多 早餐 也 很 不錯 補菜 的 速度 也 很 快 早餐 沒有 醬瓜 房間 太 昏暗, 相似度: 0.5115199089050293\n",
      "第 9 相似的文本 (索引 [1]): 整體 上 都 不錯 無 飲水機 有點 不 方便, 相似度: 0.5094053745269775\n",
      "第 10 相似的文本 (索引 [8]): 有 桌遊 可以 玩, 相似度: 0.5080175399780273\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('hfl/chinese-roberta-wwm-ext')\n",
    "model = BertModel.from_pretrained('hfl/chinese-roberta-wwm-ext', output_hidden_states=True)\n",
    "\n",
    "embeddings = []\n",
    "for sentence in sentences_tokenized_:\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # 获取最后四层的输出\n",
    "    last_four_layers = outputs.hidden_states[-4:]\n",
    "    # 将它们合并起来\n",
    "    embedding = torch.mean(torch.stack(last_four_layers), dim=0).mean(dim=1).numpy()\n",
    "    embeddings.append(embedding.squeeze())  # 添加这个改动\n",
    "\n",
    "new_tokens = ['早餐', '不好'] # 分詞\n",
    "inputs = tokenizer(new_tokens, is_split_into_words=True, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "last_four_layers = outputs.hidden_states[-4:]\n",
    "new_embedding = torch.mean(torch.stack(last_four_layers), dim=0).mean(dim=1).numpy()\n",
    "\n",
    "embeddings_np = np.array(embeddings)\n",
    "\n",
    "similarities = cosine_similarity(embeddings_np, new_embedding.reshape(1, -1))\n",
    "top_indices = np.argsort(similarities, axis=0)[::-1]\n",
    "\n",
    "for i, index in enumerate(top_indices[:10]):\n",
    "    print(f\"第 {i+1} 相似的文本 (索引 {index}): {sentences_tokenized_[index[0]]}, 相似度: {similarities[index[0]][0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec0272d",
   "metadata": {},
   "source": [
    "### 觀察自行分詞與bert內建分詞的差異"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5d4c0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['房', '間', '小']\n",
      "{'input_ids': tensor([[ 101, 2791,  102],\n",
      "        [ 101, 7279,  102],\n",
      "        [ 101, 2207,  102]]), 'token_type_ids': tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1]])}\n",
      "['房間', '小']\n",
      "{'input_ids': tensor([[ 101, 2791, 7279, 2207,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "new_sentence = \"房間小\"\n",
    "\n",
    "# inputs = tokenizer(new_sentence, return_tensors=\"pt\")\n",
    "new_tokens = tokenizer.tokenize(new_sentence)\n",
    "inputs = tokenizer(new_tokens, return_tensors=\"pt\")\n",
    "\n",
    "print(new_tokens)\n",
    "print(inputs)\n",
    "\n",
    "new_tokens = ['房間', '小'] # 分詞\n",
    "inputs = tokenizer(new_tokens, is_split_into_words=True, return_tensors=\"pt\")\n",
    "\n",
    "print(new_tokens)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9b1224",
   "metadata": {},
   "source": [
    "#### 嘗試只保留有意義的詞性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36443dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('0_10000.csv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2676eaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_pos = set(['Nep', 'Nh', 'Nb'])  # 這 3 種詞性不保留\n",
    "\n",
    "# 分詞後評論\n",
    "sentences_tokenized = [eval(d) for d in df.loc[:, \"綜合評論_ws\"].values]\n",
    "\n",
    "# 分詞後詞性\n",
    "pos_tags = [eval(d) for d in df.loc[:, \"綜合評論_pos\"].values]\n",
    "\n",
    "short_sentence = []  # 放過濾後的句子\n",
    "\n",
    "for sentence, sentence_pos_tags in zip(sentences_tokenized, pos_tags):\n",
    "\n",
    "    short_sentence_sentence = []  # 存储这个句子过滤后的词（不带词性）\n",
    "    \n",
    "    for word_ws, word_pos in zip(sentence, sentence_pos_tags):\n",
    "        \n",
    "        # 只留名詞和動詞\n",
    "        is_N_or_V = word_pos.startswith(\"V\") or word_pos.startswith(\"N\")\n",
    "\n",
    "        # 去掉名詞裡的某些詞性\n",
    "        is_not_stop_pos = word_pos not in stop_pos\n",
    "\n",
    "        if is_N_or_V and is_not_stop_pos:\n",
    "            \n",
    "            short_sentence_sentence.append(f\"{word_ws}\")\n",
    "\n",
    "    short_sentence.append(short_sentence_sentence)\n",
    "\n",
    "# print(sentences_tokenized[0:3])\n",
    "# print(short_sentence[0:3])\n",
    "\n",
    "# 去除空的列表\n",
    "short_sentence = [sentence for sentence in short_sentence if sentence]  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a0feb1",
   "metadata": {},
   "source": [
    "### 利用篩選詞性後的結果做 roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9ebfee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-roberta-wwm-ext were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentence is too long. Skipping...\n",
      "相似的文本 (索引 3834): ['房間', '大'], 相似度: 0.9999998211860657\n",
      "相似的文本 (索引 6677): ['房間', '大'], 相似度: 0.9999998211860657\n",
      "相似的文本 (索引 7534): ['房間', '大'], 相似度: 0.9999998211860657\n",
      "相似的文本 (索引 6762): ['房間', '大'], 相似度: 0.9999998211860657\n",
      "相似的文本 (索引 6697): ['房間', '大'], 相似度: 0.9999998211860657\n",
      "相似的文本 (索引 6527): ['房間', '大'], 相似度: 0.9999998211860657\n",
      "相似的文本 (索引 4170): ['房間', '小'], 相似度: 0.9714384078979492\n",
      "相似的文本 (索引 9246): ['房間', '小'], 相似度: 0.9714384078979492\n",
      "相似的文本 (索引 1637): ['房間', '小'], 相似度: 0.9714384078979492\n",
      "相似的文本 (索引 7674): ['房間', '算', '大'], 相似度: 0.9639410376548767\n",
      "相似的文本 (索引 8884): ['房間', '差'], 相似度: 0.9296784996986389\n",
      "相似的文本 (索引 9238): ['房間', '偏', '小'], 相似度: 0.927994966506958\n",
      "相似的文本 (索引 989): ['房間', '浴室', '大'], 相似度: 0.9196709394454956\n",
      "相似的文本 (索引 8559): ['房間', '小', '床', '小'], 相似度: 0.9166770577430725\n",
      "相似的文本 (索引 6916): ['乾淨', '房間', '大'], 相似度: 0.9122157096862793\n",
      "相似的文本 (索引 7094): ['房間', '大', '寬敞'], 相似度: 0.9050126075744629\n",
      "相似的文本 (索引 5854): ['床', '房間', '大小'], 相似度: 0.8997808694839478\n",
      "相似的文本 (索引 7943): ['房間', '大小'], 相似度: 0.8921487331390381\n",
      "相似的文本 (索引 1398): ['房間', '大小'], 相似度: 0.8921487331390381\n",
      "相似的文本 (索引 8146): ['房間', '大小'], 相似度: 0.8921487331390381\n",
      "相似的文本 (索引 6206): ['便宜', '房間', '大'], 相似度: 0.8913284540176392\n",
      "相似的文本 (索引 771): ['房間', '棒'], 相似度: 0.8889296054840088\n",
      "相似的文本 (索引 6279): ['寬敞', '房間', '好'], 相似度: 0.886948823928833\n",
      "相似的文本 (索引 435): ['房間', '大', '冷'], 相似度: 0.8835815787315369\n",
      "相似的文本 (索引 4534): ['房間', '大', '舒服'], 相似度: 0.8827222585678101\n",
      "相似的文本 (索引 7077): ['房間', '大', '舒服'], 相似度: 0.8827222585678101\n",
      "相似的文本 (索引 7657): ['房間', '寬敞'], 相似度: 0.8808667659759521\n",
      "相似的文本 (索引 798): ['房間', '寬敞'], 相似度: 0.8808667659759521\n",
      "相似的文本 (索引 646): ['房', '內', '空間', '大'], 相似度: 0.880531907081604\n",
      "相似的文本 (索引 6360): ['房間', '大', '舒適', '無'], 相似度: 0.8751977682113647\n",
      "相似的文本 (索引 6103): ['浴室', '大', '房間', '小', '差'], 相似度: 0.8750342726707458\n",
      "相似的文本 (索引 6720): ['房間', '吵', '差'], 相似度: 0.8724051713943481\n",
      "相似的文本 (索引 4694): ['陽台', '棒', '房間', '小'], 相似度: 0.8704512119293213\n",
      "相似的文本 (索引 7814): ['房間', '大', '舒適'], 相似度: 0.8698259592056274\n",
      "相似的文本 (索引 8387): ['房間', '大', '舒適'], 相似度: 0.8698259592056274\n",
      "相似的文本 (索引 7198): ['房間', '大', '舒適'], 相似度: 0.8698259592056274\n",
      "相似的文本 (索引 7531): ['房間', '乾淨', '大'], 相似度: 0.8679437637329102\n",
      "相似的文本 (索引 6196): ['房間', '燈', '暗'], 相似度: 0.8660553693771362\n",
      "相似的文本 (索引 492): ['浴室', '大', '寬敞', '無'], 相似度: 0.8654581904411316\n",
      "相似的文本 (索引 8710): ['房間', '大', '舒適', '沒有'], 相似度: 0.8630586266517639\n",
      "相似的文本 (索引 8040): ['房間', '空間', '大', '住', '舒服'], 相似度: 0.8618459701538086\n",
      "相似的文本 (索引 822): ['房間', '寬敞', '無'], 相似度: 0.8617373108863831\n",
      "相似的文本 (索引 597): ['房間', '舒服', '好'], 相似度: 0.8612924814224243\n",
      "相似的文本 (索引 4612): ['沒', '窗戶', '房間', '小'], 相似度: 0.8595103621482849\n",
      "相似的文本 (索引 1805): ['車位', '夠', '房間', '偏', '暗'], 相似度: 0.858141303062439\n",
      "相似的文本 (索引 7589): ['房間', '不錯', '睡'], 相似度: 0.8581225275993347\n",
      "相似的文本 (索引 1594): ['房間', '舒適', '差'], 相似度: 0.8569594621658325\n",
      "相似的文本 (索引 8981): ['房間', '海景', '棒'], 相似度: 0.854246973991394\n",
      "相似的文本 (索引 658): ['房', '內', '整體'], 相似度: 0.8537802696228027\n",
      "相似的文本 (索引 6950): ['房間', '格局'], 相似度: 0.8535634279251099\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('hfl/chinese-roberta-wwm-ext')\n",
    "model = BertModel.from_pretrained('hfl/chinese-roberta-wwm-ext')\n",
    "\n",
    "# 獲取每個文本的嵌入向量\n",
    "embeddings = []\n",
    "sentences = []  # 新建一個列表來保存被處理的句子\n",
    "\n",
    "for sentence in short_sentence:\n",
    "    \n",
    "    inputs = tokenizer.encode_plus(sentence, is_split_into_words=True, return_tensors=\"pt\")\n",
    "    \n",
    "    # 檢查序列長度是否超過 512\n",
    "    if len(inputs[\"input_ids\"][0]) > 512:\n",
    "        print(\"The sentence is too long. Skipping...\")\n",
    "        continue\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings.append(outputs.last_hidden_state.mean(dim = 1).numpy().flatten())\n",
    "    sentences.append(sentence)  # 將被處理的句子加到列表中\n",
    "\n",
    "# 新的文本\n",
    "new_sentence = \"房間大\"\n",
    "new_tokens = ['房間', '大'] # 分詞\n",
    "inputs = tokenizer(new_tokens, is_split_into_words=True, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "new_embedding = outputs.last_hidden_state.mean(dim=1).numpy().flatten()\n",
    "\n",
    "# 計算與新文本的相似性\n",
    "similarities = cosine_similarity(embeddings + [new_embedding])\n",
    "\n",
    "# 獲取最相似的 50 個文本的索引\n",
    "top50_indices = np.argsort(similarities[-1][:-1])[:-51:-1]\n",
    "\n",
    "# 印出最相似的 50 個文本\n",
    "for i, index in enumerate(top50_indices):\n",
    "    print(f\"相似的文本 (索引 {index}): {sentences[index]}, 相似度: {similarities[-1][index]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33adf89",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "697e1cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "飯店名稱：陽光滿屋民宿\n",
      "平均相似度:0.11570316506156822\n",
      "前10條相似的評論:\n",
      "評論：舒適 房間，相似度：0.5655955612619636\n",
      "評論：乾淨 舒適 房間，相似度：0.4529094419157692\n",
      "評論：推薦 房間 乾淨 不錯 房間 內 無 冰箱 有 共用，相似度：0.3734171246078148\n",
      "評論：房間 乾淨 看 出 屋主 用心，相似度：0.22074116484931985\n",
      "評論：整體 棒 地點 不錯 大 片 窗戶 外面 有 連結感 地板 乾淨 熱水 燙 水壓 足 沒 暖氣 訂 個 房間 牆壁 上 有 幅 畫 房間 陽光感 風格 不符 感覺 看 久 頭 暈 正面 床 難 看到，相似度：0.1696819678971862\n",
      "評論：陽光 地方 房間 不錯 熱水 熱 晚上 安靜 有 小 螞蟻，相似度：0.15946363273550007\n",
      "評論：民宿 地點 活水湖 近 車程 五 分鐘 無 市區 鬧區 噪音 老闆 熱心 解答 任何 問題 房間 整潔 乾淨 希望 有 含 早餐 退房 時間 早點 房間 冷氣 涼 需要 設定到 低 溫度，相似度：0.15127160639049955\n",
      "評論：海岸 公園 近 民宿 房間 內 看見 海 民宿 地點 不錯 安靜 停車 方便 進門處 鞋子 沒 地方 擺 顯 亂 房間 垃圾桶 裏 遺留 上 個 房客 泡麵碗 洗手台 螞蟻 一 大 堆 腳 撞 狂 床框，相似度：0.14818309750777228\n",
      "評論：房間 乾淨 水壓 大 熱水 快 來 洗澡 舒服 市區 開車 分鐘 內 到達 路 邊 停車 方便 消毒水 味道 重，相似度：0.11527584378112371\n",
      "評論：早上 五點 多 有 公雞 叫 起床 六點 多 有 戰機 飛過 房間 廁所 乾淨，相似度：0.09638061657609528\n",
      "\n",
      "\n",
      "飯店名稱：家有囍宿\n",
      "平均相似度:0.09766333643934164\n",
      "前10條相似的評論:\n",
      "評論：房間 大 間 衛浴 設備 讚 有 按摩 浴缸 值 高 下 次 來 時 有 房間 住，相似度：0.3924007059710153\n",
      "評論：早餐 直接 送 餐券 方便 安靜 舒適 房間 大 枕頭 扁 床 軟 中間 陷下去 睡 右邊 一點 建議 翻面，相似度：0.09591597622569287\n",
      "評論：老闆 親切 民宿 整潔 生活 機能 佳 便利商店 各 種 食 小吃 餐廳 有 停車 方便 民宿 客廳 廚房 應有盡有 床 軟，相似度：0.0\n",
      "評論：按摩 浴缸 有 老闆 服務 態度 沒有，相似度：0.0\n",
      "評論：早餐 換 早餐店 餐券 環境 鬧中取靜 女性 友好 沒 化妝桌 備品，相似度：0.0\n",
      "\n",
      "\n",
      "飯店名稱：魔法精靈民宿\n",
      "平均相似度:0.09533750208640472\n",
      "前10條相似的評論:\n",
      "評論：老闆 親切 房間 乾淨 整潔 房間 乾淨 整潔 電視 訊號 不佳，相似度：0.2786818920562877\n",
      "評論：喜歡 貴婦 風格 入住 每 間 房間 個 有 特色 房 內 獨立 冰箱 一樓 有 共用 冰箱，相似度：0.10266811628933122\n",
      "評論：老闆娘 人 親切 推離 火車站 附近，相似度：0.0\n",
      "評論：火車站 近 客廳 空間 提供 很多 茶水 餅乾，相似度：0.0\n",
      "\n",
      "\n",
      "飯店名稱：朝日民宿 - Peng's Family\n",
      "平均相似度:0.08154258317783944\n",
      "前10條相似的評論:\n",
      "評論：房間 差，相似度：1.0\n",
      "評論：房間 新 乾淨，相似度：0.603128363134945\n",
      "評論：房間 有 螞蟻，相似度：0.36113605940999854\n",
      "評論：房間 乾淨 退房，相似度：0.3388064899506201\n",
      "評論：房間 乾淨 漂亮，相似度：0.330553406895637\n",
      "評論：價格 實惠 房間 乾淨，相似度：0.25329024901002034\n",
      "評論：位置 佳 早餐 不錯 房間 有 蟑螂 木 地板 有 鐵釘 露出 電視 有 雜音 房間 設備 老舊，相似度：0.22584727722100517\n",
      "評論：老闆娘 親切 服務 房間 裝潢 用心 價格 沒有，相似度：0.19145850105749293\n",
      "評論：舒適 寬敞 房間 棒 機車 停進去，相似度：0.17583815361392374\n",
      "評論：算 一 間 旅遊 選擇 入住 民宿 房間 寬敞 光線 明亮 海岸 公路 近 老闆娘 客氣 房間 沒有 小 茶几 四 人 房 有 一 把 椅子 浴室 洗手台 沒有 置物台，相似度：0.1663963089878792\n",
      "\n",
      "\n",
      "飯店名稱：海洋風情民宿\n",
      "平均相似度:0.0786606961632868\n",
      "前10條相似的評論:\n",
      "評論：房間 乾淨 舒適 老闆娘 人 親切，相似度：0.2889327186669041\n",
      "評論：居住 環境 安靜 房間 乾淨，相似度：0.22282414969116604\n",
      "評論：老闆娘 人 親切 房間 漂亮 舒適 下 次 有 機會 入住，相似度：0.1992473366109996\n",
      "評論：老闆娘 親切 小孩 友善 房間 天花板 晚上 有 星空 孩子 喜歡，相似度：0.13088128555018277\n",
      "評論：房間 屋頂 漂亮 入住 時 告知 人 多 水 熱 等 分鐘 等 一 個 多 小時 水 冷 冷 大 冬天 冷水 有 棉被 薄 涼被 厚，相似度：0.1073690257158103\n",
      "評論：第一 次 住有 星空 景觀 房型 感覺 開心 民宿 業者 熱心 介紹 附近 景點 提供 相關 資料 住宅區 寧靜 週圍 好 停車 房間 乾淨 舒適 好 民宿，相似度：0.0733345338876655\n",
      "評論：住宿 地點 佳 環境 清幽 風景區 市區 近 停車 算 方便 客房 大 夜燈 美麗 投影 天花板 星空 讓 人 忘卻 一切 煩惱 浴室 空間 大 多功能 蓮蓬頭 喜好 調整 有 浴缸 泡澡 洗去 一 天 疲憊 浴室 漱口杯 蓋 煙蒂 想要 使用 拿起來 有 滿滿 菸味 上 一 位 房客 藏 浴室 部分 請 打掃 時 注意 一下，相似度：0.0\n",
      "評論：入住 時 老闆娘 接待 親切 提供 多 實用 旅遊 建議 代訂 早餐 好吃 之後 去 花蓮 想 入住，相似度：0.0\n",
      "評論：整體 環境 清潔度 滿意 老闆娘 熱情 無，相似度：0.0\n",
      "評論：純真 童趣 溫馨 民宿，相似度：0.0\n",
      "\n",
      "\n",
      "飯店名稱：班卡拉渡假旅店\n",
      "平均相似度:0.07695721092689337\n",
      "前10條相似的評論:\n",
      "評論：床 大 好 睡 房間 大，相似度：1.0\n",
      "評論：房間 大 舒適 無，相似度：0.5655955612619636\n",
      "評論：寬敞 房間 好，相似度：0.4340281943246662\n",
      "評論：沒有 房間 內 浴室 沒有 天花板 房間 隔開 上廁所 味道 跑到 房間 內，相似度：0.3449057772268462\n",
      "評論：老闆 主動 升等 房間 房間 乾淨 舒適 乾淨 舒適 便宜 無，相似度：0.3074710721695949\n",
      "評論：裝修 更新 一下 好 房間 大 停車 方便 廁所 房間 沒有 隔間 夫妻 入住 感覺 方便 味道 留在 房間 內，相似度：0.29420243523027806\n",
      "評論：房間 寬敞 舒適 床 軟 好 睡 早餐 好吃，相似度：0.292365179018659\n",
      "評論：停車 方便 房間 內 有 蚊子，相似度：0.27175433504817104\n",
      "評論：房間 格局 不佳，相似度：0.25024482283159055\n",
      "評論：房間 整潔 環境 清幽 安靜，相似度：0.22636900209342142\n",
      "\n",
      "\n",
      "飯店名稱：小巷民宿\n",
      "平均相似度:0.07618021017611262\n",
      "前10條相似的評論:\n",
      "評論：房間 舒適 乾淨 停車 方便 床 軟 晚上 房間 冷，相似度：0.4869619749612681\n",
      "評論：民宿 主人 友善 房間 舒適 乾淨 早餐 好吃，相似度：0.20305808167314005\n",
      "評論：家 一樣 舒適 進到 民宿 回到 家 裡面 一樣 民宿 夫婦 位 孩子 親切 問候 房間 大 床 枕頭 舒服 重點 房 內 有 個 大 桌子 房 內 用餐 方便 可惜 疫情 期間 吃 到手 作 早餐 安全 第一，相似度：0.07178204512671808\n",
      "評論：推薦給 喜歡 整潔 簡單 平價 清靜 住宿 條件 者 整潔，相似度：0.0\n",
      "評論：民宿 主人 用心 準備 早餐 熱情 分享 介紹 景點 寄宿 朋友 家 一樣 棒 住宿 經驗，相似度：0.0\n",
      "評論：溫馨 早餐 老闆娘 老闆 動手 做 好吃，相似度：0.0\n",
      "評論：乾淨 陽台 熱情 老闆，相似度：0.0\n",
      "評論：安靜 清潔 老闆 用心 經營 安排 太平山 武陵 農場 中繼 過夜 好 地點，相似度：0.0\n",
      "評論：住宿 環境 舒適 早餐 配合 天氣 提供 熱飲 熱食 好，相似度：0.0\n",
      "評論：民宿 男女 主人 服務 親切 備 小 點心 供 房客 解饞，相似度：0.0\n",
      "\n",
      "\n",
      "飯店名稱：羅東公園61號(峇里風)特色包棟民宿\n",
      "平均相似度:0.07312529675005397\n",
      "前10條相似的評論:\n",
      "評論：兩 間 房間 包 棟 不錯 早餐 用心 好吃 老闆娘 人 親切 二樓 雙人房 毛玻璃 設計 住在 同 一 層 樓 無 隱私 房間 內 沒有 衛浴 不便 民宿 乾淨 二樓 馬桶 內側 邊邊 霉垢 刷 乾淨，相似度：0.14625059350010794\n",
      "評論：超棒 好吃，相似度：0.0\n",
      "\n",
      "\n",
      "飯店名稱：陽光聖亞旅店\n",
      "平均相似度:0.07209086411636584\n",
      "前10條相似的評論:\n",
      "評論：房間 海景 棒，相似度：0.38642716625265017\n",
      "評論：房間 乾淨 寬敞，相似度：0.37642315786029673\n",
      "評論：房間 漂亮 風景 美，相似度：0.27259975472114806\n",
      "評論：喜歡 待在 房間 看到 海 人 這兒 適合 房間 乾淨 床 被子 舒適，相似度：0.25865871848912453\n",
      "評論：老闆 服務 親切 人 房間 明亮 乾淨，相似度：0.24982073940400212\n",
      "評論：地點 佳 服務 好 風景 漂亮 房間 乾淨 舒適 地點 佳 服務 好 風景 漂亮 房間 乾淨 舒適，相似度：0.21682307644909057\n",
      "評論：位於 對面 房間 欣賞 美景 附近 有 便利商店 便利 員工 服務 棒 熱情 有禮 價格 不錯 值 高 房間 乾淨，相似度：0.21589650312418757\n",
      "評論：海景 覺得 房間 放 礦泉水，相似度：0.21485729573158635\n",
      "評論：房間 風景 佳 乾淨 整潔 老闆 用心 熱情，相似度：0.19828412589351113\n",
      "評論：房間 照片 無異，相似度：0.195551135265147\n",
      "\n",
      "\n",
      "飯店名稱：風華渡假旅館\n",
      "平均相似度:0.06731216603744469\n",
      "前10條相似的評論:\n",
      "評論：房間 大，相似度：1.0\n",
      "評論：房間 大，相似度：1.0\n",
      "評論：房間 吵 差，相似度：1.0\n",
      "評論：不錯 房間 大 地點，相似度：0.4289619225684878\n",
      "評論：房間 佈置 整體 上 不錯 房間 內 蚊子 多 晚上 咬，相似度：0.3653512350265948\n",
      "評論：房間 菸味 重，相似度：0.3040495689640538\n",
      "評論：房間 大 氣氛 佳 舒適，相似度：0.2768113029495801\n",
      "評論：房間 乾淨 整齊 浴室，相似度：0.27473817713622095\n",
      "評論：浴室 大 泡澡 舒服 房間 美，相似度：0.27257806811496355\n",
      "評論：舒適度 房間 有 煙味，相似度：0.2551117363687556\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "\n",
    "df_tf_idf = pd.read_csv('0_10000.csv', header=0)\n",
    "\n",
    "sentences_tokenized = [eval(d) for d in df_tf_idf[\"綜合評論_ws\"].values]\n",
    "pos_tags = [eval(d) for d in df_tf_idf[\"綜合評論_pos\"].values]\n",
    "\n",
    "# 嘗試只保留有意義的詞性\n",
    "stop_pos = set(['Nep', 'Nh', 'Nb'])  # 這 3 種詞性不保留\n",
    "\n",
    "short_with_pos = []  # 放過濾後的詞性與句子\n",
    "short_sentence = []  # 放過濾後的句子\n",
    "\n",
    "for sentence, sentence_pos_tags in zip(sentences_tokenized, pos_tags):\n",
    "    short_with_pos_sentence = []  # 儲存這個句子過濾後的詞（帶詞性）\n",
    "    short_sentence_sentence = []  # 儲存這個句子過濾後的詞（不帶詞性）\n",
    "    \n",
    "    for word_ws, word_pos in zip(sentence, sentence_pos_tags):\n",
    "        \n",
    "        # 只留名詞和動詞\n",
    "        is_N_or_V = word_pos.startswith(\"V\") or word_pos.startswith(\"N\")\n",
    "\n",
    "        # 去掉名詞裡的某些詞性\n",
    "        is_not_stop_pos = word_pos not in stop_pos\n",
    "\n",
    "        if is_N_or_V and is_not_stop_pos:\n",
    "            short_with_pos_sentence.append(f\"{word_ws}({word_pos})\")\n",
    "            short_sentence_sentence.append(f\"{word_ws}\")\n",
    "\n",
    "    short_with_pos.append(short_with_pos_sentence)\n",
    "    short_sentence.append(short_sentence_sentence)\n",
    "\n",
    "# 將 '綜合評論_ws' 欄位更新為 'short_sentence' 列表\n",
    "df_tf_idf['綜合評論_ws'] = [' '.join(s) for s in short_sentence]\n",
    "\n",
    "# 將綜合評論過濾後為空的資料去除\n",
    "non_empty_mask = df_tf_idf['綜合評論_ws'].str.strip() != ''\n",
    "\n",
    "# 使用此遮罩來過濾 DataFrame\n",
    "df_tf_idf = df_tf_idf[non_empty_mask]\n",
    "\n",
    "# 使用 TF-IDF 將所有評論文本轉換為數值向量\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(df_tf_idf['綜合評論_ws'])\n",
    "\n",
    "# 假設新用戶輸入的需求\n",
    "new_user_input = \"房間 大\"\n",
    "\n",
    "# 將新用戶輸入轉換為相同的數值向量\n",
    "new_vector = vectorizer.transform([new_user_input])\n",
    "\n",
    "# 獲取所有的飯店名稱\n",
    "hotel_names = df['飯店名稱'].unique()\n",
    "\n",
    "# 初始化一個空的字典來儲存每個飯店的所有評論相似度和評論文本\n",
    "hotel_reviews_similarities = {}\n",
    "\n",
    "# 對每個飯店進行處理\n",
    "for hotel_name in hotel_names:\n",
    "    # 只選取該飯店的評論\n",
    "    hotel_reviews = df_tf_idf[df_tf_idf['飯店名稱'] == hotel_name]['綜合評論_ws']\n",
    "    \n",
    "    # 使用擬合好的TF-IDF將飯店的評論轉換為數值向量\n",
    "    X = vectorizer.transform(hotel_reviews)\n",
    "\n",
    "    # 計算新用戶輸入與所有評論的 cosine similarity\n",
    "    cos_similarities = cosine_similarity(new_vector, X).flatten()\n",
    "    \n",
    "    # 將這個飯店的所有評論相似度和評論文本儲存到字典中\n",
    "    hotel_reviews_similarities[hotel_name] = list(zip(hotel_reviews, cos_similarities))\n",
    "\n",
    "# 初始化一個空的字典來儲存每個飯店與新用戶需求的平均相似度\n",
    "hotel_similarities = {hotel: np.mean([sim for _, sim in reviews]) for hotel, reviews in hotel_reviews_similarities.items()}\n",
    "\n",
    "# 對飯店的平均相似度進行排序，並只取前10個\n",
    "top10_hotels = sorted(hotel_similarities.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "# 輸出與新用戶需求最相似的前 10 個飯店\n",
    "for hotel_name, avg_sim in top10_hotels:\n",
    "    print(f\"飯店名稱：{hotel_name}\")\n",
    "    print(f\"平均相似度:{avg_sim}\")\n",
    "    \n",
    "    # 對這家飯店的所有評論相似度進行排序，並只取前 10 個\n",
    "    top10_reviews = sorted(hotel_reviews_similarities[hotel_name], key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "    # 輸出與新用戶需求最相似的前10條評論\n",
    "    print(\"前10條相似的評論:\")\n",
    "    for review_text, sim in top10_reviews:\n",
    "        print(f\"評論：{review_text}，相似度：{sim}\")\n",
    "    \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9130ce6c",
   "metadata": {},
   "source": [
    "#### 將同一間飯店評論合併後，使用 roberta 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d145de73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-roberta-wwm-ext were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (5910) must match the size of tensor b (512) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 67\u001b[0m\n\u001b[0;32m     65\u001b[0m         inputs \u001b[38;5;241m=\u001b[39m tokenizer(sentence, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     66\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 67\u001b[0m             outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m     68\u001b[0m         hotel_embeddings[i]\u001b[38;5;241m.\u001b[39mappend(outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mflatten())\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# Compute the average embedding for each hotel\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\web_scraping\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\web_scraping\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1013\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1006\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m   1007\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m   1008\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m   1009\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m   1010\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m   1011\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m-> 1013\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1019\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1020\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[0;32m   1021\u001b[0m     embedding_output,\n\u001b[0;32m   1022\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1030\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1031\u001b[0m )\n\u001b[0;32m   1032\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\web_scraping\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\web_scraping\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:236\u001b[0m, in \u001b[0;36mBertEmbeddings.forward\u001b[1;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabsolute\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    235\u001b[0m     position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embeddings(position_ids)\n\u001b[1;32m--> 236\u001b[0m     embeddings \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m position_embeddings\n\u001b[0;32m    237\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(embeddings)\n\u001b[0;32m    238\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(embeddings)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (5910) must match the size of tensor b (512) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from collections import defaultdict\n",
    "\n",
    "def sentence_based_sliding_window(text, window_size):\n",
    "    sentences = re.split('(?<=[。])', text)\n",
    "    current_group = []\n",
    "    current_length = 0\n",
    "    grouped_sentences = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_length = len(tokenizer.tokenize(sentence))\n",
    "        if sentence_length > window_size:\n",
    "            # If the length of a single sentence exceeds the window size, skip that data\n",
    "            continue\n",
    "        elif current_length + sentence_length > window_size:\n",
    "            grouped_sentences.append(\"\".join(current_group))\n",
    "            current_group = [sentence]\n",
    "            current_length = sentence_length\n",
    "        else:\n",
    "            current_group.append(sentence)\n",
    "            current_length += sentence_length\n",
    "\n",
    "    # Add the remaining sentences to grouped_sentences\n",
    "    if current_group:\n",
    "        grouped_sentences.append(\"\".join(current_group))\n",
    "\n",
    "    return grouped_sentences\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('hfl/chinese-roberta-wwm-ext')\n",
    "model = BertModel.from_pretrained('hfl/chinese-roberta-wwm-ext')\n",
    "\n",
    "\n",
    "df = pd.read_csv('./booking_comments_分詞update.csv', header=0)\n",
    "\n",
    "# Merge the comprehensive comments of the same hotel using a period\n",
    "df_grouped = df[0:10000].groupby('飯店名稱')['綜合評論'].apply(lambda x: '。'.join(x)).reset_index()\n",
    "\n",
    "# Text\n",
    "sentences = list(df_grouped.loc[:, '綜合評論'])\n",
    "\n",
    "# Get the embeddings for each text\n",
    "embeddings = []\n",
    "hotel_indices = []\n",
    "hotel_embeddings = defaultdict(list)\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    # If the sentence is too long, split it into multiple fragments\n",
    "    if len(tokenizer.tokenize(sentence)) > tokenizer.model_max_length:\n",
    "        sentence_windows = sentence_based_sliding_window(sentence, 4000)\n",
    "        for sentence_window in sentence_windows:\n",
    "            if len(tokenizer.tokenize(sentence_window)) > tokenizer.model_max_length:\n",
    "                continue\n",
    "            inputs = tokenizer(sentence_window, return_tensors=\"pt\", truncation=False, padding=True)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "            hotel_embeddings[i].append(outputs.last_hidden_state.mean(dim=1).numpy().flatten())\n",
    "    else:\n",
    "        inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=False, padding=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        hotel_embeddings[i].append(outputs.last_hidden_state.mean(dim=1).numpy().flatten())\n",
    "\n",
    "# Compute the average embedding for each hotel\n",
    "for i, hotel_embedding in hotel_embeddings.items():\n",
    "    embeddings.append(np.mean(hotel_embedding, axis=0))\n",
    "    hotel_indices.append(i)\n",
    "\n",
    "# 新的文本\n",
    "new_sentence = \"房間大\"\n",
    "inputs = tokenizer(new_sentence, return_tensors=\"pt\", truncation=True, max_length=512, padding=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "new_embedding = outputs.last_hidden_state.mean(dim=1).numpy().flatten()\n",
    "\n",
    "\n",
    "# Calculate similarity with the new text\n",
    "similarities = cosine_similarity(embeddings + [new_embedding])\n",
    "\n",
    "# Get the indices of the top 10 most similar texts\n",
    "top10_indices = np.argsort(similarities[-1][:-1])[:-10:-1]\n",
    "\n",
    "# Print the top 10 most similar texts\n",
    "for index in top10_indices:\n",
    "    print(f\"Similar hotel: {df_grouped.loc[hotel_indices[index], '飯店名稱']}, similarity: {similarities[-1][index]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac70b1f3",
   "metadata": {},
   "source": [
    "#### 將同一間飯店評論合併後，使用可處理長句子的 bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b866d944",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_based_sliding_window(text, window_size):\n",
    "    sentences = re.split('(?<=[。])', text)\n",
    "    current_group = []\n",
    "    current_length = 0\n",
    "    grouped_sentences = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_length = len(tokenizer.tokenize(sentence))\n",
    "        if sentence_length > window_size:\n",
    "            # 如果單一句子的長度超過窗口大小，跳過該筆數據\n",
    "            continue\n",
    "        elif current_length + sentence_length > window_size:\n",
    "            grouped_sentences.append(\"\".join(current_group))\n",
    "            current_group = [sentence]\n",
    "            current_length = sentence_length\n",
    "        else:\n",
    "            current_group.append(sentence)\n",
    "            current_length += sentence_length\n",
    "\n",
    "    # 把剩下的句子加入 grouped_sentences\n",
    "    if current_group:\n",
    "        grouped_sentences.append(\"\".join(current_group))\n",
    "    \n",
    "    return grouped_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c19da6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "相似的飯店: 大寶的民宿 Tabohouse, 相似度: 0.9586089849472046\n",
      "相似的飯店: 翠園歐風庭園民宿, 相似度: 0.9230974912643433\n",
      "相似的飯店: 花爵墾丁, 相似度: 0.9208060503005981\n",
      "相似的飯店: 白鷺灣 民宿(安平古堡), 相似度: 0.9181367754936218\n",
      "相似的飯店: 希望恆春休閒會館, 相似度: 0.9181272983551025\n",
      "相似的飯店: 卡爾登飯店 the Carlton, 相似度: 0.9166074991226196\n",
      "相似的飯店: 陽光滿屋民宿, 相似度: 0.9149898886680603\n",
      "相似的飯店: 家有囍宿, 相似度: 0.9144690036773682\n",
      "相似的飯店: 海洋風情民宿, 相似度: 0.9132213592529297\n"
     ]
    }
   ],
   "source": [
    "from transformers import LongformerModel, LongformerTokenizer\n",
    "from collections import defaultdict\n",
    "\n",
    "tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
    "model = LongformerModel.from_pretrained('allenai/longformer-base-4096')\n",
    "\n",
    "df = pd.read_csv('./booking_comments_分詞update.csv', header=0)\n",
    "\n",
    "# 將相同飯店的綜合評論用句號合併\n",
    "df_grouped = df[0:10000].groupby('飯店名稱')['綜合評論'].apply(lambda x: '。'.join(x)).reset_index()\n",
    "\n",
    "# 文本\n",
    "sentences = list(df_grouped.loc[:, '綜合評論'])\n",
    "\n",
    "# 獲取每個文本的嵌入向量\n",
    "embeddings = []\n",
    "hotel_indices = []\n",
    "hotel_embeddings = defaultdict(list)\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    # 如果句子太長，就將其分割成多個片段\n",
    "    if len(tokenizer.tokenize(sentence)) > tokenizer.model_max_length:\n",
    "        sentence_windows = sentence_based_sliding_window(sentence, 4000)\n",
    "        for sentence_window in sentence_windows:\n",
    "            if len(tokenizer.tokenize(sentence_window)) > tokenizer.model_max_length:\n",
    "                continue\n",
    "            inputs = tokenizer(sentence_window, return_tensors=\"pt\", truncation=False, padding=True)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "            hotel_embeddings[i].append(outputs.last_hidden_state.mean(dim=1).numpy().flatten())\n",
    "    else:\n",
    "        inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=False, padding=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        hotel_embeddings[i].append(outputs.last_hidden_state.mean(dim=1).numpy().flatten())\n",
    "\n",
    "# 計算每一間飯店的平均向量\n",
    "for i, hotel_embedding in hotel_embeddings.items():\n",
    "    embeddings.append(np.mean(hotel_embedding, axis=0))\n",
    "    hotel_indices.append(i)\n",
    "\n",
    "# 新的文本\n",
    "new_sentence = \"房間大\"\n",
    "inputs = tokenizer(new_sentence, return_tensors=\"pt\", truncation=False, padding=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "new_embedding = outputs.last_hidden_state.mean(dim=1).numpy().flatten()\n",
    "\n",
    "# 計算與新文本的相似性\n",
    "similarities = cosine_similarity(embeddings + [new_embedding])\n",
    "\n",
    "# 獲取最相似的 10 個文本的索引\n",
    "top10_indices = np.argsort(similarities[-1][:-1])[:-10:-1]\n",
    "\n",
    "# 印出最相似的 10 個文本\n",
    "for index in top10_indices:\n",
    "    print(f\"相似的飯店: {df_grouped.loc[hotel_indices[index], '飯店名稱']}, 相似度: {similarities[-1][index]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cf20c8",
   "metadata": {},
   "source": [
    "#### 將同一間飯店評論合併後，使用可處理長句子的 bert，並排除評論數不足30的飯店"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707cd6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LongformerModel, LongformerTokenizer\n",
    "\n",
    "tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
    "model = LongformerModel.from_pretrained('allenai/longformer-base-4096')\n",
    "\n",
    "df = pd.read_csv('./booking_comments_分詞update.csv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaab081",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# 篩選評論數大於等於 30 的飯店\n",
    "df_filtered = df[0:10000].groupby('飯店名稱').filter(lambda x: len(x) >= 30);df_filtered\n",
    "\n",
    "# 將相同飯店的綜合評論用句號合併，並保留其他重要欄位\n",
    "df_grouped = df_filtered.groupby('飯店名稱').agg({\n",
    "    '綜合評論': lambda x: '。'.join(x),\n",
    "    '縣市': lambda x: x.iloc[0],\n",
    "    '鄉鎮': lambda x: x.iloc[0],\n",
    "    '整體評分': lambda x: x.iloc[0],\n",
    "    '單項評分_整潔度': lambda x: x.iloc[0],\n",
    "    '單項評分_舒適程度': lambda x: x.iloc[0],\n",
    "    '單項評分_住宿地點': lambda x: x.iloc[0],\n",
    "    '單項評分_設施': lambda x: x.iloc[0],\n",
    "    '單項評分_員工素質': lambda x: x.iloc[0],\n",
    "    '單項評分_性價比': lambda x: x.iloc[0]\n",
    "}).reset_index();df_grouped\n",
    "\n",
    "# 文本\n",
    "sentences = list(df_grouped.loc[:, '綜合評論'])\n",
    "\n",
    "# 獲取每個文本的嵌入向量\n",
    "embeddings = []\n",
    "hotel_indices = []\n",
    "hotel_embeddings = defaultdict(list)\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    \n",
    "    # 如果句子太長，就分割成多個片段\n",
    "    if len(tokenizer.tokenize(sentence)) > tokenizer.model_max_length:\n",
    "        sentence_windows = sentence_based_sliding_window(sentence, 4000)\n",
    "        for sentence_window in sentence_windows:\n",
    "            if len(tokenizer.tokenize(sentence_window)) > tokenizer.model_max_length:\n",
    "                continue\n",
    "            inputs = tokenizer(sentence_window, return_tensors=\"pt\", truncation=False, padding=True)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "            hotel_embeddings[i].append(outputs.last_hidden_state.mean(dim=1).numpy().flatten())\n",
    "    else:\n",
    "        inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=False, padding=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        hotel_embeddings[i].append(outputs.last_hidden_state.mean(dim=1).numpy().flatten())\n",
    "\n",
    "# 計算每間飯店的平均向量\n",
    "for i, hotel_embedding in hotel_embeddings.items():\n",
    "    embeddings.append(np.mean(hotel_embedding, axis=0))\n",
    "    hotel_indices.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "203b096b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 儲存嵌入向量\n",
    "np.save('embeddings_longformer_base_4096.npy', embeddings)\n",
    "\n",
    "# 儲存飯店資訊\n",
    "df_grouped.loc[hotel_indices, :].to_csv('hotel_info_longformerbase4096.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8251730b",
   "metadata": {},
   "source": [
    "#### 使用迴圈跑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2abfae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_based_sliding_window(text, window_size):\n",
    "    sentences = re.split('(?<=[。])', text)\n",
    "    current_group = []\n",
    "    current_length = 0\n",
    "    grouped_sentences = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_length = len(tokenizer.tokenize(sentence))\n",
    "        if sentence_length > window_size:\n",
    "            # If the length of a single sentence exceeds the window size, skip that data\n",
    "            continue\n",
    "        elif current_length + sentence_length > window_size:\n",
    "            grouped_sentences.append(\"\".join(current_group))\n",
    "            current_group = [sentence]\n",
    "            current_length = sentence_length\n",
    "        else:\n",
    "            current_group.append(sentence)\n",
    "            current_length += sentence_length\n",
    "\n",
    "    # Add the remaining sentences to grouped_sentences\n",
    "    if current_group:\n",
    "        grouped_sentences.append(\"\".join(current_group))\n",
    "\n",
    "    return grouped_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "07709eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing hotels 0 to 50\n",
      "Processing hotels 50 to 100\n",
      "Processing hotels 100 to 150\n",
      "Processing hotels 150 to 200\n",
      "Processing hotels 200 to 250\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 60\u001b[0m\n\u001b[0;32m     58\u001b[0m         inputs \u001b[38;5;241m=\u001b[39m tokenizer(sentence_window, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     59\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 60\u001b[0m             outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m     61\u001b[0m         hotel_embeddings[hotel_name]\u001b[38;5;241m.\u001b[39mappend(outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mflatten())\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\web_scraping\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\web_scraping\\lib\\site-packages\\transformers\\models\\longformer\\modeling_longformer.py:1747\u001b[0m, in \u001b[0;36mLongformerModel.forward\u001b[1;34m(self, input_ids, attention_mask, global_attention_mask, head_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1739\u001b[0m extended_attention_mask: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_extended_attention_mask(attention_mask, input_shape)[\n\u001b[0;32m   1740\u001b[0m     :, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, :\n\u001b[0;32m   1741\u001b[0m ]\n\u001b[0;32m   1743\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m   1744\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids, position_ids\u001b[38;5;241m=\u001b[39mposition_ids, token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids, inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds\n\u001b[0;32m   1745\u001b[0m )\n\u001b[1;32m-> 1747\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1748\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1751\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1752\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1753\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1755\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1756\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1757\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\web_scraping\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\web_scraping\\lib\\site-packages\\transformers\\models\\longformer\\modeling_longformer.py:1323\u001b[0m, in \u001b[0;36mLongformerEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, padding_len, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[0;32m   1315\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m   1316\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1320\u001b[0m         is_index_global_attn,\n\u001b[0;32m   1321\u001b[0m     )\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1323\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1326\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_index_masked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_index_masked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_index_global_attn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_index_global_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_global_attn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_global_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1330\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1331\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1332\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1334\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[0;32m   1335\u001b[0m     \u001b[38;5;66;03m# bzs x seq_len x num_attn_heads x (num_global_attn + attention_window_len + 1) => bzs x num_attn_heads x seq_len x (num_global_attn + attention_window_len + 1)\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\web_scraping\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\web_scraping\\lib\\site-packages\\transformers\\models\\longformer\\modeling_longformer.py:1258\u001b[0m, in \u001b[0;36mLongformerLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, layer_head_mask, is_index_masked, is_index_global_attn, is_global_attn, output_attentions)\u001b[0m\n\u001b[0;32m   1255\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m self_attn_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1256\u001b[0m outputs \u001b[38;5;241m=\u001b[39m self_attn_outputs[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m-> 1258\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1259\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mff_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_output\u001b[49m\n\u001b[0;32m   1260\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1261\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[0;32m   1262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\web_scraping\\lib\\site-packages\\transformers\\pytorch_utils.py:237\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\web_scraping\\lib\\site-packages\\transformers\\models\\longformer\\modeling_longformer.py:1265\u001b[0m, in \u001b[0;36mLongformerLayer.ff_chunk\u001b[1;34m(self, attn_output)\u001b[0m\n\u001b[0;32m   1264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mff_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attn_output):\n\u001b[1;32m-> 1265\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1266\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attn_output)\n\u001b[0;32m   1267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\web_scraping\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\web_scraping\\lib\\site-packages\\transformers\\models\\longformer\\modeling_longformer.py:1208\u001b[0m, in \u001b[0;36mLongformerIntermediate.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m   1206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m   1207\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(hidden_states)\n\u001b[1;32m-> 1208\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate_act_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1209\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\web_scraping\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\web_scraping\\lib\\site-packages\\transformers\\activations.py:78\u001b[0m, in \u001b[0;36mGELUActivation.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from transformers import LongformerModel, LongformerTokenizer\n",
    "import re\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 如果有 GPU 就使用，否則用 CPU \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
    "model = LongformerModel.from_pretrained('allenai/longformer-base-4096')\n",
    "model = model.to(device)\n",
    "\n",
    "df = pd.read_csv('./booking_comments_分詞update.csv', header=0)\n",
    "\n",
    "# 讀取之前的檔案\n",
    "try:\n",
    "    embeddings = np.load('embeddings_longformer_base_4096.npy').tolist()\n",
    "    df_grouped = pd.read_csv('hotel_info_longformerbase4096.csv')\n",
    "except FileNotFoundError:\n",
    "    embeddings = {}\n",
    "    df_grouped = pd.DataFrame()\n",
    "\n",
    "hotel_names = df['飯店名稱'].unique()\n",
    "n_hotels = len(hotel_names)\n",
    "\n",
    "# 分批執行(每50間飯店儲存一次)\n",
    "for i in range(0, n_hotels, 50):\n",
    "    print(f'Processing hotels {i} to {min(i + 50, n_hotels)}')\n",
    "    batch_hotel_names = hotel_names[i:i + 50]\n",
    "    df_filtered = df[df['飯店名稱'].isin(batch_hotel_names)]\n",
    "    df_grouped_batch = df_filtered.groupby('飯店名稱').agg({\n",
    "        '正評': lambda x: '。'.join(str(v) for v in x if not pd.isna(v)),\n",
    "        '縣市': lambda x: x.iloc[0],\n",
    "        '鄉鎮': lambda x: x.iloc[0],\n",
    "        '整體評分': lambda x: x.iloc[0],\n",
    "        '單項評分_整潔度': lambda x: x.iloc[0],\n",
    "        '單項評分_舒適程度': lambda x: x.iloc[0],\n",
    "        '單項評分_住宿地點': lambda x: x.iloc[0],\n",
    "        '單項評分_設施': lambda x: x.iloc[0],\n",
    "        '單項評分_員工素質': lambda x: x.iloc[0],\n",
    "        '單項評分_性價比': lambda x: x.iloc[0]\n",
    "    }).reset_index()\n",
    "    \n",
    "    sentences = list(df_grouped_batch.loc[:, '正評'])\n",
    "    hotel_embeddings = defaultdict(list)\n",
    "\n",
    "    for hotel_name, sentence in zip(batch_hotel_names, sentences):\n",
    "        \n",
    "        # 如果文本過長就拆分\n",
    "        if len(tokenizer.tokenize(sentence)) > tokenizer.model_max_length:\n",
    "            sentence_windows = sentence_based_sliding_window(sentence, 4000)\n",
    "            for sentence_window in sentence_windows:\n",
    "                if len(tokenizer.tokenize(sentence_window)) > tokenizer.model_max_length:\n",
    "                    continue\n",
    "                inputs = tokenizer(sentence_window, return_tensors=\"pt\", truncation=False, padding=True).to(device)\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(**inputs)\n",
    "                hotel_embeddings[hotel_name].append(outputs.last_hidden_state.mean(dim=1).cpu().numpy().flatten())\n",
    "        else:\n",
    "            inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=False, padding=True).to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "            hotel_embeddings[hotel_name].append(outputs.last_hidden_state.mean(dim=1).cpu().numpy().flatten())\n",
    "            \n",
    "    # Compute average embeddings\n",
    "    for hotel_name, embeds in hotel_embeddings.items():\n",
    "        if len(embeds) > 0:\n",
    "            embeddings[hotel_name] = np.mean(embeds, axis=0)\n",
    "\n",
    "    # 合併文件内容\n",
    "    df_grouped = pd.concat([df_grouped, df_grouped_batch])\n",
    "    \n",
    "    # 儲存\n",
    "    np.savez('embeddings_longformer_base_4096.npy', **embeddings)\n",
    "    df_grouped.to_csv('hotel_info_longformerbase4096.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691c3acf",
   "metadata": {},
   "source": [
    "#### 進行二次篩選(根據某些特定條件)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d2bba03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_hotels(condition, embeddings, new_embedding, hotel_data, n):\n",
    "    \n",
    "    # 根據條件篩選出飯店\n",
    "    filtered_hotels = hotel_data[condition]\n",
    "\n",
    "    # 取得符合條件的飯店嵌入向量\n",
    "    filtered_embeddings = embeddings[filtered_hotels.index]\n",
    "\n",
    "    # 計算與新文本的相似性\n",
    "    similarities = cosine_similarity(np.concatenate([filtered_embeddings, new_embedding[None, :]]))\n",
    "\n",
    "    # 獲取最相似的 n 個文本的索引\n",
    "    topn_indices = np.argsort(similarities[-1][:-1])[:-n-1:-1]\n",
    "\n",
    "    # 印出最相似的 n 個飯店\n",
    "    for index in topn_indices:\n",
    "        print(f\"相似的飯店:\\n{filtered_hotels.iloc[index][['飯店名稱']]}\")\n",
    "        print(f\"縣市:{filtered_hotels.iloc[index][['縣市']]}\")\n",
    "        print(f\"鄉鎮:{filtered_hotels.iloc[index][['鄉鎮']]}\")\n",
    "        print(f\"整體評分:{filtered_hotels.iloc[index][['整體評分']]}\")\n",
    "        print(f\"相似度: {similarities[-1][index]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c6396d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Index([7], dtype='int64') is not a file in the archive\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 28\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(embeddings))\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# # 新的文本\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# new_sentence = \"房間大\"\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# inputs = tokenizer(new_sentence, return_tensors=\"pt\", truncation=True, max_length=512, padding=True)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m \n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# 搜尋'南投縣'的前 5 個相似的飯店\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m \u001b[43mget_similar_hotels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_hotel_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m縣市\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m南投縣\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_embedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_hotel_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[14], line 7\u001b[0m, in \u001b[0;36mget_similar_hotels\u001b[1;34m(condition, embeddings, new_embedding, hotel_data, n)\u001b[0m\n\u001b[0;32m      4\u001b[0m filtered_hotels \u001b[38;5;241m=\u001b[39m hotel_data[condition]\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# 取得符合條件的飯店嵌入向量\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m filtered_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43membeddings\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfiltered_hotels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# 計算與新文本的相似性\u001b[39;00m\n\u001b[0;32m     10\u001b[0m similarities \u001b[38;5;241m=\u001b[39m cosine_similarity(np\u001b[38;5;241m.\u001b[39mconcatenate([filtered_embeddings, new_embedding[\u001b[38;5;28;01mNone\u001b[39;00m, :]]))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\web_scraping\\lib\\site-packages\\numpy\\lib\\npyio.py:260\u001b[0m, in \u001b[0;36mNpzFile.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzip\u001b[38;5;241m.\u001b[39mread(key)\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 260\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not a file in the archive\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m key)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Index([7], dtype='int64') is not a file in the archive\""
     ]
    }
   ],
   "source": [
    "# 讀取 DataFrame\n",
    "df_hotel_info = pd.read_csv('hotel_info_longformerbase4096.csv')\n",
    "print(len(df_hotel_info))\n",
    "# 讀取 embeddings\n",
    "embeddings = np.load('embeddings_longformer_base_4096.npy.npz')\n",
    "print(len(embeddings))\n",
    "\n",
    "# # 新的文本\n",
    "# new_sentence = \"房間大\"\n",
    "# inputs = tokenizer(new_sentence, return_tensors=\"pt\", truncation=True, max_length=512, padding=True)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     outputs = model(**inputs)\n",
    "# new_embedding = outputs.last_hidden_state.mean(dim=1).numpy().flatten()\n",
    "\n",
    "# 搜尋'南投縣'的前 5 個相似的飯店\n",
    "get_similar_hotels(df_hotel_info['縣市'] == '南投縣', embeddings, new_embedding, df_hotel_info, 5)\n",
    "\n",
    "# # 搜尋整體評分大於 8 的前 10 個相似的飯店\n",
    "# get_similar_hotels(hotel_data['整體評分'] > 8, embeddings, new_embedding, hotel_data, 10)\n",
    "\n",
    "# # 搜尋'南投縣'且整體評分大於 8 的前 15 個相似的飯店\n",
    "# get_similar_hotels((hotel_data['縣市'] == '南投縣') & (hotel_data['整體評分'] > 8), embeddings, new_embedding, hotel_data, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c291be26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "飯店名稱: 太魯閣阿騫的家民宿, 整體評分: 9.6\n",
      "飯店名稱: 鹿台民宿, 整體評分: 9.1\n",
      "飯店名稱: 花蓮綠舍 The Green Villa l 花蓮親子溜滑梯民宿, 整體評分: 8.8\n",
      "飯店名稱: 親水棧民宿, 整體評分: 8.6\n",
      "飯店名稱: 磐石旅店, 整體評分: 8.4\n",
      "飯店名稱: 雁窩民宿, 整體評分: 8.3\n",
      "飯店名稱: 朝日民宿 - Peng's Family, 整體評分: 8.0\n",
      "飯店名稱: 卡爾登飯店 the Carlton, 整體評分: 7.9\n",
      "飯店名稱: 樂活休閒海景飯店, 整體評分: 7.9\n"
     ]
    }
   ],
   "source": [
    "# top10_hotel_scores = []\n",
    "# for name in top10_hotel_name:\n",
    "#     score = df[df['飯店名稱'] == name]['整體評分'].values[0]\n",
    "#     top10_hotel_scores.append((name, score))\n",
    "\n",
    "# # 根據評分進行排序，評分高的在前\n",
    "# top10_hotel_scores_sorted = sorted(top10_hotel_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# # 印出排序後的飯店和評分\n",
    "# for name, score in top10_hotel_scores_sorted:\n",
    "#     print(f\"飯店名稱: {name}, 整體評分: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fbb50b",
   "metadata": {},
   "source": [
    "#### GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1fdeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import time\n",
    "\n",
    "openai.api_key = 'your-api-key'\n",
    "\n",
    "questions = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"Who wrote the book '1984'?\",\n",
    "    \"What is the distance from Earth to the Moon?\"\n",
    "]\n",
    "\n",
    "responses = []\n",
    "\n",
    "for question in questions:\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-004\",\n",
    "        prompt=question,\n",
    "        max_tokens=100\n",
    "    )\n",
    "    responses.append(response.choices[0].text.strip())\n",
    "    time.sleep(1)  # 避免在短时间内发送过多请求\n",
    "\n",
    "for question, answer in zip(questions, responses):\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f8a448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下載預訓練模型和分詞工具\n",
    "model = BertModel.from_pretrained('bert-base-chinese', ignore_mismatched_sizes=True)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "\n",
    "# 假設數據\n",
    "texts = [('今天天氣真好', '今天天氣很不錯'), ('你好早安', '我想回家')]\n",
    "scores = [1.0, 0.0]\n",
    "\n",
    "# 对数据进行编码\n",
    "input_ids = tokenizer(texts, return_tensors='pt', padding=True, truncation=True);input_ids\n",
    "\n",
    "# 分詞\n",
    "# input_ids = tokenizer.encode(\"你好，世界！\", add_special_tokens=True);input_ids\n",
    "\n",
    "# 創建 Tensors\n",
    "scores = torch.tensor(scores)\n",
    "\n",
    "# 使用模型得到句子的表示\n",
    "outputs = model(input_ids)\n",
    "\n",
    "# 計算句子表示的餘弦相似度\n",
    "similarity_scores = torch.nn.functional.cosine_similarity(outputs[0], outputs[1])\n",
    "\n",
    "# # 微调模型\n",
    "# optimizer = Adam(model.parameters())\n",
    "# loss = torch.nn.functional.mse_loss(similarity_scores, scores)\n",
    "# loss.backward()\n",
    "# optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df30a475",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\student\\anaconda3\\envs\\web_scraping\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "# 載入預訓練模型及其分詞器\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "model = BertModel.from_pretrained('bert-base-chinese')\n",
    "\n",
    "# 輸入你的文本\n",
    "text = \"請將我換成你想要的任何文本。\"\n",
    "\n",
    "# 使用分詞器將文本轉換為模型可以理解的形式\n",
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "# 使用模型獲得文本的表示\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# 'outputs' 是一個 tuple，我們需要的是文本表示，它被儲存在第一個元素中\n",
    "text_embedding = outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba0d84d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.7956,  0.8762, -0.1610,  ...,  1.1726, -0.1286, -0.5807],\n",
      "         [-0.7632,  0.2071,  0.1727,  ...,  0.1169, -0.9048,  0.0898],\n",
      "         [-0.9492,  0.2783,  0.9922,  ...,  1.1149, -0.1266,  0.0922],\n",
      "         ...,\n",
      "         [ 0.0631,  0.8451,  0.0971,  ...,  1.0712,  0.7042, -0.1742],\n",
      "         [-0.3739,  1.4240,  0.2010,  ...,  0.6602, -0.2418, -0.4565],\n",
      "         [-1.2477, -0.0717,  0.4747,  ...,  0.0043, -0.0576,  0.0467]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(text_embedding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
