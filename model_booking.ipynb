{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd36d42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install torch\n",
    "# !pip install sentence_transformers\n",
    "# !pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3011c116",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\student\\anaconda3\\envs\\web_scraping\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "from torch.optim import Adam\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c998c4f0",
   "metadata": {},
   "source": [
    "## roberta 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0c81cd",
   "metadata": {},
   "source": [
    "### 原始評論(利用roberta原始的分詞方法)(會把每一個字都拆開)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b3c8a83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>飯店名稱</th>\n",
       "      <th>綜合評論</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>貓咪民宿Mini館-貓行為諮詢</td>\n",
       "      <td>乾淨、舒適、民宿內有3隻貓咪，非常可愛。民宿老闆很親切、開放提早中午就可以入住、熱心分享當地...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>貓咪民宿Mini館-貓行為諮詢</td>\n",
       "      <td>注重生活機能的話要三思。民宿主人非常熱情回答我的養貓困擾，也給我很多建議。位置太偏僻，飲食選擇不多</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>貓咪民宿Mini館-貓行為諮詢</td>\n",
       "      <td>服務人員熱心親切，貓咪很乖很可愛。無</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>貓咪民宿Mini館-貓行為諮詢</td>\n",
       "      <td>貓可愛，老闆親切。一樓廚房有碗盤及雜物堆積，想洗環保餐具時看到水槽太髒，就躲回房間浴室洗了</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>貓咪民宿Mini館-貓行為諮詢</td>\n",
       "      <td>老闆娘人非常好，很熱心的介紹好吃的食物，民宿的貓都很可愛很有個性。第一天晚上隔壁房客的小孩尖...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1130754</th>\n",
       "      <td>華大旅店-南西館</td>\n",
       "      <td>電梯</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1130755</th>\n",
       "      <td>一八遛遛居民宿</td>\n",
       "      <td>很棒的體驗，建築很新也很乾淨，房間裡的用品也都很有質感喔！小可惜是我們住的房間沒有桌椅，化妝...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1130756</th>\n",
       "      <td>風之島背包客民宿</td>\n",
       "      <td>讚。地點偏市區外整體住起來很舒服放假來這很愜意。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1130757</th>\n",
       "      <td>風之島背包客民宿</td>\n",
       "      <td>乾凈.舒適.安靜。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1130758</th>\n",
       "      <td>風之島背包客民宿</td>\n",
       "      <td>老闆人非常好，很溫馨的地方，環境乾淨，還有一隻超級可愛的狗狗。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1130759 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    飯店名稱                                               綜合評論\n",
       "0        貓咪民宿Mini館-貓行為諮詢  乾淨、舒適、民宿內有3隻貓咪，非常可愛。民宿老闆很親切、開放提早中午就可以入住、熱心分享當地...\n",
       "1        貓咪民宿Mini館-貓行為諮詢  注重生活機能的話要三思。民宿主人非常熱情回答我的養貓困擾，也給我很多建議。位置太偏僻，飲食選擇不多\n",
       "2        貓咪民宿Mini館-貓行為諮詢                                 服務人員熱心親切，貓咪很乖很可愛。無\n",
       "3        貓咪民宿Mini館-貓行為諮詢      貓可愛，老闆親切。一樓廚房有碗盤及雜物堆積，想洗環保餐具時看到水槽太髒，就躲回房間浴室洗了\n",
       "4        貓咪民宿Mini館-貓行為諮詢  老闆娘人非常好，很熱心的介紹好吃的食物，民宿的貓都很可愛很有個性。第一天晚上隔壁房客的小孩尖...\n",
       "...                  ...                                                ...\n",
       "1130754         華大旅店-南西館                                                 電梯\n",
       "1130755          一八遛遛居民宿  很棒的體驗，建築很新也很乾淨，房間裡的用品也都很有質感喔！小可惜是我們住的房間沒有桌椅，化妝...\n",
       "1130756         風之島背包客民宿                           讚。地點偏市區外整體住起來很舒服放假來這很愜意。\n",
       "1130757         風之島背包客民宿                                          乾凈.舒適.安靜。\n",
       "1130758         風之島背包客民宿                    老闆人非常好，很溫馨的地方，環境乾淨，還有一隻超級可愛的狗狗。\n",
       "\n",
       "[1130759 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ = pd.read_csv('./booking_comments_分詞update.csv', header=0)\n",
    "\n",
    "# 篩選評論數大於等於 30 的飯店\n",
    "df_filtered = df_.groupby('飯店名稱').filter(lambda x: len(x) >= 30)\n",
    "\n",
    "len(df_filtered['飯店名稱'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0a32b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 開始測量(爬取時間)\n",
    "startime = datetime.datetime.now()\n",
    "\n",
    "# 初始化 RoBERTa-wwm-ext 模型和分詞器\n",
    "tokenizer = BertTokenizer.from_pretrained('hfl/chinese-roberta-wwm-ext')\n",
    "model = BertModel.from_pretrained('hfl/chinese-roberta-wwm-ext')\n",
    "\n",
    "# 評論文本\n",
    "sentences = list(df_filtered.loc[:, '綜合評論'])\n",
    "\n",
    "# 初始化一個空的 list 用於儲存飯店資訊\n",
    "hotel_info_list = []\n",
    "\n",
    "# 獲取每個文本的嵌入向量，並儲存對應的飯店資訊\n",
    "embeddings = []\n",
    "for idx, sentence in enumerate(sentences):\n",
    "    tokens = tokenizer.tokenize(sentence)  # 分詞\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "    # 檢查序列長度是否超過 512\n",
    "    if len(inputs[\"input_ids\"][0]) > 512:\n",
    "        print(\"The sentence is too long. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings.append(outputs.last_hidden_state.mean(dim=1).numpy().flatten())\n",
    "    hotel_info_list.append(df_filtered.iloc[idx].to_dict())\n",
    "\n",
    "# 儲存飯店資訊與 embeddings\n",
    "df_hotel_info = pd.DataFrame(hotel_info_list)\n",
    "df_hotel_info.to_csv('hotel_info_roberta.csv', index=False)\n",
    "np.save('embeddings_roberta.npy', embeddings)\n",
    "\n",
    "# 結束測量\n",
    "endtime = datetime.datetime.now()\n",
    "\n",
    "# 輸出結果\n",
    "print(\"執行時間：\", endtime - startime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d30566c",
   "metadata": {},
   "source": [
    "##### 利用迴圈跑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63782e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "import datetime\n",
    "\n",
    "# 開始測量(時間)\n",
    "startime = datetime.datetime.now()\n",
    "\n",
    "df_ = pd.read_csv('./booking_comments_分詞update.csv', header=0)\n",
    "\n",
    "# 篩選評論數大於等於 30 的飯店\n",
    "df_filtered = df_.groupby('飯店名稱').filter(lambda x: len(x) >= 30)\n",
    "\n",
    "df_filtered = df_filtered[0:100000]\n",
    "\n",
    "df_filtered = df_filtered.reset_index(drop=True)\n",
    "\n",
    "# 初始化 RoBERTa-wwm-ext 模型和分詞器\n",
    "tokenizer = BertTokenizer.from_pretrained('hfl/chinese-roberta-wwm-ext')\n",
    "model = BertModel.from_pretrained('hfl/chinese-roberta-wwm-ext')\n",
    "\n",
    "# 定義每個批次的大小\n",
    "batch_size = 5000\n",
    "\n",
    "# 計算需要進行的批次數\n",
    "n_batches = (len(df_filtered) // batch_size) + 1\n",
    "\n",
    "# 開始進行批次處理\n",
    "for batch_idx in range(n_batches):\n",
    "    print(f\"Processing batch {batch_idx + 1} of {n_batches}...\")\n",
    "    \n",
    "    start_idx = batch_idx * batch_size\n",
    "\n",
    "    end_idx = (batch_idx + 1) * batch_size\n",
    "\n",
    "    # 評論文本\n",
    "    sentences = list(df_filtered.loc[start_idx:end_idx-1, '綜合評論'])\n",
    "\n",
    "    # 如果 sentences 是空的，則跳過該批次\n",
    "    if not sentences:\n",
    "        continue\n",
    "\n",
    "    # 初始化一個空的 list 用於儲存飯店資訊\n",
    "    hotel_info_list = []\n",
    "\n",
    "    # 獲取每個文本的嵌入向量，並儲存對應的飯店資訊\n",
    "    embeddings = []\n",
    "    for idx, sentence in enumerate(sentences):\n",
    "        if isinstance(sentence, str):\n",
    "            tokens = tokenizer.tokenize(sentence)  # 分詞\n",
    "            inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "        else:\n",
    "            print(f\"Sentence at index {idx} is not a string. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # 檢查序列長度是否超過 512\n",
    "        if len(inputs[\"input_ids\"][0]) > 512:\n",
    "            print(\"The sentence is too long. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        embeddings.append(outputs.last_hidden_state.mean(dim=1).numpy().flatten())\n",
    "        hotel_info_list.append(df_filtered.iloc[start_idx + idx].to_dict())\n",
    "\n",
    "    # 讀取原有的飯店資訊和 embeddings\n",
    "    if os.path.exists('hotel_info_roberta_綜合評論.csv') and os.path.exists('embeddings_roberta_綜合評論.npy'):\n",
    "        df_hotel_info_old = pd.read_csv('hotel_info_roberta_綜合評論.csv')\n",
    "        embeddings_old = np.load('embeddings_roberta_綜合評論.npy')\n",
    "\n",
    "    else:\n",
    "        df_hotel_info_old = pd.DataFrame()\n",
    "        embeddings_old = np.array([]).reshape(0,768)\n",
    "\n",
    "    # 儲存飯店資訊與 embeddings\n",
    "    df_hotel_info_new = pd.DataFrame(hotel_info_list)\n",
    "    df_hotel_info = pd.concat([df_hotel_info_old, df_hotel_info_new], ignore_index=True)\n",
    "    df_hotel_info.to_csv('hotel_info_roberta_綜合評論.csv', index=False)\n",
    "\n",
    "    embeddings_new = np.array(embeddings)\n",
    "    embeddings = np.vstack([embeddings_old, embeddings_new])\n",
    "    np.save('embeddings_roberta_綜合評論.npy', embeddings)\n",
    "    \n",
    "    print(f\"第{batch_idx + 1}批次儲存完畢\")\n",
    "    \n",
    "# 結束測量\n",
    "endtime = datetime.datetime.now()\n",
    "\n",
    "# 輸出結果\n",
    "print(\"執行時間：\", endtime - startime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edceaba2",
   "metadata": {},
   "source": [
    "### 讀取並進行篩選(先計算相似度再取平均)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83b080b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "\n",
    "def get_similar_hotels(df, embeddings, filter_dict, sentence, top_n, batch_size=5000):\n",
    "    # 篩選\n",
    "    for key, value in filter_dict.items():\n",
    "        if key in df.columns:\n",
    "            operation, target_value = value\n",
    "            if operation == \"==\":\n",
    "                df = df[df[key] == target_value]\n",
    "            elif operation == \"!=\":\n",
    "                df = df[df[key] != target_value]\n",
    "            elif operation == \">\":\n",
    "                df = df[df[key] > target_value]\n",
    "            elif operation == \"<\":\n",
    "                df = df[df[key] < target_value]\n",
    "            elif operation == \">=\":\n",
    "                df = df[df[key] >= target_value]\n",
    "            elif operation == \"<=\":\n",
    "                df = df[df[key] <= target_value]\n",
    "\n",
    "    embeddings_filtered = embeddings[df.index.values]\n",
    "\n",
    "    # 新的文本\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    new_embedding = outputs.last_hidden_state.mean(dim=1).numpy().flatten()\n",
    "\n",
    "    # 計算與新文本的相似性\n",
    "    similarities = []\n",
    "    num_batches = len(embeddings_filtered) // batch_size + 1\n",
    "    for i in range(num_batches):\n",
    "        start_index = i * batch_size\n",
    "        end_index = start_index + batch_size\n",
    "        batch = embeddings_filtered[start_index:end_index]\n",
    "        batch_similarity = cosine_similarity(np.vstack((batch, new_embedding)))\n",
    "        similarities.append(batch_similarity[-1, :-1])\n",
    "    similarities = np.concatenate(similarities)\n",
    "\n",
    "    # 建立新的 DataFrame，儲存相似度與對應的飯店名稱\n",
    "    df_similarity = pd.DataFrame({'飯店名稱': df['飯店名稱'], '相似度': similarities})\n",
    "\n",
    "    # 以飯店名稱分組，計算每間飯店的平均相似度\n",
    "    df_avg_similarity = df_similarity.groupby('飯店名稱').mean()\n",
    "\n",
    "    # 取出平均相似度最高的前 top_n 間飯店\n",
    "    top_hotels = df_avg_similarity.sort_values(by='相似度', ascending=False).head(top_n)\n",
    "    \n",
    "    return top_hotels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9b859d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取 DataFrame\n",
    "df_hotel_info = pd.read_csv('hotel_info_roberta_綜合評論.csv')\n",
    "\n",
    "# 讀取 embeddings\n",
    "embeddings = np.load('embeddings_roberta_綜合評論.npy')\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('hfl/chinese-roberta-wwm-ext')\n",
    "model = BertModel.from_pretrained('hfl/chinese-roberta-wwm-ext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77f87e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      相似度\n",
      "飯店名稱                     \n",
      "磐石旅店             0.652121\n",
      "太魯閣阿騫的家民宿        0.651672\n",
      "鹿台民宿             0.651581\n",
      "貓咪民宿Mini館-貓行為諮詢  0.650581\n",
      "墾丁君臨農場           0.650515\n",
      "嘉義優遊商旅           0.650436\n",
      "星享道酒店            0.650195\n",
      "宜蘭明水露渡假民宿        0.650051\n",
      "台糖長榮酒店- 台南       0.649993\n",
      "漫遊舍民宿            0.649875\n"
     ]
    }
   ],
   "source": [
    "# 建立篩選條件字典\n",
    "filter_dict = {'整體評分': (\">\", 7.0)}\n",
    "\n",
    "# 使用函數\n",
    "top_hotels = get_similar_hotels(df_hotel_info, embeddings, filter_dict, \"房間好\", 10)\n",
    "\n",
    "print(top_hotels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69048962",
   "metadata": {},
   "source": [
    "### 讀取並進行篩選(先對向量取平均再計算相似度)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c85e006c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算每間飯店的評論向量的平均值\n",
    "def compute_average_vectors(df, embeddings):\n",
    "    \n",
    "    df['embedding'] = list(embeddings)\n",
    "    df_avg_embedding = df.groupby('飯店名稱')['embedding'].apply(np.mean)\n",
    "\n",
    "    return df_avg_embedding\n",
    "\n",
    "# 根據篩選條件與新文本計算平均相似度\n",
    "def compute_similarity(df_avg_embedding, df, filter_dict, sentence):\n",
    "    \n",
    "    # 篩選\n",
    "    for key, value in filter_dict.items():\n",
    "        if key in df.columns:\n",
    "            operation, target_value = value\n",
    "            if operation == \"==\":\n",
    "                df = df[df[key] == target_value]\n",
    "            elif operation == \"!=\":\n",
    "                df = df[df[key] != target_value]\n",
    "            elif operation == \">\":\n",
    "                df = df[df[key] > target_value]\n",
    "            elif operation == \"<\":\n",
    "                df = df[df[key] < target_value]\n",
    "            elif operation == \">=\":\n",
    "                df = df[df[key] >= target_value]\n",
    "            elif operation == \"<=\":\n",
    "                df = df[df[key] <= target_value]\n",
    "\n",
    "    df_avg_embedding = df_avg_embedding[df_avg_embedding.index.isin(df['飯店名稱'])]\n",
    "\n",
    "    # 新的文本\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    new_embedding = outputs.last_hidden_state.mean(dim=1).numpy().flatten()\n",
    "\n",
    "    # 計算與新文本的相似性\n",
    "    similarities = cosine_similarity(np.vstack((df_avg_embedding.values.tolist(), new_embedding)))[-1, :-1]\n",
    "\n",
    "    # 建立新的 DataFrame，儲存相似度與對應的飯店名稱\n",
    "    df_similarity = pd.DataFrame({'飯店名稱': df_avg_embedding.index, '相似度': similarities})\n",
    "\n",
    "    return df_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7440bd62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-roberta-wwm-ext were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# 讀取 DataFrame\n",
    "df_hotel_info = pd.read_csv('hotel_info_roberta_綜合評論.csv')\n",
    "\n",
    "# 讀取 embeddings\n",
    "embeddings = np.load('embeddings_roberta_綜合評論.npy')\n",
    "\n",
    "# 計算平均向量\n",
    "df_avg_embedding = compute_average_vectors(df_hotel_info, embeddings)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('hfl/chinese-roberta-wwm-ext')\n",
    "model = BertModel.from_pretrained('hfl/chinese-roberta-wwm-ext')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5611c617",
   "metadata": {},
   "source": [
    "##### 篩選並計算相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d6109413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           飯店名稱       相似度\n",
      "197        涵園民宿  0.756361\n",
      "39         世奇商旅  0.754723\n",
      "124    康橋商旅-七賢館  0.741242\n",
      "203   玫瑰花園汽車旅館   0.735814\n",
      "165       旗山三合院  0.734303\n",
      "127    康橋商旅-覺民館  0.733816\n",
      "93      威尼斯汽車旅館  0.732871\n",
      "125  康橋商旅-三多商圈館  0.730270\n",
      "67        喜達絲飯店  0.730034\n",
      "94        媚力泊飯店  0.727803\n"
     ]
    }
   ],
   "source": [
    "# 建立篩選條件字典\n",
    "filter_dict = {'整體評分': (\">\", 3.0),'縣市': (\"==\", \"高雄市\")}\n",
    "\n",
    "df_similarity = compute_similarity(df_avg_embedding, df_hotel_info, filter_dict, \"早餐好吃\")\n",
    "\n",
    "top_n = 10\n",
    "\n",
    "top_hotels = df_similarity.sort_values(by='相似度', ascending=False).head(top_n)\n",
    "\n",
    "print(top_hotels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1609e2c",
   "metadata": {},
   "source": [
    "##### 比較花費時間測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd19eb65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-roberta-wwm-ext were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1 of 21...\n",
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n",
      "第1批次儲存完畢\n",
      "Processing batch 2 of 21...\n",
      "The sentence is too long. Skipping...\n",
      "第2批次儲存完畢\n",
      "Processing batch 3 of 21...\n",
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n",
      "第3批次儲存完畢\n",
      "Processing batch 4 of 21...\n",
      "The sentence is too long. Skipping...\n",
      "第4批次儲存完畢\n",
      "Processing batch 5 of 21...\n",
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n",
      "第5批次儲存完畢\n",
      "Processing batch 6 of 21...\n",
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n",
      "第6批次儲存完畢\n",
      "Processing batch 7 of 21...\n",
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n",
      "第7批次儲存完畢\n",
      "Processing batch 8 of 21...\n",
      "The sentence is too long. Skipping...\n",
      "第8批次儲存完畢\n",
      "Processing batch 9 of 21...\n",
      "The sentence is too long. Skipping...\n",
      "第9批次儲存完畢\n",
      "Processing batch 10 of 21...\n",
      "The sentence is too long. Skipping...\n",
      "第10批次儲存完畢\n",
      "Processing batch 11 of 21...\n",
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n",
      "第11批次儲存完畢\n",
      "Processing batch 12 of 21...\n",
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n",
      "第12批次儲存完畢\n",
      "Processing batch 13 of 21...\n",
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n",
      "第13批次儲存完畢\n",
      "Processing batch 14 of 21...\n",
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n",
      "第14批次儲存完畢\n",
      "Processing batch 15 of 21...\n",
      "第15批次儲存完畢\n",
      "Processing batch 16 of 21...\n",
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n",
      "第16批次儲存完畢\n",
      "Processing batch 17 of 21...\n",
      "The sentence is too long. Skipping...\n",
      "第17批次儲存完畢\n",
      "Processing batch 18 of 21...\n",
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n",
      "第18批次儲存完畢\n",
      "Processing batch 19 of 21...\n",
      "The sentence is too long. Skipping...\n",
      "第19批次儲存完畢\n",
      "Processing batch 20 of 21...\n",
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n",
      "第20批次儲存完畢\n",
      "Processing batch 21 of 21...\n",
      "執行時間： 1:26:39.549895\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "import datetime\n",
    "\n",
    "# 開始測量(時間)\n",
    "startime = datetime.datetime.now()\n",
    "\n",
    "df_ = pd.read_csv('./booking_comments_分詞update.csv', header=0)\n",
    "\n",
    "# 篩選評論數大於等於 30 的飯店\n",
    "df_filtered = df_.groupby('飯店名稱').filter(lambda x: len(x) >= 30)\n",
    "\n",
    "df_filtered = df_filtered[0:100000]\n",
    "\n",
    "df_filtered = df_filtered.reset_index(drop=True)\n",
    "\n",
    "# 初始化 RoBERTa-wwm-ext 模型和分詞器\n",
    "tokenizer = BertTokenizer.from_pretrained('hfl/chinese-roberta-wwm-ext')\n",
    "model = BertModel.from_pretrained('hfl/chinese-roberta-wwm-ext')\n",
    "\n",
    "# 定義每個批次的大小\n",
    "batch_size = 5000\n",
    "\n",
    "# 計算需要進行的批次數\n",
    "n_batches = (len(df_filtered) // batch_size) + 1\n",
    "\n",
    "# 開始進行批次處理\n",
    "for batch_idx in range(n_batches):\n",
    "    print(f\"Processing batch {batch_idx + 1} of {n_batches}...\")\n",
    "    \n",
    "    start_idx = batch_idx * batch_size\n",
    "\n",
    "    end_idx = (batch_idx + 1) * batch_size\n",
    "\n",
    "    # 評論文本\n",
    "    sentences = list(df_filtered.loc[start_idx:end_idx-1, '綜合評論'])\n",
    "\n",
    "    # 如果 sentences 是空的，則跳過該批次\n",
    "    if not sentences:\n",
    "        continue\n",
    "\n",
    "    # 初始化一個空的 list 用於儲存飯店資訊\n",
    "    hotel_info_list = []\n",
    "\n",
    "    # 獲取每個文本的嵌入向量，並儲存對應的飯店資訊\n",
    "    embeddings = []\n",
    "    for idx, sentence in enumerate(sentences):\n",
    "        if isinstance(sentence, str):\n",
    "            tokens = tokenizer.tokenize(sentence)  # 分詞\n",
    "            inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "        else:\n",
    "            print(f\"Sentence at index {idx} is not a string. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # 檢查序列長度是否超過 512\n",
    "        if len(inputs[\"input_ids\"][0]) > 512:\n",
    "            print(\"The sentence is too long. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        embeddings.append(outputs.last_hidden_state.mean(dim=1).numpy().flatten())\n",
    "        hotel_info_list.append(df_filtered.iloc[start_idx + idx].to_dict())\n",
    "\n",
    "    # 讀取原有的飯店資訊和 embeddings\n",
    "    if os.path.exists('hotel_info_roberta_test.csv') and os.path.exists('embeddings_roberta_test.npy'):\n",
    "        df_hotel_info_old = pd.read_csv('hotel_info_roberta_test.csv')\n",
    "        embeddings_old = np.load('embeddings_roberta_test.npy')\n",
    "\n",
    "    else:\n",
    "        df_hotel_info_old = pd.DataFrame()\n",
    "        embeddings_old = np.array([]).reshape(0,768)\n",
    "\n",
    "    # 儲存飯店資訊與 embeddings\n",
    "    df_hotel_info_new = pd.DataFrame(hotel_info_list)\n",
    "    df_hotel_info = pd.concat([df_hotel_info_old, df_hotel_info_new], ignore_index=True)\n",
    "    df_hotel_info.to_csv('hotel_info_roberta_test.csv', index=False)\n",
    "\n",
    "    embeddings_new = np.array(embeddings)\n",
    "    embeddings = np.vstack([embeddings_old, embeddings_new])\n",
    "    np.save('embeddings_roberta_test.npy', embeddings)\n",
    "    \n",
    "    print(f\"第{batch_idx + 1}批次儲存完畢\")\n",
    "    \n",
    "# 結束測量\n",
    "endtime = datetime.datetime.now()\n",
    "\n",
    "# 輸出結果\n",
    "print(\"執行時間：\", endtime - startime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aad9583",
   "metadata": {},
   "source": [
    "### 測試自行斷詞後的結果(roberta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45b12aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('0_10000.csv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c19b6062",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-roberta-wwm-ext were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n",
      "相似的文本 (索引 9302): ['房間', '小'], 相似度: 0.9714384078979492\n",
      "相似的文本 (索引 7713): ['房間', '算', '大'], 相似度: 0.9639410376548767\n",
      "相似的文本 (索引 6559): ['房間', '很', '大'], 相似度: 0.9479893445968628\n",
      "相似的文本 (索引 6710): ['房間', '很', '大'], 相似度: 0.9479893445968628\n",
      "相似的文本 (索引 6730): ['房間', '很', '大'], 相似度: 0.9479893445968628\n",
      "相似的文本 (索引 3851): ['房間', '很', '大'], 相似度: 0.9479893445968628\n",
      "相似的文本 (索引 7572): ['房間', '很', '大'], 相似度: 0.9479893445968628\n",
      "相似的文本 (索引 6796): ['房間', '很', '大'], 相似度: 0.9479893445968628\n",
      "相似的文本 (索引 9294): ['房間', '偏', '小'], 相似度: 0.927994966506958\n",
      "相似的文本 (索引 4188): ['房間', '稍', '小'], 相似度: 0.9203976392745972\n",
      "相似的文本 (索引 6951): ['乾淨', '房間', '大'], 相似度: 0.9122157096862793\n",
      "相似的文本 (索引 1642): ['房間', '太', '小'], 相似度: 0.900887131690979\n",
      "相似的文本 (索引 7985): ['房間', '大小'], 相似度: 0.8921487331390381\n",
      "相似的文本 (索引 1403): ['房間', '大小'], 相似度: 0.8921487331390381\n",
      "相似的文本 (索引 8195): ['房間', '大小'], 相似度: 0.8921487331390381\n",
      "相似的文本 (索引 6233): ['便宜', '房間', '大'], 相似度: 0.8913284540176392\n",
      "相似的文本 (索引 8610): ['房間', '小', '床', '很', '小'], 相似度: 0.8826092481613159\n",
      "相似的文本 (索引 774): ['房間', '很', '棒'], 相似度: 0.8820592164993286\n",
      "相似的文本 (索引 801): ['房間', '寬敞'], 相似度: 0.8808667659759521\n",
      "相似的文本 (索引 7569): ['房間', '乾淨', '和', '大'], 相似度: 0.8764951825141907\n",
      "相似的文本 (索引 992): ['房間', '浴室', '都', '很', '大'], 相似度: 0.8760460615158081\n",
      "相似的文本 (索引 6389): ['房間', '大', '舒適', '無'], 相似度: 0.8751977682113647\n",
      "相似的文本 (索引 436): ['房間', '很', '大', '太', '冷'], 相似度: 0.8732443451881409\n",
      "相似的文本 (索引 7235): ['房間', '大', '舒適'], 相似度: 0.8698259592056274\n",
      "相似的文本 (索引 7696): ['房間', '超大', '寬敞'], 相似度: 0.8666406273841858\n",
      "相似的文本 (索引 825): ['房間', '寬敞', '無'], 相似度: 0.8617373108863831\n",
      "相似的文本 (索引 7113): ['房間', '夠', '大', '舒服'], 相似度: 0.8598159551620483\n",
      "相似的文本 (索引 4631): ['沒', '窗戶', '房間', '小'], 相似度: 0.8595103621482849\n",
      "相似的文本 (索引 5244): ['房間', '格局', '很', '大', '舒適'], 相似度: 0.8584761619567871\n",
      "相似的文本 (索引 659): ['房', '內', '整體'], 相似度: 0.8537802696228027\n",
      "相似的文本 (索引 6986): ['房間', '格局'], 相似度: 0.8535634279251099\n",
      "相似的文本 (索引 5326): ['乾凈', '舒適', '房間', '大'], 相似度: 0.8530415296554565\n",
      "相似的文本 (索引 4075): ['房間', '不錯'], 相似度: 0.8530404567718506\n",
      "相似的文本 (索引 7602): ['床', '夠', '大'], 相似度: 0.8511996269226074\n",
      "相似的文本 (索引 8480): ['床', '夠', '大'], 相似度: 0.8511996269226074\n",
      "相似的文本 (索引 7678): ['床', '夠', '大'], 相似度: 0.8511996269226074\n",
      "相似的文本 (索引 2697): ['浴室', '大'], 相似度: 0.8497729301452637\n",
      "相似的文本 (索引 5879): ['都', '還', '可', '床', '與', '房間', '大小'], 相似度: 0.8483333587646484\n",
      "相似的文本 (索引 2580): ['房間', '大', '沒', '壓迫感'], 相似度: 0.8476015329360962\n",
      "相似的文本 (索引 4699): ['房間', '設施', '有', '蟲'], 相似度: 0.8450119495391846\n",
      "相似的文本 (索引 1810): ['車位', '不', '夠', '房間', '偏', '暗'], 相似度: 0.8447525501251221\n",
      "相似的文本 (索引 3250): ['空間', '小'], 相似度: 0.8442726731300354\n",
      "相似的文本 (索引 647): ['房', '內', '空間', '不', '太', '大'], 相似度: 0.8430776000022888\n",
      "相似的文本 (索引 7130): ['房間', '很', '大', '很', '寬敞'], 相似度: 0.842853307723999\n",
      "相似的文本 (索引 6667): ['浴室', '超大', '沒有'], 相似度: 0.840947687625885\n",
      "相似的文本 (索引 7133): ['床', '很', '大'], 相似度: 0.8402475714683533\n",
      "相似的文本 (索引 8936): ['房間', '隔音', '差'], 相似度: 0.8391607999801636\n",
      "相似的文本 (索引 860): ['房間', '很', '大', '床組', '很', '好', '睡'], 相似度: 0.837617039680481\n",
      "相似的文本 (索引 1551): ['床', '軟', '好', '睡', '房間', '有點', '小'], 相似度: 0.8361782431602478\n",
      "相似的文本 (索引 5521): ['人房', '空間', '大', '床', '好', '睡'], 相似度: 0.8358633518218994\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('hfl/chinese-roberta-wwm-ext')\n",
    "model = BertModel.from_pretrained('hfl/chinese-roberta-wwm-ext')\n",
    "\n",
    "sentences_tokenized = [eval(d) for d in df.loc[:, \"綜合評論_ws\"].values]\n",
    "\n",
    "# 獲取每個文本的嵌入向量\n",
    "embeddings = []\n",
    "sentences = []  # 新建一個列表來保存被處理的句子\n",
    "\n",
    "for sentence in sentences_tokenized:\n",
    "    \n",
    "    inputs = tokenizer.encode_plus(sentence, is_split_into_words=True, return_tensors=\"pt\")\n",
    "    \n",
    "    # 檢查序列長度是否超過 512\n",
    "    if len(inputs[\"input_ids\"][0]) > 512:\n",
    "        print(\"The sentence is too long. Skipping...\")\n",
    "        continue\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings.append(outputs.last_hidden_state.mean(dim = 1).numpy().flatten())\n",
    "    sentences.append(sentence)  # 將被處理的句子加到列表中\n",
    "\n",
    "# 新的文本\n",
    "new_sentence = \"房間大\"\n",
    "new_tokens = ['房間', '大'] # 分詞\n",
    "inputs = tokenizer(new_tokens, is_split_into_words=True, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "new_embedding = outputs.last_hidden_state.mean(dim=1).numpy().flatten()\n",
    "\n",
    "# 計算與新文本的相似性\n",
    "similarities = cosine_similarity(embeddings + [new_embedding])\n",
    "\n",
    "# 獲取最相似的 50 個文本的索引\n",
    "top50_indices = np.argsort(similarities[-1][:-1])[:-51:-1]\n",
    "\n",
    "# 印出最相似的 50 個文本\n",
    "for i, index in enumerate(top50_indices):\n",
    "    print(f\"相似的文本 (索引 {index}): {sentences[index]}, 相似度: {similarities[-1][index]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca29473a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "相似的文本 (索引 4343): ['早餐', '好吃'], 相似度: 1.000000238418579\n",
      "相似的文本 (索引 1529): ['早餐', '好吃'], 相似度: 1.000000238418579\n",
      "相似的文本 (索引 6567): ['早餐', '好吃'], 相似度: 1.000000238418579\n",
      "相似的文本 (索引 7509): ['早餐', '好吃'], 相似度: 1.000000238418579\n",
      "相似的文本 (索引 4348): ['早餐', '好吃'], 相似度: 1.000000238418579\n",
      "相似的文本 (索引 6958): ['早餐', '好吃'], 相似度: 1.000000238418579\n",
      "相似的文本 (索引 6491): ['早餐', '好吃'], 相似度: 1.000000238418579\n",
      "相似的文本 (索引 4098): ['早餐', '好吃'], 相似度: 1.000000238418579\n",
      "相似的文本 (索引 2429): ['早餐', '好吃'], 相似度: 1.000000238418579\n",
      "相似的文本 (索引 4827): ['早餐', '好吃'], 相似度: 1.000000238418579\n",
      "相似的文本 (索引 4190): ['早餐', '很', '好吃'], 相似度: 0.9652129411697388\n",
      "相似的文本 (索引 6052): ['早餐', '很', '好吃'], 相似度: 0.9652129411697388\n",
      "相似的文本 (索引 5181): ['早餐', '很', '好吃'], 相似度: 0.9652129411697388\n",
      "相似的文本 (索引 6429): ['早餐', '很', '好吃'], 相似度: 0.9652129411697388\n",
      "相似的文本 (索引 4300): ['早餐', '很', '好吃'], 相似度: 0.9652129411697388\n",
      "相似的文本 (索引 9561): ['早餐', '超', '好吃'], 相似度: 0.9607133865356445\n",
      "相似的文本 (索引 1245): ['早餐', '不', '好吃'], 相似度: 0.9587073922157288\n",
      "相似的文本 (索引 9445): ['早餐', '很', '好吃', '呦'], 相似度: 0.9332779049873352\n",
      "相似的文本 (索引 535): ['早餐', '很', '好'], 相似度: 0.9180421829223633\n",
      "相似的文本 (索引 1106): ['乾淨', '早餐', '好吃'], 相似度: 0.9138054847717285\n",
      "相似的文本 (索引 4362): ['早餐', '很', '棒'], 相似度: 0.9111084938049316\n",
      "相似的文本 (索引 4333): ['早餐', '很', '棒'], 相似度: 0.9111084938049316\n",
      "相似的文本 (索引 1109): ['早餐', '很', '棒'], 相似度: 0.9111084938049316\n",
      "相似的文本 (索引 599): ['早餐', '很', '棒'], 相似度: 0.9111084938049316\n",
      "相似的文本 (索引 9901): ['早餐', '很', '棒'], 相似度: 0.9111084938049316\n",
      "相似的文本 (索引 4880): ['早餐', '很', '棒'], 相似度: 0.9111084938049316\n",
      "相似的文本 (索引 741): ['早餐', '很', '棒'], 相似度: 0.9111084938049316\n",
      "相似的文本 (索引 4101): ['早餐', '很', '棒'], 相似度: 0.9111084938049316\n",
      "相似的文本 (索引 5153): ['早餐', '豐盛', '好吃'], 相似度: 0.909956693649292\n",
      "相似的文本 (索引 6007): ['早餐', '中西式', '都', '好吃'], 相似度: 0.9071521162986755\n",
      "相似的文本 (索引 4192): ['現', '煮', '早餐', '好好', '吃'], 相似度: 0.9032139778137207\n",
      "相似的文本 (索引 4140): ['早餐', '超棒'], 相似度: 0.9013780951499939\n",
      "相似的文本 (索引 1554): ['早餐', '好吃', '沒有'], 相似度: 0.9006266593933105\n",
      "相似的文本 (索引 7478): ['早餐', '好吃', '位置', '很', '好'], 相似度: 0.8989447951316833\n",
      "相似的文本 (索引 129): ['很', '棒', '的', '早餐'], 相似度: 0.8969095945358276\n",
      "相似的文本 (索引 6409): ['早餐', '豐盛', '好吃', '無'], 相似度: 0.8909722566604614\n",
      "相似的文本 (索引 9638): ['早餐', '很', '好吃', '很', '飽'], 相似度: 0.8906499743461609\n",
      "相似的文本 (索引 1098): ['早餐', '豐富', '美味'], 相似度: 0.8905787467956543\n",
      "相似的文本 (索引 4414): ['早餐', '好吃', '很', '豐盛'], 相似度: 0.8902044296264648\n",
      "相似的文本 (索引 4358): ['健康', '早餐', '很', '棒'], 相似度: 0.8896884322166443\n",
      "相似的文本 (索引 2910): ['早餐', '好吃值', '頗', '高'], 相似度: 0.8879324197769165\n",
      "相似的文本 (索引 4216): ['早餐', '很', '好吃', '超棒', '的'], 相似度: 0.8856137990951538\n",
      "相似的文本 (索引 7349): ['早餐', '很', '豐富'], 相似度: 0.8758634924888611\n",
      "相似的文本 (索引 1116): ['早餐', '超級', '好'], 相似度: 0.8735265731811523\n",
      "相似的文本 (索引 805): ['早餐', '非常', '棒'], 相似度: 0.8723956346511841\n",
      "相似的文本 (索引 4210): ['早餐', '非常', '棒'], 相似度: 0.8723956346511841\n",
      "相似的文本 (索引 1820): ['早餐', '非常', '不', '可口'], 相似度: 0.8720617890357971\n",
      "相似的文本 (索引 4296): ['豐盛', '的', '早餐'], 相似度: 0.8715789318084717\n",
      "相似的文本 (索引 757): ['早餐', '很', '豐盛'], 相似度: 0.8710860013961792\n",
      "相似的文本 (索引 8078): ['早餐', '很', '豐盛'], 相似度: 0.8710860013961792\n"
     ]
    }
   ],
   "source": [
    "# 新的文本\n",
    "new_sentence = \"早餐好吃\"\n",
    "new_tokens = ['早餐', '好吃'] # 分詞\n",
    "inputs = tokenizer(new_tokens, is_split_into_words=True, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "new_embedding = outputs.last_hidden_state.mean(dim=1).numpy().flatten()\n",
    "\n",
    "# 計算與新文本的相似性\n",
    "similarities = cosine_similarity(embeddings + [new_embedding])\n",
    "\n",
    "# 獲取最相似的 50 個文本的索引\n",
    "top50_indices = np.argsort(similarities[-1][:-1])[:-51:-1]\n",
    "\n",
    "# 印出最相似的 50 個文本\n",
    "for i, index in enumerate(top50_indices):\n",
    "    print(f\"相似的文本 (索引 {index}): {sentences[index]}, 相似度: {similarities[-1][index]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b1ae46",
   "metadata": {},
   "source": [
    "#### 使用roberta的不同層輸出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fee133fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_tokenized_ = [\" \".join(eval(d)) for d in df_.loc[0:50, \"綜合評論_ws\"].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8c622770",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-roberta-wwm-ext were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1 相似的文本 (索引 [45]): 早餐 超 豐盛 超, 相似度: 0.6999987363815308\n",
      "第 2 相似的文本 (索引 [3]): 沒有 附 早餐, 相似度: 0.6805572509765625\n",
      "第 3 相似的文本 (索引 [22]): 滿意, 相似度: 0.5698429346084595\n",
      "第 4 相似的文本 (索引 [23]): 沒有 髒, 相似度: 0.558742105960846\n",
      "第 5 相似的文本 (索引 [35]): 地 上 有 頭髮 不 夠 乾淨, 相似度: 0.5520568490028381\n",
      "第 6 相似的文本 (索引 [39]): 房間 升級 房間 及 衛浴 空間 舒適 早餐 每 天 都 是 培根 起司 口味, 相似度: 0.5247654914855957\n",
      "第 7 相似的文本 (索引 [40]): 草地 夠 大 風 好 大, 相似度: 0.5237064957618713\n",
      "第 8 相似的文本 (索引 [17]): 地點 方便 停車位 多 早餐 也 很 不錯 補菜 的 速度 也 很 快 早餐 沒有 醬瓜 房間 太 昏暗, 相似度: 0.5115199089050293\n",
      "第 9 相似的文本 (索引 [1]): 整體 上 都 不錯 無 飲水機 有點 不 方便, 相似度: 0.5094053745269775\n",
      "第 10 相似的文本 (索引 [8]): 有 桌遊 可以 玩, 相似度: 0.5080175399780273\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('hfl/chinese-roberta-wwm-ext')\n",
    "model = BertModel.from_pretrained('hfl/chinese-roberta-wwm-ext', output_hidden_states=True)\n",
    "\n",
    "embeddings = []\n",
    "for sentence in sentences_tokenized_:\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # 獲取最後四層的輸出\n",
    "    last_four_layers = outputs.hidden_states[-4:]\n",
    "    # 合併\n",
    "    embedding = torch.mean(torch.stack(last_four_layers), dim=0).mean(dim=1).numpy()\n",
    "    embeddings.append(embedding.squeeze())\n",
    "\n",
    "new_tokens = ['早餐', '不好'] # 分詞\n",
    "inputs = tokenizer(new_tokens, is_split_into_words=True, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "last_four_layers = outputs.hidden_states[-4:]\n",
    "new_embedding = torch.mean(torch.stack(last_four_layers), dim=0).mean(dim=1).numpy()\n",
    "\n",
    "embeddings_np = np.array(embeddings)\n",
    "\n",
    "similarities = cosine_similarity(embeddings_np, new_embedding.reshape(1, -1))\n",
    "top_indices = np.argsort(similarities, axis=0)[::-1]\n",
    "\n",
    "for i, index in enumerate(top_indices[:10]):\n",
    "    print(f\"第 {i+1} 相似的文本 (索引 {index}): {sentences_tokenized_[index[0]]}, 相似度: {similarities[index[0]][0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec0272d",
   "metadata": {},
   "source": [
    "### 觀察自行分詞與bert內建分詞的差異"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5d4c0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['房', '間', '小']\n",
      "{'input_ids': tensor([[ 101, 2791,  102],\n",
      "        [ 101, 7279,  102],\n",
      "        [ 101, 2207,  102]]), 'token_type_ids': tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1]])}\n",
      "['房間', '小']\n",
      "{'input_ids': tensor([[ 101, 2791, 7279, 2207,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "new_sentence = \"房間小\"\n",
    "\n",
    "# inputs = tokenizer(new_sentence, return_tensors=\"pt\")\n",
    "new_tokens = tokenizer.tokenize(new_sentence)\n",
    "inputs = tokenizer(new_tokens, return_tensors=\"pt\")\n",
    "\n",
    "print(new_tokens)\n",
    "print(inputs)\n",
    "\n",
    "new_tokens = ['房間', '小'] # 分詞\n",
    "inputs = tokenizer(new_tokens, is_split_into_words=True, return_tensors=\"pt\")\n",
    "\n",
    "print(new_tokens)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9b1224",
   "metadata": {},
   "source": [
    "#### 嘗試只保留有意義的詞性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36443dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('0_10000.csv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2676eaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_pos = set(['Nep', 'Nh', 'Nb'])  # 這 3 種詞性不保留\n",
    "\n",
    "# 分詞後評論\n",
    "sentences_tokenized = [eval(d) for d in df.loc[:, \"綜合評論_ws\"].values]\n",
    "\n",
    "# 分詞後詞性\n",
    "pos_tags = [eval(d) for d in df.loc[:, \"綜合評論_pos\"].values]\n",
    "\n",
    "short_sentence = []  # 放過濾後的句子\n",
    "\n",
    "for sentence, sentence_pos_tags in zip(sentences_tokenized, pos_tags):\n",
    "\n",
    "    short_sentence_sentence = []  # 儲存過濾後的句子（不帶詞性）\n",
    "    \n",
    "    for word_ws, word_pos in zip(sentence, sentence_pos_tags):\n",
    "        \n",
    "        # 只留名詞和動詞\n",
    "        is_N_or_V = word_pos.startswith(\"V\") or word_pos.startswith(\"N\")\n",
    "\n",
    "        # 去掉名詞裡的某些詞性\n",
    "        is_not_stop_pos = word_pos not in stop_pos\n",
    "\n",
    "        if is_N_or_V and is_not_stop_pos:\n",
    "            \n",
    "            short_sentence_sentence.append(f\"{word_ws}\")\n",
    "\n",
    "    short_sentence.append(short_sentence_sentence)\n",
    "\n",
    "# print(sentences_tokenized[0:3])\n",
    "# print(short_sentence[0:3])\n",
    "\n",
    "# 去除空的列表\n",
    "short_sentence = [sentence for sentence in short_sentence if sentence]  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a0feb1",
   "metadata": {},
   "source": [
    "### 利用篩選詞性後的結果做 roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9ebfee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-roberta-wwm-ext were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentence is too long. Skipping...\n",
      "相似的文本 (索引 3834): ['房間', '大'], 相似度: 0.9999998211860657\n",
      "相似的文本 (索引 6677): ['房間', '大'], 相似度: 0.9999998211860657\n",
      "相似的文本 (索引 7534): ['房間', '大'], 相似度: 0.9999998211860657\n",
      "相似的文本 (索引 6762): ['房間', '大'], 相似度: 0.9999998211860657\n",
      "相似的文本 (索引 6697): ['房間', '大'], 相似度: 0.9999998211860657\n",
      "相似的文本 (索引 6527): ['房間', '大'], 相似度: 0.9999998211860657\n",
      "相似的文本 (索引 4170): ['房間', '小'], 相似度: 0.9714384078979492\n",
      "相似的文本 (索引 9246): ['房間', '小'], 相似度: 0.9714384078979492\n",
      "相似的文本 (索引 1637): ['房間', '小'], 相似度: 0.9714384078979492\n",
      "相似的文本 (索引 7674): ['房間', '算', '大'], 相似度: 0.9639410376548767\n",
      "相似的文本 (索引 8884): ['房間', '差'], 相似度: 0.9296784996986389\n",
      "相似的文本 (索引 9238): ['房間', '偏', '小'], 相似度: 0.927994966506958\n",
      "相似的文本 (索引 989): ['房間', '浴室', '大'], 相似度: 0.9196709394454956\n",
      "相似的文本 (索引 8559): ['房間', '小', '床', '小'], 相似度: 0.9166770577430725\n",
      "相似的文本 (索引 6916): ['乾淨', '房間', '大'], 相似度: 0.9122157096862793\n",
      "相似的文本 (索引 7094): ['房間', '大', '寬敞'], 相似度: 0.9050126075744629\n",
      "相似的文本 (索引 5854): ['床', '房間', '大小'], 相似度: 0.8997808694839478\n",
      "相似的文本 (索引 7943): ['房間', '大小'], 相似度: 0.8921487331390381\n",
      "相似的文本 (索引 1398): ['房間', '大小'], 相似度: 0.8921487331390381\n",
      "相似的文本 (索引 8146): ['房間', '大小'], 相似度: 0.8921487331390381\n",
      "相似的文本 (索引 6206): ['便宜', '房間', '大'], 相似度: 0.8913284540176392\n",
      "相似的文本 (索引 771): ['房間', '棒'], 相似度: 0.8889296054840088\n",
      "相似的文本 (索引 6279): ['寬敞', '房間', '好'], 相似度: 0.886948823928833\n",
      "相似的文本 (索引 435): ['房間', '大', '冷'], 相似度: 0.8835815787315369\n",
      "相似的文本 (索引 4534): ['房間', '大', '舒服'], 相似度: 0.8827222585678101\n",
      "相似的文本 (索引 7077): ['房間', '大', '舒服'], 相似度: 0.8827222585678101\n",
      "相似的文本 (索引 7657): ['房間', '寬敞'], 相似度: 0.8808667659759521\n",
      "相似的文本 (索引 798): ['房間', '寬敞'], 相似度: 0.8808667659759521\n",
      "相似的文本 (索引 646): ['房', '內', '空間', '大'], 相似度: 0.880531907081604\n",
      "相似的文本 (索引 6360): ['房間', '大', '舒適', '無'], 相似度: 0.8751977682113647\n",
      "相似的文本 (索引 6103): ['浴室', '大', '房間', '小', '差'], 相似度: 0.8750342726707458\n",
      "相似的文本 (索引 6720): ['房間', '吵', '差'], 相似度: 0.8724051713943481\n",
      "相似的文本 (索引 4694): ['陽台', '棒', '房間', '小'], 相似度: 0.8704512119293213\n",
      "相似的文本 (索引 7814): ['房間', '大', '舒適'], 相似度: 0.8698259592056274\n",
      "相似的文本 (索引 8387): ['房間', '大', '舒適'], 相似度: 0.8698259592056274\n",
      "相似的文本 (索引 7198): ['房間', '大', '舒適'], 相似度: 0.8698259592056274\n",
      "相似的文本 (索引 7531): ['房間', '乾淨', '大'], 相似度: 0.8679437637329102\n",
      "相似的文本 (索引 6196): ['房間', '燈', '暗'], 相似度: 0.8660553693771362\n",
      "相似的文本 (索引 492): ['浴室', '大', '寬敞', '無'], 相似度: 0.8654581904411316\n",
      "相似的文本 (索引 8710): ['房間', '大', '舒適', '沒有'], 相似度: 0.8630586266517639\n",
      "相似的文本 (索引 8040): ['房間', '空間', '大', '住', '舒服'], 相似度: 0.8618459701538086\n",
      "相似的文本 (索引 822): ['房間', '寬敞', '無'], 相似度: 0.8617373108863831\n",
      "相似的文本 (索引 597): ['房間', '舒服', '好'], 相似度: 0.8612924814224243\n",
      "相似的文本 (索引 4612): ['沒', '窗戶', '房間', '小'], 相似度: 0.8595103621482849\n",
      "相似的文本 (索引 1805): ['車位', '夠', '房間', '偏', '暗'], 相似度: 0.858141303062439\n",
      "相似的文本 (索引 7589): ['房間', '不錯', '睡'], 相似度: 0.8581225275993347\n",
      "相似的文本 (索引 1594): ['房間', '舒適', '差'], 相似度: 0.8569594621658325\n",
      "相似的文本 (索引 8981): ['房間', '海景', '棒'], 相似度: 0.854246973991394\n",
      "相似的文本 (索引 658): ['房', '內', '整體'], 相似度: 0.8537802696228027\n",
      "相似的文本 (索引 6950): ['房間', '格局'], 相似度: 0.8535634279251099\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('hfl/chinese-roberta-wwm-ext')\n",
    "model = BertModel.from_pretrained('hfl/chinese-roberta-wwm-ext')\n",
    "\n",
    "# 獲取每個文本的嵌入向量\n",
    "embeddings = []\n",
    "sentences = []  # 新建一個列表來保存被處理的句子\n",
    "\n",
    "for sentence in short_sentence:\n",
    "    \n",
    "    inputs = tokenizer.encode_plus(sentence, is_split_into_words=True, return_tensors=\"pt\")\n",
    "    \n",
    "    # 檢查序列長度是否超過 512\n",
    "    if len(inputs[\"input_ids\"][0]) > 512:\n",
    "        print(\"The sentence is too long. Skipping...\")\n",
    "        continue\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings.append(outputs.last_hidden_state.mean(dim = 1).numpy().flatten())\n",
    "    sentences.append(sentence)  # 將被處理的句子加到 list\n",
    "\n",
    "# 新的文本\n",
    "new_sentence = \"房間大\"\n",
    "new_tokens = ['房間', '大'] # 分詞\n",
    "inputs = tokenizer(new_tokens, is_split_into_words=True, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "new_embedding = outputs.last_hidden_state.mean(dim=1).numpy().flatten()\n",
    "\n",
    "# 計算與新文本的相似性\n",
    "similarities = cosine_similarity(embeddings + [new_embedding])\n",
    "\n",
    "# 獲取最相似的 50 個文本的索引\n",
    "top50_indices = np.argsort(similarities[-1][:-1])[:-51:-1]\n",
    "\n",
    "# 最相似的 50 個文本\n",
    "for i, index in enumerate(top50_indices):\n",
    "    print(f\"相似的文本 (索引 {index}): {sentences[index]}, 相似度: {similarities[-1][index]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483fdb21",
   "metadata": {},
   "source": [
    "## Doc2Vec 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d22f35c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "執行時間： 2:39:24.482711\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# 開始測量(時間)\n",
    "startime = datetime.datetime.now()\n",
    "\n",
    "df_ = pd.read_csv('./booking_comments_分詞update.csv', header=0)\n",
    "\n",
    "# 篩選評論數大於等於 30 的飯店\n",
    "df_filtered_doc2vec = df_.groupby('飯店名稱').filter(lambda x: len(x) >= 30)\n",
    "\n",
    "df_filtered_doc2vec = df_filtered_doc2vec[0:100000]\n",
    "\n",
    "df_filtered_doc2vec = df_filtered_doc2vec.reset_index(drop=True)\n",
    "\n",
    "# 分詞後評論\n",
    "sentences_tokenized = [eval(d) for d in df_filtered_doc2vec.loc[:, \"綜合評論_cut\"].values]\n",
    "\n",
    "# 準備訓練數據(轉成模型會吃的樣子)\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(sentences_tokenized)]\n",
    "\n",
    "# 訓練 Doc2Vec 模型\n",
    "model = Doc2Vec(documents, vector_size=768, min_count=2, epochs=1000) # min_count:單詞最少須出現次數\n",
    "\n",
    "# 使用模型將文本轉換為向量\n",
    "embeddings = [model.infer_vector(doc) for doc in sentences_tokenized]\n",
    "print(len(embeddings))\n",
    "\n",
    "# 結束測量\n",
    "endtime = datetime.datetime.now()\n",
    "\n",
    "# 輸出結果\n",
    "print(\"執行時間：\", endtime - startime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "379da968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1 相似的文本 (索引 [20418]): ['支付', '路邊', '停車費', '充足', '停車', '位', '自行', '開車', '房客', '地點', '不錯', '晚餐', '散步', '至愛河', '入住', '家庭', '房', '玩具', '出發', '前有', '提醒', '消毒', '房內', '玩具', '積', '木盒', '開啟', '充滿', '未乾', '酒精', '自行', '乾布', '擦拭', '房務人員', '開啟', '盒子', '噴上', '酒精', '關上', '幼兒', '澡盆', '水瓢', '小椅子', '全是', '發黴', '狀態', '清潔', '加強', '早餐', '種類', '選擇', '少', '15', '用餐', '菜盤', '已空', '不補', '空盤', '空碗', '不補'], 相似度: 0.13786755502223969\n",
      "第 2 相似的文本 (索引 [35488]): ['房內', '裝置', '老舊', '不好', '停車', '早餐', '選擇', '較少'], 相似度: 0.12939956784248352\n",
      "第 3 相似的文本 (索引 [49408]): ['天悅剛', '營運', '每次', '入住', '很棒', '入住', '走道', '房內', '地毯', '久', '老舊', '裝置', '維護', '感謝', '房務人員', '用心', '整理', '房間', '謝謝', '早餐', '接待', '人員'], 相似度: 0.1287374198436737\n",
      "第 4 相似的文本 (索引 [40442]): ['地點', '佳', '早餐', '豐盛', '用餐', '環境', '整潔', '餐廳', '入內時', '接待', '人員', '提醒', '取餐', '配戴', '口罩', '房內', '衛浴', '乾淨', '整潔', '裝置', '新穎', '值得', '推薦', '電視', '遙控器', '連線', '不佳', '隔音', '稍差', '房間', '內會', '聽到', '隔壁', '電視', '聲', '浴室', '水時', '聲音'], 相似度: 0.12402409315109253\n",
      "第 5 相似的文本 (索引 [62702]): ['未', '享用', '早餐', '地點', '廁所', '浴室', '不適', '應'], 相似度: 0.12295426428318024\n",
      "第 6 相似的文本 (索引 [75511]): ['地點', '很棒', '枕頭', '太軟', '支撐', '脖子', '插座', '太少', '床邊', '插座'], 相似度: 0.12229496240615845\n",
      "第 7 相似的文本 (索引 [25890]): ['很棒', '清爽', '早餐'], 相似度: 0.1212189719080925\n",
      "第 8 相似的文本 (索引 [42415]): ['揹', '包', '客', '棧', '價效', '高', 'hostel', '睡覺', '空間', '安靜', '乾淨', '公共', '空間', '很大', '特色', '適合', '發懶', '安靜', '看書', '或用', '筆電', '最驚', '豔', '健康', '美味', '早餐', '好吃', '👍', '整體', '滿意', '次', '揹', '包', '體驗', '廁所', '少', '增加', '間會'], 相似度: 0.12041069567203522\n",
      "第 9 相似的文本 (索引 [69823]): ['廁所', '🚽', '衛浴', '裝置', '水管聲', '音', '吵雜', '安心', '入睡'], 相似度: 0.1193365529179573\n",
      "第 10 相似的文本 (索引 [84055]): ['客人', '走道', '交談', '聲會', '影響', '房內', '休息'], 相似度: 0.11838981509208679\n"
     ]
    }
   ],
   "source": [
    "# 對新的句子執行相同的操作\n",
    "new_sentence = \"早餐好吃\"\n",
    "new_embedding = model.infer_vector(new_sentence.split())\n",
    "\n",
    "# 將嵌入向量列表轉為 numpy array，以便後續使用 cosine_similarity 函数\n",
    "embeddings_np = np.array(embeddings)\n",
    "\n",
    "# 計算新的句子和已有句子的相似度\n",
    "similarities = cosine_similarity(embeddings_np, new_embedding.reshape(1, -1))\n",
    "\n",
    "# 排列相似度\n",
    "top_indices = np.argsort(similarities, axis=0)[::-1]\n",
    "\n",
    "# 印出最相似的 n 個句子\n",
    "for i, index in enumerate(top_indices[:10]):\n",
    "    print(f\"第 {i+1} 相似的文本 (索引 {index}): {sentences_tokenized[index[0]]}, 相似度: {similarities[index[0]][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33adf89",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "697e1cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "飯店名稱：陽光滿屋民宿\n",
      "平均相似度:0.11570316506156822\n",
      "前10條相似的評論:\n",
      "評論：舒適 房間，相似度：0.5655955612619636\n",
      "評論：乾淨 舒適 房間，相似度：0.4529094419157692\n",
      "評論：推薦 房間 乾淨 不錯 房間 內 無 冰箱 有 共用，相似度：0.3734171246078148\n",
      "評論：房間 乾淨 看 出 屋主 用心，相似度：0.22074116484931985\n",
      "評論：整體 棒 地點 不錯 大 片 窗戶 外面 有 連結感 地板 乾淨 熱水 燙 水壓 足 沒 暖氣 訂 個 房間 牆壁 上 有 幅 畫 房間 陽光感 風格 不符 感覺 看 久 頭 暈 正面 床 難 看到，相似度：0.1696819678971862\n",
      "評論：陽光 地方 房間 不錯 熱水 熱 晚上 安靜 有 小 螞蟻，相似度：0.15946363273550007\n",
      "評論：民宿 地點 活水湖 近 車程 五 分鐘 無 市區 鬧區 噪音 老闆 熱心 解答 任何 問題 房間 整潔 乾淨 希望 有 含 早餐 退房 時間 早點 房間 冷氣 涼 需要 設定到 低 溫度，相似度：0.15127160639049955\n",
      "評論：海岸 公園 近 民宿 房間 內 看見 海 民宿 地點 不錯 安靜 停車 方便 進門處 鞋子 沒 地方 擺 顯 亂 房間 垃圾桶 裏 遺留 上 個 房客 泡麵碗 洗手台 螞蟻 一 大 堆 腳 撞 狂 床框，相似度：0.14818309750777228\n",
      "評論：房間 乾淨 水壓 大 熱水 快 來 洗澡 舒服 市區 開車 分鐘 內 到達 路 邊 停車 方便 消毒水 味道 重，相似度：0.11527584378112371\n",
      "評論：早上 五點 多 有 公雞 叫 起床 六點 多 有 戰機 飛過 房間 廁所 乾淨，相似度：0.09638061657609528\n",
      "\n",
      "\n",
      "飯店名稱：家有囍宿\n",
      "平均相似度:0.09766333643934164\n",
      "前10條相似的評論:\n",
      "評論：房間 大 間 衛浴 設備 讚 有 按摩 浴缸 值 高 下 次 來 時 有 房間 住，相似度：0.3924007059710153\n",
      "評論：早餐 直接 送 餐券 方便 安靜 舒適 房間 大 枕頭 扁 床 軟 中間 陷下去 睡 右邊 一點 建議 翻面，相似度：0.09591597622569287\n",
      "評論：老闆 親切 民宿 整潔 生活 機能 佳 便利商店 各 種 食 小吃 餐廳 有 停車 方便 民宿 客廳 廚房 應有盡有 床 軟，相似度：0.0\n",
      "評論：按摩 浴缸 有 老闆 服務 態度 沒有，相似度：0.0\n",
      "評論：早餐 換 早餐店 餐券 環境 鬧中取靜 女性 友好 沒 化妝桌 備品，相似度：0.0\n",
      "\n",
      "\n",
      "飯店名稱：魔法精靈民宿\n",
      "平均相似度:0.09533750208640472\n",
      "前10條相似的評論:\n",
      "評論：老闆 親切 房間 乾淨 整潔 房間 乾淨 整潔 電視 訊號 不佳，相似度：0.2786818920562877\n",
      "評論：喜歡 貴婦 風格 入住 每 間 房間 個 有 特色 房 內 獨立 冰箱 一樓 有 共用 冰箱，相似度：0.10266811628933122\n",
      "評論：老闆娘 人 親切 推離 火車站 附近，相似度：0.0\n",
      "評論：火車站 近 客廳 空間 提供 很多 茶水 餅乾，相似度：0.0\n",
      "\n",
      "\n",
      "飯店名稱：朝日民宿 - Peng's Family\n",
      "平均相似度:0.08154258317783944\n",
      "前10條相似的評論:\n",
      "評論：房間 差，相似度：1.0\n",
      "評論：房間 新 乾淨，相似度：0.603128363134945\n",
      "評論：房間 有 螞蟻，相似度：0.36113605940999854\n",
      "評論：房間 乾淨 退房，相似度：0.3388064899506201\n",
      "評論：房間 乾淨 漂亮，相似度：0.330553406895637\n",
      "評論：價格 實惠 房間 乾淨，相似度：0.25329024901002034\n",
      "評論：位置 佳 早餐 不錯 房間 有 蟑螂 木 地板 有 鐵釘 露出 電視 有 雜音 房間 設備 老舊，相似度：0.22584727722100517\n",
      "評論：老闆娘 親切 服務 房間 裝潢 用心 價格 沒有，相似度：0.19145850105749293\n",
      "評論：舒適 寬敞 房間 棒 機車 停進去，相似度：0.17583815361392374\n",
      "評論：算 一 間 旅遊 選擇 入住 民宿 房間 寬敞 光線 明亮 海岸 公路 近 老闆娘 客氣 房間 沒有 小 茶几 四 人 房 有 一 把 椅子 浴室 洗手台 沒有 置物台，相似度：0.1663963089878792\n",
      "\n",
      "\n",
      "飯店名稱：海洋風情民宿\n",
      "平均相似度:0.0786606961632868\n",
      "前10條相似的評論:\n",
      "評論：房間 乾淨 舒適 老闆娘 人 親切，相似度：0.2889327186669041\n",
      "評論：居住 環境 安靜 房間 乾淨，相似度：0.22282414969116604\n",
      "評論：老闆娘 人 親切 房間 漂亮 舒適 下 次 有 機會 入住，相似度：0.1992473366109996\n",
      "評論：老闆娘 親切 小孩 友善 房間 天花板 晚上 有 星空 孩子 喜歡，相似度：0.13088128555018277\n",
      "評論：房間 屋頂 漂亮 入住 時 告知 人 多 水 熱 等 分鐘 等 一 個 多 小時 水 冷 冷 大 冬天 冷水 有 棉被 薄 涼被 厚，相似度：0.1073690257158103\n",
      "評論：第一 次 住有 星空 景觀 房型 感覺 開心 民宿 業者 熱心 介紹 附近 景點 提供 相關 資料 住宅區 寧靜 週圍 好 停車 房間 乾淨 舒適 好 民宿，相似度：0.0733345338876655\n",
      "評論：住宿 地點 佳 環境 清幽 風景區 市區 近 停車 算 方便 客房 大 夜燈 美麗 投影 天花板 星空 讓 人 忘卻 一切 煩惱 浴室 空間 大 多功能 蓮蓬頭 喜好 調整 有 浴缸 泡澡 洗去 一 天 疲憊 浴室 漱口杯 蓋 煙蒂 想要 使用 拿起來 有 滿滿 菸味 上 一 位 房客 藏 浴室 部分 請 打掃 時 注意 一下，相似度：0.0\n",
      "評論：入住 時 老闆娘 接待 親切 提供 多 實用 旅遊 建議 代訂 早餐 好吃 之後 去 花蓮 想 入住，相似度：0.0\n",
      "評論：整體 環境 清潔度 滿意 老闆娘 熱情 無，相似度：0.0\n",
      "評論：純真 童趣 溫馨 民宿，相似度：0.0\n",
      "\n",
      "\n",
      "飯店名稱：班卡拉渡假旅店\n",
      "平均相似度:0.07695721092689337\n",
      "前10條相似的評論:\n",
      "評論：床 大 好 睡 房間 大，相似度：1.0\n",
      "評論：房間 大 舒適 無，相似度：0.5655955612619636\n",
      "評論：寬敞 房間 好，相似度：0.4340281943246662\n",
      "評論：沒有 房間 內 浴室 沒有 天花板 房間 隔開 上廁所 味道 跑到 房間 內，相似度：0.3449057772268462\n",
      "評論：老闆 主動 升等 房間 房間 乾淨 舒適 乾淨 舒適 便宜 無，相似度：0.3074710721695949\n",
      "評論：裝修 更新 一下 好 房間 大 停車 方便 廁所 房間 沒有 隔間 夫妻 入住 感覺 方便 味道 留在 房間 內，相似度：0.29420243523027806\n",
      "評論：房間 寬敞 舒適 床 軟 好 睡 早餐 好吃，相似度：0.292365179018659\n",
      "評論：停車 方便 房間 內 有 蚊子，相似度：0.27175433504817104\n",
      "評論：房間 格局 不佳，相似度：0.25024482283159055\n",
      "評論：房間 整潔 環境 清幽 安靜，相似度：0.22636900209342142\n",
      "\n",
      "\n",
      "飯店名稱：小巷民宿\n",
      "平均相似度:0.07618021017611262\n",
      "前10條相似的評論:\n",
      "評論：房間 舒適 乾淨 停車 方便 床 軟 晚上 房間 冷，相似度：0.4869619749612681\n",
      "評論：民宿 主人 友善 房間 舒適 乾淨 早餐 好吃，相似度：0.20305808167314005\n",
      "評論：家 一樣 舒適 進到 民宿 回到 家 裡面 一樣 民宿 夫婦 位 孩子 親切 問候 房間 大 床 枕頭 舒服 重點 房 內 有 個 大 桌子 房 內 用餐 方便 可惜 疫情 期間 吃 到手 作 早餐 安全 第一，相似度：0.07178204512671808\n",
      "評論：推薦給 喜歡 整潔 簡單 平價 清靜 住宿 條件 者 整潔，相似度：0.0\n",
      "評論：民宿 主人 用心 準備 早餐 熱情 分享 介紹 景點 寄宿 朋友 家 一樣 棒 住宿 經驗，相似度：0.0\n",
      "評論：溫馨 早餐 老闆娘 老闆 動手 做 好吃，相似度：0.0\n",
      "評論：乾淨 陽台 熱情 老闆，相似度：0.0\n",
      "評論：安靜 清潔 老闆 用心 經營 安排 太平山 武陵 農場 中繼 過夜 好 地點，相似度：0.0\n",
      "評論：住宿 環境 舒適 早餐 配合 天氣 提供 熱飲 熱食 好，相似度：0.0\n",
      "評論：民宿 男女 主人 服務 親切 備 小 點心 供 房客 解饞，相似度：0.0\n",
      "\n",
      "\n",
      "飯店名稱：羅東公園61號(峇里風)特色包棟民宿\n",
      "平均相似度:0.07312529675005397\n",
      "前10條相似的評論:\n",
      "評論：兩 間 房間 包 棟 不錯 早餐 用心 好吃 老闆娘 人 親切 二樓 雙人房 毛玻璃 設計 住在 同 一 層 樓 無 隱私 房間 內 沒有 衛浴 不便 民宿 乾淨 二樓 馬桶 內側 邊邊 霉垢 刷 乾淨，相似度：0.14625059350010794\n",
      "評論：超棒 好吃，相似度：0.0\n",
      "\n",
      "\n",
      "飯店名稱：陽光聖亞旅店\n",
      "平均相似度:0.07209086411636584\n",
      "前10條相似的評論:\n",
      "評論：房間 海景 棒，相似度：0.38642716625265017\n",
      "評論：房間 乾淨 寬敞，相似度：0.37642315786029673\n",
      "評論：房間 漂亮 風景 美，相似度：0.27259975472114806\n",
      "評論：喜歡 待在 房間 看到 海 人 這兒 適合 房間 乾淨 床 被子 舒適，相似度：0.25865871848912453\n",
      "評論：老闆 服務 親切 人 房間 明亮 乾淨，相似度：0.24982073940400212\n",
      "評論：地點 佳 服務 好 風景 漂亮 房間 乾淨 舒適 地點 佳 服務 好 風景 漂亮 房間 乾淨 舒適，相似度：0.21682307644909057\n",
      "評論：位於 對面 房間 欣賞 美景 附近 有 便利商店 便利 員工 服務 棒 熱情 有禮 價格 不錯 值 高 房間 乾淨，相似度：0.21589650312418757\n",
      "評論：海景 覺得 房間 放 礦泉水，相似度：0.21485729573158635\n",
      "評論：房間 風景 佳 乾淨 整潔 老闆 用心 熱情，相似度：0.19828412589351113\n",
      "評論：房間 照片 無異，相似度：0.195551135265147\n",
      "\n",
      "\n",
      "飯店名稱：風華渡假旅館\n",
      "平均相似度:0.06731216603744469\n",
      "前10條相似的評論:\n",
      "評論：房間 大，相似度：1.0\n",
      "評論：房間 大，相似度：1.0\n",
      "評論：房間 吵 差，相似度：1.0\n",
      "評論：不錯 房間 大 地點，相似度：0.4289619225684878\n",
      "評論：房間 佈置 整體 上 不錯 房間 內 蚊子 多 晚上 咬，相似度：0.3653512350265948\n",
      "評論：房間 菸味 重，相似度：0.3040495689640538\n",
      "評論：房間 大 氣氛 佳 舒適，相似度：0.2768113029495801\n",
      "評論：房間 乾淨 整齊 浴室，相似度：0.27473817713622095\n",
      "評論：浴室 大 泡澡 舒服 房間 美，相似度：0.27257806811496355\n",
      "評論：舒適度 房間 有 煙味，相似度：0.2551117363687556\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "\n",
    "df_tf_idf = pd.read_csv('0_10000.csv', header=0)\n",
    "\n",
    "sentences_tokenized = [eval(d) for d in df_tf_idf[\"綜合評論_ws\"].values]\n",
    "pos_tags = [eval(d) for d in df_tf_idf[\"綜合評論_pos\"].values]\n",
    "\n",
    "# 嘗試只保留有意義的詞性\n",
    "stop_pos = set(['Nep', 'Nh', 'Nb'])  # 這 3 種詞性不保留\n",
    "\n",
    "short_with_pos = []  # 放過濾後的詞性與句子\n",
    "short_sentence = []  # 放過濾後的句子\n",
    "\n",
    "for sentence, sentence_pos_tags in zip(sentences_tokenized, pos_tags):\n",
    "    short_with_pos_sentence = []  # 儲存這個句子過濾後的詞（帶詞性）\n",
    "    short_sentence_sentence = []  # 儲存這個句子過濾後的詞（不帶詞性）\n",
    "    \n",
    "    for word_ws, word_pos in zip(sentence, sentence_pos_tags):\n",
    "        \n",
    "        # 只留名詞和動詞\n",
    "        is_N_or_V = word_pos.startswith(\"V\") or word_pos.startswith(\"N\")\n",
    "\n",
    "        # 去掉名詞裡的某些詞性\n",
    "        is_not_stop_pos = word_pos not in stop_pos\n",
    "\n",
    "        if is_N_or_V and is_not_stop_pos:\n",
    "            short_with_pos_sentence.append(f\"{word_ws}({word_pos})\")\n",
    "            short_sentence_sentence.append(f\"{word_ws}\")\n",
    "\n",
    "    short_with_pos.append(short_with_pos_sentence)\n",
    "    short_sentence.append(short_sentence_sentence)\n",
    "\n",
    "# 將 '綜合評論_ws' 欄位更新為 'short_sentence' 列表\n",
    "df_tf_idf['綜合評論_ws'] = [' '.join(s) for s in short_sentence]\n",
    "\n",
    "# 將綜合評論過濾後為空的資料去除\n",
    "non_empty_mask = df_tf_idf['綜合評論_ws'].str.strip() != ''\n",
    "\n",
    "# 使用此遮罩來過濾 DataFrame\n",
    "df_tf_idf = df_tf_idf[non_empty_mask]\n",
    "\n",
    "# 使用 TF-IDF 將所有評論文本轉換為數值向量\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(df_tf_idf['綜合評論_ws'])\n",
    "\n",
    "# 假設新用戶輸入的需求\n",
    "new_user_input = \"房間 大\"\n",
    "\n",
    "# 將新用戶輸入轉換為相同的數值向量\n",
    "new_vector = vectorizer.transform([new_user_input])\n",
    "\n",
    "# 獲取所有的飯店名稱\n",
    "hotel_names = df['飯店名稱'].unique()\n",
    "\n",
    "# 初始化一個空的字典來儲存每個飯店的所有評論相似度和評論文本\n",
    "hotel_reviews_similarities = {}\n",
    "\n",
    "# 對每個飯店進行處理\n",
    "for hotel_name in hotel_names:\n",
    "    # 只選取該飯店的評論\n",
    "    hotel_reviews = df_tf_idf[df_tf_idf['飯店名稱'] == hotel_name]['綜合評論_ws']\n",
    "    \n",
    "    # 使用擬合好的TF-IDF將飯店的評論轉換為數值向量\n",
    "    X = vectorizer.transform(hotel_reviews)\n",
    "\n",
    "    # 計算新用戶輸入與所有評論的餘弦相似度\n",
    "    cos_similarities = cosine_similarity(new_vector, X).flatten()\n",
    "    \n",
    "    # 將這個飯店的所有評論相似度和評論文本儲存到字典中\n",
    "    hotel_reviews_similarities[hotel_name] = list(zip(hotel_reviews, cos_similarities))\n",
    "\n",
    "# 初始化一個空的字典來儲存每個飯店與新用戶需求的平均相似度\n",
    "hotel_similarities = {hotel: np.mean([sim for _, sim in reviews]) for hotel, reviews in hotel_reviews_similarities.items()}\n",
    "\n",
    "# 對飯店的平均相似度進行排序，並只取前10個\n",
    "top10_hotels = sorted(hotel_similarities.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "# 輸出與新用戶需求最相似的前 10 個飯店\n",
    "for hotel_name, avg_sim in top10_hotels:\n",
    "    print(f\"飯店名稱：{hotel_name}\")\n",
    "    print(f\"平均相似度:{avg_sim}\")\n",
    "    \n",
    "    # 對這家飯店的所有評論相似度進行排序，並只取前 10 個\n",
    "    top10_reviews = sorted(hotel_reviews_similarities[hotel_name], key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "    # 輸出與新用戶需求最相似的前10條評論\n",
    "    print(\"前10條相似的評論:\")\n",
    "    for review_text, sim in top10_reviews:\n",
    "        print(f\"評論：{review_text}，相似度：{sim}\")\n",
    "    \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9130ce6c",
   "metadata": {},
   "source": [
    "##### 將同一間飯店評論合併後，使用 roberta 模型(有誤  還須測試)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d145de73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from collections import defaultdict\n",
    "\n",
    "def sentence_based_sliding_window(text, window_size):\n",
    "    sentences = re.split('(?<=[。])', text)\n",
    "    current_group = []\n",
    "    current_length = 0\n",
    "    grouped_sentences = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_length = len(tokenizer.tokenize(sentence))\n",
    "        if sentence_length > window_size:\n",
    "            # If the length of a single sentence exceeds the window size, skip that data\n",
    "            continue\n",
    "        elif current_length + sentence_length > window_size:\n",
    "            grouped_sentences.append(\"\".join(current_group))\n",
    "            current_group = [sentence]\n",
    "            current_length = sentence_length\n",
    "        else:\n",
    "            current_group.append(sentence)\n",
    "            current_length += sentence_length\n",
    "\n",
    "    # Add the remaining sentences to grouped_sentences\n",
    "    if current_group:\n",
    "        grouped_sentences.append(\"\".join(current_group))\n",
    "\n",
    "    return grouped_sentences\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('hfl/chinese-roberta-wwm-ext')\n",
    "model = BertModel.from_pretrained('hfl/chinese-roberta-wwm-ext')\n",
    "\n",
    "\n",
    "df = pd.read_csv('./booking_comments_分詞update.csv', header=0)\n",
    "\n",
    "# Merge the comprehensive comments of the same hotel using a period\n",
    "df_grouped = df[0:10000].groupby('飯店名稱')['綜合評論'].apply(lambda x: '。'.join(x)).reset_index()\n",
    "\n",
    "# Text\n",
    "sentences = list(df_grouped.loc[:, '綜合評論'])\n",
    "\n",
    "# Get the embeddings for each text\n",
    "embeddings = []\n",
    "hotel_indices = []\n",
    "hotel_embeddings = defaultdict(list)\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    # If the sentence is too long, split it into multiple fragments\n",
    "    if len(tokenizer.tokenize(sentence)) > tokenizer.model_max_length:\n",
    "        sentence_windows = sentence_based_sliding_window(sentence, 4000)\n",
    "        for sentence_window in sentence_windows:\n",
    "            if len(tokenizer.tokenize(sentence_window)) > tokenizer.model_max_length:\n",
    "                continue\n",
    "            inputs = tokenizer(sentence_window, return_tensors=\"pt\", truncation=False, padding=True)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "            hotel_embeddings[i].append(outputs.last_hidden_state.mean(dim=1).numpy().flatten())\n",
    "    else:\n",
    "        inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=False, padding=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        hotel_embeddings[i].append(outputs.last_hidden_state.mean(dim=1).numpy().flatten())\n",
    "\n",
    "# Compute the average embedding for each hotel\n",
    "for i, hotel_embedding in hotel_embeddings.items():\n",
    "    embeddings.append(np.mean(hotel_embedding, axis=0))\n",
    "    hotel_indices.append(i)\n",
    "\n",
    "# 新的文本\n",
    "new_sentence = \"房間大\"\n",
    "inputs = tokenizer(new_sentence, return_tensors=\"pt\", truncation=True, max_length=512, padding=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "new_embedding = outputs.last_hidden_state.mean(dim=1).numpy().flatten()\n",
    "\n",
    "\n",
    "# Calculate similarity with the new text\n",
    "similarities = cosine_similarity(embeddings + [new_embedding])\n",
    "\n",
    "# Get the indices of the top 10 most similar texts\n",
    "top10_indices = np.argsort(similarities[-1][:-1])[:-10:-1]\n",
    "\n",
    "# Print the top 10 most similar texts\n",
    "for index in top10_indices:\n",
    "    print(f\"Similar hotel: {df_grouped.loc[hotel_indices[index], '飯店名稱']}, similarity: {similarities[-1][index]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285b2649",
   "metadata": {},
   "source": [
    "## Longformer 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac70b1f3",
   "metadata": {},
   "source": [
    "#### 將同一間飯店評論合併後，使用可處理長句子的 longformer 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b866d944",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_based_sliding_window(text, window_size):\n",
    "    sentences = re.split('(?<=[。])', text)\n",
    "    current_group = []\n",
    "    current_length = 0\n",
    "    grouped_sentences = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_length = len(tokenizer.tokenize(sentence))\n",
    "        if sentence_length > window_size:\n",
    "            # 如果單一句子的長度超過大小(4096)，跳過該筆數據\n",
    "            continue\n",
    "        elif current_length + sentence_length > window_size:\n",
    "            grouped_sentences.append(\"\".join(current_group))\n",
    "            current_group = [sentence]\n",
    "            current_length = sentence_length\n",
    "        else:\n",
    "            current_group.append(sentence)\n",
    "            current_length += sentence_length\n",
    "\n",
    "    # 把剩下的句子加入 grouped_sentences\n",
    "    if current_group:\n",
    "        grouped_sentences.append(\"\".join(current_group))\n",
    "    \n",
    "    return grouped_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c19da6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "相似的飯店: 大寶的民宿 Tabohouse, 相似度: 0.9586089849472046\n",
      "相似的飯店: 翠園歐風庭園民宿, 相似度: 0.9230974912643433\n",
      "相似的飯店: 花爵墾丁, 相似度: 0.9208060503005981\n",
      "相似的飯店: 白鷺灣 民宿(安平古堡), 相似度: 0.9181367754936218\n",
      "相似的飯店: 希望恆春休閒會館, 相似度: 0.9181272983551025\n",
      "相似的飯店: 卡爾登飯店 the Carlton, 相似度: 0.9166074991226196\n",
      "相似的飯店: 陽光滿屋民宿, 相似度: 0.9149898886680603\n",
      "相似的飯店: 家有囍宿, 相似度: 0.9144690036773682\n",
      "相似的飯店: 海洋風情民宿, 相似度: 0.9132213592529297\n"
     ]
    }
   ],
   "source": [
    "from transformers import LongformerModel, LongformerTokenizer\n",
    "from collections import defaultdict\n",
    "\n",
    "tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
    "model = LongformerModel.from_pretrained('allenai/longformer-base-4096')\n",
    "\n",
    "df = pd.read_csv('./booking_comments_分詞update.csv', header=0)\n",
    "\n",
    "# 將相同飯店的綜合評論用句號合併\n",
    "df_grouped = df[0:10000].groupby('飯店名稱')['綜合評論'].apply(lambda x: '。'.join(x)).reset_index()\n",
    "\n",
    "# 文本\n",
    "sentences = list(df_grouped.loc[:, '綜合評論'])\n",
    "\n",
    "# 獲取每個文本的嵌入向量\n",
    "embeddings = []\n",
    "hotel_indices = []\n",
    "hotel_embeddings = defaultdict(list)\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    # 如果句子太長，就將其分割成多個片段\n",
    "    if len(tokenizer.tokenize(sentence)) > tokenizer.model_max_length:\n",
    "        sentence_windows = sentence_based_sliding_window(sentence, 4000)\n",
    "        for sentence_window in sentence_windows:\n",
    "            if len(tokenizer.tokenize(sentence_window)) > tokenizer.model_max_length:\n",
    "                continue\n",
    "            inputs = tokenizer(sentence_window, return_tensors=\"pt\", truncation=False, padding=True)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "            hotel_embeddings[i].append(outputs.last_hidden_state.mean(dim=1).numpy().flatten())\n",
    "    else:\n",
    "        inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=False, padding=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        hotel_embeddings[i].append(outputs.last_hidden_state.mean(dim=1).numpy().flatten())\n",
    "\n",
    "# 計算每一間飯店的平均向量\n",
    "for i, hotel_embedding in hotel_embeddings.items():\n",
    "    embeddings.append(np.mean(hotel_embedding, axis=0))\n",
    "    hotel_indices.append(i)\n",
    "\n",
    "# 新的文本\n",
    "new_sentence = \"房間大\"\n",
    "inputs = tokenizer(new_sentence, return_tensors=\"pt\", truncation=False, padding=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "new_embedding = outputs.last_hidden_state.mean(dim=1).numpy().flatten()\n",
    "\n",
    "# 計算與新文本的相似性\n",
    "similarities = cosine_similarity(embeddings + [new_embedding])\n",
    "\n",
    "# 獲取最相似的 10 個文本的索引\n",
    "top10_indices = np.argsort(similarities[-1][:-1])[:-10:-1]\n",
    "\n",
    "# 印出最相似的 10 個文本\n",
    "for index in top10_indices:\n",
    "    print(f\"相似的飯店: {df_grouped.loc[hotel_indices[index], '飯店名稱']}, 相似度: {similarities[-1][index]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cf20c8",
   "metadata": {},
   "source": [
    "#### 將同一間飯店評論合併後，使用可處理長句子的 longformer，並排除評論數不足30的飯店"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "707cd6ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import LongformerModel, LongformerTokenizer\n",
    "\n",
    "tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
    "model = LongformerModel.from_pretrained('allenai/longformer-base-4096')\n",
    "\n",
    "df = pd.read_csv('./booking_comments_分詞update.csv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaab081",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# 篩選評論數大於等於 30 的飯店\n",
    "df_filtered = df[0:10000].groupby('飯店名稱').filter(lambda x: len(x) >= 30);df_filtered\n",
    "\n",
    "# 將相同飯店的綜合評論用句號合併，並保留其他重要欄位\n",
    "df_grouped = df_filtered.groupby('飯店名稱').agg({\n",
    "    '綜合評論': lambda x: '。'.join(x),\n",
    "    '縣市': lambda x: x.iloc[0],\n",
    "    '鄉鎮': lambda x: x.iloc[0],\n",
    "    '整體評分': lambda x: x.iloc[0],\n",
    "    '單項評分_整潔度': lambda x: x.iloc[0],\n",
    "    '單項評分_舒適程度': lambda x: x.iloc[0],\n",
    "    '單項評分_住宿地點': lambda x: x.iloc[0],\n",
    "    '單項評分_設施': lambda x: x.iloc[0],\n",
    "    '單項評分_員工素質': lambda x: x.iloc[0],\n",
    "    '單項評分_性價比': lambda x: x.iloc[0]\n",
    "}).reset_index();df_grouped\n",
    "\n",
    "# 文本\n",
    "sentences = list(df_grouped.loc[:, '綜合評論'])\n",
    "\n",
    "# 獲取每個文本的嵌入向量\n",
    "embeddings = []\n",
    "hotel_indices = []\n",
    "hotel_embeddings = defaultdict(list)\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    \n",
    "    # 如果句子太長，就分割成多個片段\n",
    "    if len(tokenizer.tokenize(sentence)) > tokenizer.model_max_length:\n",
    "        sentence_windows = sentence_based_sliding_window(sentence, 4000)\n",
    "        for sentence_window in sentence_windows:\n",
    "            if len(tokenizer.tokenize(sentence_window)) > tokenizer.model_max_length:\n",
    "                continue\n",
    "            inputs = tokenizer(sentence_window, return_tensors=\"pt\", truncation=False, padding=True)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "            hotel_embeddings[i].append(outputs.last_hidden_state.mean(dim=1).numpy().flatten())\n",
    "    else:\n",
    "        inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=False, padding=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        hotel_embeddings[i].append(outputs.last_hidden_state.mean(dim=1).numpy().flatten())\n",
    "\n",
    "# 計算每間飯店的平均向量\n",
    "for i, hotel_embedding in hotel_embeddings.items():\n",
    "    embeddings.append(np.mean(hotel_embedding, axis=0))\n",
    "    hotel_indices.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "203b096b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 儲存嵌入向量\n",
    "np.save('embeddings_longformer_base_4096.npy', embeddings)\n",
    "\n",
    "# 儲存飯店資訊\n",
    "df_grouped.loc[hotel_indices, :].to_csv('hotel_info_longformerbase4096.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8251730b",
   "metadata": {},
   "source": [
    "#### 使用迴圈跑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2abfae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_based_sliding_window(text, window_size):\n",
    "    sentences = re.split('(?<=[。])', text)\n",
    "    current_group = []\n",
    "    current_length = 0\n",
    "    grouped_sentences = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_length = len(tokenizer.tokenize(sentence))\n",
    "        if sentence_length > window_size:\n",
    "            # 如果單一文本太長就跳過\n",
    "            continue\n",
    "        elif current_length + sentence_length > window_size:\n",
    "            grouped_sentences.append(\"\".join(current_group))\n",
    "            current_group = [sentence]\n",
    "            current_length = sentence_length\n",
    "        else:\n",
    "            current_group.append(sentence)\n",
    "            current_length += sentence_length\n",
    "\n",
    "    # 把剩下的句子加入 grouped_sentences\n",
    "    if current_group:\n",
    "        grouped_sentences.append(\"\".join(current_group))\n",
    "\n",
    "    return grouped_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07709eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from transformers import LongformerModel, LongformerTokenizer\n",
    "import re\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 如果有 GPU 就使用，否則用 CPU \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
    "model = LongformerModel.from_pretrained('allenai/longformer-base-4096')\n",
    "model = model.to(device)\n",
    "\n",
    "df = pd.read_csv('./booking_comments_分詞update.csv', header=0)\n",
    "\n",
    "# 讀取之前的檔案\n",
    "try:\n",
    "    embeddings = np.load('embeddings_longformer_base_4096.npy').tolist()\n",
    "    df_grouped = pd.read_csv('hotel_info_longformerbase4096.csv')\n",
    "except FileNotFoundError:\n",
    "    embeddings = {}\n",
    "    df_grouped = pd.DataFrame()\n",
    "\n",
    "hotel_names = df['飯店名稱'].unique()\n",
    "n_hotels = len(hotel_names)\n",
    "\n",
    "# 分批執行(每50間飯店儲存一次)\n",
    "for i in range(0, n_hotels, 50):\n",
    "    print(f'Processing hotels {i} to {min(i + 50, n_hotels)}')\n",
    "    batch_hotel_names = hotel_names[i:i + 50]\n",
    "    df_filtered = df[df['飯店名稱'].isin(batch_hotel_names)]\n",
    "    df_grouped_batch = df_filtered.groupby('飯店名稱').agg({\n",
    "        '正評': lambda x: '。'.join(str(v) for v in x if not pd.isna(v)),\n",
    "        '縣市': lambda x: x.iloc[0],\n",
    "        '鄉鎮': lambda x: x.iloc[0],\n",
    "        '整體評分': lambda x: x.iloc[0],\n",
    "        '單項評分_整潔度': lambda x: x.iloc[0],\n",
    "        '單項評分_舒適程度': lambda x: x.iloc[0],\n",
    "        '單項評分_住宿地點': lambda x: x.iloc[0],\n",
    "        '單項評分_設施': lambda x: x.iloc[0],\n",
    "        '單項評分_員工素質': lambda x: x.iloc[0],\n",
    "        '單項評分_性價比': lambda x: x.iloc[0]\n",
    "    }).reset_index()\n",
    "    \n",
    "    sentences = list(df_grouped_batch.loc[:, '正評'])\n",
    "    hotel_embeddings = defaultdict(list)\n",
    "\n",
    "    for hotel_name, sentence in zip(batch_hotel_names, sentences):\n",
    "        \n",
    "        # 如果文本過長就拆分\n",
    "        if len(tokenizer.tokenize(sentence)) > tokenizer.model_max_length:\n",
    "            sentence_windows = sentence_based_sliding_window(sentence, 4000)\n",
    "            for sentence_window in sentence_windows:\n",
    "                if len(tokenizer.tokenize(sentence_window)) > tokenizer.model_max_length:\n",
    "                    continue\n",
    "                inputs = tokenizer(sentence_window, return_tensors=\"pt\", truncation=False, padding=True).to(device)\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(**inputs)\n",
    "                hotel_embeddings[hotel_name].append(outputs.last_hidden_state.mean(dim=1).cpu().numpy().flatten())\n",
    "        else:\n",
    "            inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=False, padding=True).to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "            hotel_embeddings[hotel_name].append(outputs.last_hidden_state.mean(dim=1).cpu().numpy().flatten())\n",
    "            \n",
    "    # 計算平均向量\n",
    "    for hotel_name, embeds in hotel_embeddings.items():\n",
    "        if len(embeds) > 0:\n",
    "            embeddings[hotel_name] = np.mean(embeds, axis=0)\n",
    "\n",
    "    # 合併文件内容\n",
    "    df_grouped = pd.concat([df_grouped, df_grouped_batch])\n",
    "    \n",
    "    # 儲存\n",
    "    np.savez('embeddings_longformer_base_4096.npy', **embeddings)\n",
    "    df_grouped.to_csv('hotel_info_longformerbase4096.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691c3acf",
   "metadata": {},
   "source": [
    "#### 進行二次篩選(根據某些特定條件)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d2bba03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_hotels(condition, embeddings, new_embedding, hotel_data, n):\n",
    "    \n",
    "    # 根據條件篩選出飯店\n",
    "    filtered_hotels = hotel_data[condition]\n",
    "\n",
    "    # 取得符合條件的飯店嵌入向量\n",
    "    filtered_embeddings = embeddings[filtered_hotels.index]\n",
    "\n",
    "    # 計算與新文本的相似性\n",
    "    similarities = cosine_similarity(np.concatenate([filtered_embeddings, new_embedding[None, :]]))\n",
    "\n",
    "    # 獲取最相似的 n 個文本的索引\n",
    "    topn_indices = np.argsort(similarities[-1][:-1])[:-n-1:-1]\n",
    "\n",
    "    # 印出最相似的 n 個飯店\n",
    "    for index in topn_indices:\n",
    "        print(f\"相似的飯店:\\n{filtered_hotels.iloc[index][['飯店名稱']]}\")\n",
    "        print(f\"縣市:{filtered_hotels.iloc[index][['縣市']]}\")\n",
    "        print(f\"鄉鎮:{filtered_hotels.iloc[index][['鄉鎮']]}\")\n",
    "        print(f\"整體評分:{filtered_hotels.iloc[index][['整體評分']]}\")\n",
    "        print(f\"相似度: {similarities[-1][index]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6396d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取 DataFrame\n",
    "df_hotel_info = pd.read_csv('hotel_info_longformerbase4096.csv')\n",
    "\n",
    "# 讀取 embeddings\n",
    "embeddings = np.load('embeddings_longformer_base_4096.npy.npz')\n",
    "\n",
    "\n",
    "# 新的文本\n",
    "new_sentence = \"房間大\"\n",
    "inputs = tokenizer(new_sentence, return_tensors=\"pt\", truncation=True, max_length=512, padding=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "new_embedding = outputs.last_hidden_state.mean(dim=1).numpy().flatten()\n",
    "\n",
    "# 搜尋'南投縣'的前 5 個相似的飯店\n",
    "get_similar_hotels(df_hotel_info['縣市'] == '南投縣', embeddings, new_embedding, df_hotel_info, 5)\n",
    "\n",
    "# 搜尋整體評分大於 8 的前 10 個相似的飯店\n",
    "get_similar_hotels(hotel_data['整體評分'] > 8, embeddings, new_embedding, hotel_data, 10)\n",
    "\n",
    "# 搜尋'南投縣'且整體評分大於 8 的前 15 個相似的飯店\n",
    "get_similar_hotels((hotel_data['縣市'] == '南投縣') & (hotel_data['整體評分'] > 8), embeddings, new_embedding, hotel_data, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e94608",
   "metadata": {},
   "source": [
    "##### 花費時間測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97fc711c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "執行時間： 3:49:31.820817\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from transformers import LongformerModel, LongformerTokenizer\n",
    "import re\n",
    "\n",
    "# 開始測量(時間)\n",
    "startime = datetime.datetime.now()\n",
    "\n",
    "tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
    "model = LongformerModel.from_pretrained('allenai/longformer-base-4096')\n",
    "\n",
    "df = pd.read_csv('./booking_comments_分詞update.csv', header=0)\n",
    "\n",
    "# 篩選評論數大於等於 30 的飯店\n",
    "df_filtered = df[0:100000].groupby('飯店名稱').filter(lambda x: len(x) >= 30);df_filtered\n",
    "\n",
    "# 將相同飯店的綜合評論用句號合併，並保留其他重要欄位\n",
    "df_grouped = df_filtered.groupby('飯店名稱').agg({\n",
    "    '綜合評論': lambda x: '。'.join(x),\n",
    "    '縣市': lambda x: x.iloc[0],\n",
    "    '鄉鎮': lambda x: x.iloc[0],\n",
    "    '整體評分': lambda x: x.iloc[0],\n",
    "    '單項評分_整潔度': lambda x: x.iloc[0],\n",
    "    '單項評分_舒適程度': lambda x: x.iloc[0],\n",
    "    '單項評分_住宿地點': lambda x: x.iloc[0],\n",
    "    '單項評分_設施': lambda x: x.iloc[0],\n",
    "    '單項評分_員工素質': lambda x: x.iloc[0],\n",
    "    '單項評分_性價比': lambda x: x.iloc[0]\n",
    "}).reset_index();df_grouped\n",
    "\n",
    "# 文本\n",
    "sentences = list(df_grouped.loc[:, '綜合評論'])\n",
    "\n",
    "# 獲取每個文本的嵌入向量\n",
    "embeddings = []\n",
    "hotel_indices = []\n",
    "hotel_embeddings = defaultdict(list)\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    \n",
    "    # 如果句子太長，就分割成多個片段\n",
    "    if len(tokenizer.tokenize(sentence)) > tokenizer.model_max_length:\n",
    "        sentence_windows = sentence_based_sliding_window(sentence, 4000)\n",
    "        for sentence_window in sentence_windows:\n",
    "            if len(tokenizer.tokenize(sentence_window)) > tokenizer.model_max_length:\n",
    "                continue\n",
    "            inputs = tokenizer(sentence_window, return_tensors=\"pt\", truncation=False, padding=True)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "            hotel_embeddings[i].append(outputs.last_hidden_state.mean(dim=1).numpy().flatten())\n",
    "    else:\n",
    "        inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=False, padding=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        hotel_embeddings[i].append(outputs.last_hidden_state.mean(dim=1).numpy().flatten())\n",
    "\n",
    "# 計算每間飯店的平均向量\n",
    "for i, hotel_embedding in hotel_embeddings.items():\n",
    "    embeddings.append(np.mean(hotel_embedding, axis=0))\n",
    "    hotel_indices.append(i)\n",
    "    \n",
    "# 儲存嵌入向量\n",
    "np.save('embeddings_longformer_base_4096_test.npy', embeddings)\n",
    "\n",
    "# 儲存飯店資訊\n",
    "df_grouped.loc[hotel_indices, :].to_csv('hotel_info_longformerbase4096_test.csv')\n",
    "\n",
    "# 結束測量\n",
    "endtime = datetime.datetime.now()\n",
    "\n",
    "# 輸出結果\n",
    "print(\"執行時間：\", endtime - startime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4e75f3",
   "metadata": {},
   "source": [
    "## BERT 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "934e16c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1 of 21...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (519 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n",
      "第1批次儲存完畢\n",
      "Processing batch 2 of 21...\n",
      "The sentence is too long. Skipping...\n",
      "第2批次儲存完畢\n",
      "Processing batch 3 of 21...\n",
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n",
      "第3批次儲存完畢\n",
      "Processing batch 4 of 21...\n",
      "The sentence is too long. Skipping...\n",
      "第4批次儲存完畢\n",
      "Processing batch 5 of 21...\n",
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n",
      "第5批次儲存完畢\n",
      "Processing batch 6 of 21...\n",
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n",
      "第6批次儲存完畢\n",
      "Processing batch 7 of 21...\n",
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n",
      "第7批次儲存完畢\n",
      "Processing batch 8 of 21...\n",
      "The sentence is too long. Skipping...\n",
      "第8批次儲存完畢\n",
      "Processing batch 9 of 21...\n",
      "The sentence is too long. Skipping...\n",
      "第9批次儲存完畢\n",
      "Processing batch 10 of 21...\n",
      "The sentence is too long. Skipping...\n",
      "第10批次儲存完畢\n",
      "Processing batch 11 of 21...\n",
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n",
      "第11批次儲存完畢\n",
      "Processing batch 12 of 21...\n",
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n",
      "第12批次儲存完畢\n",
      "Processing batch 13 of 21...\n",
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n",
      "第13批次儲存完畢\n",
      "Processing batch 14 of 21...\n",
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n",
      "第14批次儲存完畢\n",
      "Processing batch 15 of 21...\n",
      "第15批次儲存完畢\n",
      "Processing batch 16 of 21...\n",
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n",
      "第16批次儲存完畢\n",
      "Processing batch 17 of 21...\n",
      "The sentence is too long. Skipping...\n",
      "第17批次儲存完畢\n",
      "Processing batch 18 of 21...\n",
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n",
      "第18批次儲存完畢\n",
      "Processing batch 19 of 21...\n",
      "The sentence is too long. Skipping...\n",
      "第19批次儲存完畢\n",
      "Processing batch 20 of 21...\n",
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n",
      "The sentence is too long. Skipping...\n",
      "第20批次儲存完畢\n",
      "Processing batch 21 of 21...\n",
      "執行時間： 1:26:17.641749\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "import datetime\n",
    "\n",
    "# 開始測量(時間)\n",
    "startime = datetime.datetime.now()\n",
    "\n",
    "df_ = pd.read_csv('./booking_comments_分詞update.csv', header=0)\n",
    "\n",
    "# 篩選評論數大於等於 30 的飯店\n",
    "df_filtered = df_.groupby('飯店名稱').filter(lambda x: len(x) >= 30)\n",
    "\n",
    "df_filtered = df_filtered[0:100000]\n",
    "\n",
    "df_filtered = df_filtered.reset_index(drop=True)\n",
    "\n",
    "# 初始化 BERT 模型和分詞器\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "model = BertModel.from_pretrained('bert-base-chinese')\n",
    "\n",
    "# 定義每個批次的大小\n",
    "batch_size = 5000\n",
    "\n",
    "# 計算需要進行的批次數\n",
    "n_batches = (len(df_filtered) // batch_size) + 1\n",
    "\n",
    "# 開始進行批次處理\n",
    "for batch_idx in range(n_batches):\n",
    "    print(f\"Processing batch {batch_idx + 1} of {n_batches}...\")\n",
    "    \n",
    "    start_idx = batch_idx * batch_size\n",
    "    end_idx = (batch_idx + 1) * batch_size\n",
    "\n",
    "    # 評論文本\n",
    "    sentences = list(df_filtered.loc[start_idx:end_idx-1, '綜合評論'])\n",
    "\n",
    "    # 如果 sentences 是空的，則跳過該批次\n",
    "    if not sentences:\n",
    "        continue\n",
    "\n",
    "    # 初始化一個空的 list 用於儲存飯店資訊\n",
    "    hotel_info_list = []\n",
    "\n",
    "    # 獲取每個文本的嵌入向量，並儲存對應的飯店資訊\n",
    "    embeddings = []\n",
    "    for idx, sentence in enumerate(sentences):\n",
    "        if isinstance(sentence, str):\n",
    "            tokens = tokenizer.tokenize(sentence)  # 分詞\n",
    "            inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "        else:\n",
    "            print(f\"Sentence at index {idx} is not a string. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # 檢查序列長度是否超過 512\n",
    "        if len(inputs[\"input_ids\"][0]) > 512:\n",
    "            print(\"The sentence is too long. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        embeddings.append(outputs.last_hidden_state.mean(dim=1).numpy().flatten())\n",
    "        hotel_info_list.append(df_filtered.iloc[start_idx + idx].to_dict())\n",
    "\n",
    "    # 讀取原有的飯店資訊和 embeddings\n",
    "    if os.path.exists('hotel_info_bert_綜合評論.csv') and os.path.exists('embeddings_bert_綜合評論.npy'):\n",
    "        df_hotel_info_old = pd.read_csv('hotel_info_bert_綜合評論.csv')\n",
    "        embeddings_old = np.load('embeddings_bert_綜合評論.npy')\n",
    "\n",
    "    else:\n",
    "        df_hotel_info_old = pd.DataFrame()\n",
    "        embeddings_old = np.array([]).reshape(0,768)\n",
    "\n",
    "    # 儲存飯店資訊與 embeddings\n",
    "    df_hotel_info_new = pd.DataFrame(hotel_info_list)\n",
    "    df_hotel_info = pd.concat([df_hotel_info_old, df_hotel_info_new], ignore_index=True)\n",
    "    df_hotel_info.to_csv('hotel_info_bert_綜合評論.csv', index=False)\n",
    "\n",
    "    embeddings_new = np.array(embeddings)\n",
    "    embeddings = np.vstack([embeddings_old, embeddings_new])\n",
    "    np.save('embeddings_bert_綜合評論.npy', embeddings)\n",
    "    \n",
    "    print(f\"第{batch_idx + 1}批次儲存完畢\")\n",
    "    \n",
    "# 結束測量\n",
    "endtime = datetime.datetime.now()\n",
    "\n",
    "# 輸出結果\n",
    "print(\"執行時間：\", endtime - startime)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fbb50b",
   "metadata": {},
   "source": [
    "#### GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1fdeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import time\n",
    "\n",
    "openai.api_key = 'your-api-key'\n",
    "\n",
    "questions = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"Who wrote the book '1984'?\",\n",
    "    \"What is the distance from Earth to the Moon?\"\n",
    "]\n",
    "\n",
    "responses = []\n",
    "\n",
    "for question in questions:\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-004\",\n",
    "        prompt=question,\n",
    "        max_tokens=100\n",
    "    )\n",
    "    responses.append(response.choices[0].text.strip())\n",
    "    time.sleep(1)\n",
    "\n",
    "for question, answer in zip(questions, responses):\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
